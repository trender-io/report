{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div align=\"center\">\n",
      "<img src=\"https://trenderio.s3.amazonaws.com/trender-logo.png\" width=\"800\" style=\"margin-bottom:20px;\" />\n",
      "</div>\n",
      "\n",
      "<div align=\"center\">\n",
      "<img src=\"https://trenderio.s3.amazonaws.com/trender-grab.png\" width=\"800\" />\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# trender.io - the top news from the best sources \n",
      "\n",
      "**Team members:** Brinda Vijaykumar, Bryan Kate, Ollie Newth, Santosh Kandi\n",
      "\n",
      "This document provides an introduction to the thinking behind [trender.io](http://www.trender.io), explaining the path we went down including the ups and downs. All the current live code can be found at our [github repo](https://github.com/trender-io), where you can explore the python scripts responsible for processing the data in addition to the Rails site we've set up to display the output to the public.\n",
      "\n",
      "## Contents\n",
      "\n",
      "- [Introduction](#Introduction)\n",
      "- [Part 1 - Scraping the Stories](#Part-1---Scraping-the-Stories)\n",
      "- [Part 2 - Detecting Trends Using Social Sources](#Part-2---Detecting-Trends-Using-Social-Sources)\n",
      "- [Part 3 - Identifying Themes in Scraped Stories](#Part-3---Identifying-Themes-in-Scraped-Stories)\n",
      "- [Part 4 - Quantifying Bias](#Part-4---Quantifying-Bias)\n",
      "- [Part 5 - Creating the Website](#Part-5---Creating-the-Website)\n",
      "- [Part 6 - Reflections](#Part-6---Reflections)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Trender.io aims to find the top trending topics and objectively deliver the best sources to read about them. Currently there is a number of locations individuals go to where they source these stories (Facebook, twitter, CNN) but each of these sites has a bias (what their friends are sharing and what CNN classes as the best news may be neither the most important news nor the best place to read about it). Another example of bias is geographical preference - the BBC is likely to classify UK news as more important than CNN would. Our aim is to analyze the trending topics in addition to the best place to read these, and deliver these to the reader in a simple, transparent way."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<iframe width=\"800\" height=\"450\" src=\"//www.youtube.com/embed/tleshPiMTCQ\" frameborder=\"0\" allowfullscreen></iframe>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Please note that while you can download this notebook and run the code locally, **we do not recommend doing so as it most likely will not run**. This is because it has been pulled together to describe the process we went through, and we have removed some configuration parameters such as API keys for security purposes.\n",
      "\n",
      "Below we have imported some of the main libraries we have used in this project."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import math\n",
      "import time \n",
      "\n",
      "import feedparser\n",
      "import networkx as nx\n",
      "import requests\n",
      "from pattern import web\n",
      "from bs4 import BeautifulSoup \n",
      "import urllib2\n",
      "import sqlite3\n",
      "import json\n",
      "from collections import defaultdict\n",
      "\n",
      "import re\n",
      "import pandas as pd\n",
      "from pandas.io import sql\n",
      "\n",
      "from pygments import highlight\n",
      "from pygments.lexers import PythonLexer\n",
      "from pygments.formatters import HtmlFormatter\n",
      "\n",
      "from IPython.display import HTML\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib as mpl\n",
      "%matplotlib inline\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)\n",
      "\n",
      "#colorbrewer2 Dark2 qualitative color table\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 1 - Scraping the Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scraping news from major news sources is a critical component of trender.io. The stories and its contents are used in determining the most popular stories and to eventually present the story on our webpage.This ipython notebook has the code to periodically scrape from the RSS feeds and write to an SQLLITE DB.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.1 List of RSS feeds"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We start by reading from a CSV file which has a list of RSS feeds. Most sites had multiple RSS feeds. The file also has 2 additional fields ETAG and Last modified date (to be explained below). We initially started with 10 news sources which included sites like Wallstreet Journal and Economist. But due to subscription issues we had to remove some of them.\n",
      "The code below reads the CSV and loads it into a DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url1=pd.DataFrame.from_csv('url.csv').reset_index()\n",
      "print url1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                                 site etag modified\n",
        "0         'feed://rss.cnn.com/rss/cnn_topstories.rss\u2019  \u20190\u2019      \u20190\u2019\n",
        "1              'feed://rss.cnn.com/rss/cnn_world.rss'  \u20190\u2019      \u20190\u2019\n",
        "2                 'feed://rss.cnn.com/rss/cnn_us.rss'  \u20190'      '0'\n",
        "3              'feed://feeds.bbci.co.uk/news/rss.xml'  \u20190'      '0\u2019\n",
        "4        'feed://feeds.bbci.co.uk/news/world/rss.xml'  '0'      '0\u2019\n",
        "5   'feed://feeds.theguardian.com/theguardian/uk/rss'  '0'      '0'\n",
        "6   'feed://feeds.theguardian.com/theguardian/us/rss'  '0'      '0'\n",
        "7   'feed://www.cnbc.com/id/100003114/device/rss/r...  \u20190'      '0'\n",
        "8   'feed://www.cnbc.com/id/100727362/device/rss/r...  \u20190\u2019      '0'\n",
        "9         'feed://feeds.nbcnews.com/feeds/topstories'  '0'      '0'\n",
        "10  'feed://rss.nytimes.com/services/xml/rss/nyt/U...  '0'      '0'\n",
        "11  'feed://rss.nytimes.com/services/xml/rss/nyt/W...  '0'      '0'\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.2 Write a function that will read and process RSS feeds"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write a Function to process each website. Since each website has its own unique tags and style, a seperate function is written for each website.The function below processes each story with in a particular feed and extracts the following fields:\n",
      "\n",
      "Publish date, Title, Link, Picture, Image Height, Image Width, Story Description, Story Content, and Source.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def get_cnn(feed,date,url):\n",
      "    \"\"\"\n",
      "    This functions parses ONLY CNN feeds.\n",
      "    input: feed, max last modified date, url\n",
      "    output: Dataframe with 1 row for each story.\n",
      "    ['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    \n",
      "    \"\"\"    \n",
      "    columns=['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    df=pd.DataFrame(columns=columns)\n",
      "    for entry in feed.entries:\n",
      "        #check for new story\n",
      "        if entry.get('published_parsed') > date:\n",
      "            data = requests.get(entry['link']).text\n",
      "            dom = web.Element(data)\n",
      "            cont=''\n",
      "            pic=None\n",
      "            picture=False\n",
      "            wid=0\n",
      "            hgt=0\n",
      "            #extract image\n",
      "            for p in dom.by_tag('img'):\n",
      "                if hasattr(p,'src'):\n",
      "                    if \"jpg\" in str(p.attributes['src']) and \"logo\" not in str(p.attributes['src']):\n",
      "                        if hasattr(p,'height') and hasattr(p,'width'):\n",
      "                            if int(p.attributes['height'])>100:\n",
      "                                pic= p.attributes['src']\n",
      "                                wid= p.attributes['width']\n",
      "                                hgt= p.attributes['height']\n",
      "                                picture=True\n",
      "                                break\n",
      "\n",
      "            if picture==False:\n",
      "                continue\n",
      "            #extract content\n",
      "            for p in dom.by_tag('p'):\n",
      "                try:\n",
      "                    if \"<strong>\" in str(p.content) or \"cnn_storypgraph\" in str(p.attributes.values()):\n",
      "                        cont+= re.sub('<[^<]+?>', '', p.content)  #remove HTML tags and add to cont\n",
      "                except:\n",
      "                    continue\n",
      "            #ignore story is content less than 200 letters. Else write to DataFrame and return\n",
      "            if len(cont) >200:\n",
      "                dicts=[{'published':entry['published'],'title':entry['title'],'link':entry['link'],\n",
      "                        'picture':pic,'height':hgt,\n",
      "                        'width':wid,'description':entry['summary_detail'].value.split('<')[0],'content':cont,'source':\"cnn\"}]\n",
      "                df = df.append(dicts, ignore_index=True)\n",
      "            #break\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 119
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.2 continued..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_bbc(feed,date,url):\n",
      "    \"\"\"\n",
      "    This functions parses ONLY BCC feeds.\n",
      "    input: feed, max last modified date, url\n",
      "    output: Dataframe with 1 row for each story.\n",
      "    ['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    \n",
      "    \"\"\"    \n",
      "    columns=['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    df=pd.DataFrame(columns=columns)\n",
      "    for entry in feed.entries:\n",
      "        #check for new story\n",
      "        if entry.get('published_parsed') > date:\n",
      "            data = requests.get(entry['link']).text\n",
      "            dom = web.Element(data)\n",
      "            cont=''\n",
      "            pic=None\n",
      "            wid=0\n",
      "            hgt=0\n",
      "            picture=False\n",
      "            #extract image\n",
      "            for p in dom.by_tag('img'):\n",
      "                if \"jpg\" in str(p.attributes['src']):\n",
      "                    try:\n",
      "                        if int(p.attributes['height'])>150:\n",
      "                            picture= True\n",
      "                            pic= p.attributes['src']\n",
      "                            wid= p.attributes['width']\n",
      "                            hgt= p.attributes['height']\n",
      "                            break\n",
      "                    except:\n",
      "                        continue\n",
      "            if picture==False:\n",
      "                continue\n",
      "            #extract content\n",
      "            for div in dom.by_tag('div'):\n",
      "                #print type(div)\n",
      "                #print div\n",
      "                try:\n",
      "                    if str(div.attributes['class'])==\"story-body\":\n",
      "                        #print str(div.attributes['class'])\n",
      "                        for p in div.by_tag('p'):\n",
      "                            #remove HTML tags and add to cont\n",
      "                            cont+= re.sub('<[^<]+?>', '', p.content)\n",
      "                        #ignore story is content less than 200 letters. Else write to DataFrame and return\n",
      "                        if len(cont) >200:\n",
      "                            dicts=[{'published':entry['published'],'title':entry['title'],'link':entry['link'],\n",
      "                                    'picture':pic,'height':hgt,\n",
      "                                    'width':wid,'description':entry['summary_detail'].value.split('<')[0],'content':cont,'source':\"bbc\"}]\n",
      "                            df = df.append(dicts, ignore_index=True)\n",
      "                except:\n",
      "                    continue\n",
      "            \n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 120
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.2 continued..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_guardian(feed,date,url):\n",
      "    \"\"\"\n",
      "    This functions parses ONLY Guardian feeds.\n",
      "    input: feed, max last modified date, url\n",
      "    output: Dataframe with 1 row for each story.\n",
      "    ['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    \n",
      "    \"\"\"    \n",
      "    columns=['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    df=pd.DataFrame(columns=columns)\n",
      "    for entry in feed.entries:\n",
      "        #check for new story\n",
      "        if entry.get('published_parsed') > date:\n",
      "            data = requests.get(entry['link']).text\n",
      "            dom = web.Element(data)\n",
      "            cont=''\n",
      "            pic=None\n",
      "            picture=False\n",
      "            wid=0\n",
      "            hgt=0\n",
      "            #extract image\n",
      "\n",
      "            #extract content\n",
      "            for div in dom.by_tag('div'):\n",
      "                if hasattr(div,'id'):\n",
      "                    try:\n",
      "                        if str(div.attributes['id'])==\"article-body-blocks\":\n",
      "                            for p in div.by_tag('p'):\n",
      "                                cont+= re.sub('<[^<]+?>', '', p.content)\n",
      "                    except:\n",
      "                        continue\n",
      "                    try:\n",
      "                        if str(div.attributes['id'])==\"main-content-picture\":\n",
      "                            for p in div.by_tag('img'):\n",
      "                                if hasattr(p,'src'):\n",
      "                                    if \"jpg\" in str(p.attributes['src']): # and \"logo\" not in str(p.attributes['src']):\n",
      "                                        if hasattr(p,'height') and hasattr(p,'width'):\n",
      "                                            if int(p.attributes['height'])>100:\n",
      "                                                pic= p.attributes['src']\n",
      "                                                wid= p.attributes['width']\n",
      "                                                hgt= p.attributes['height']\n",
      "                                                picture=True\n",
      "                                                break\n",
      "                    except:\n",
      "                        continue\n",
      "\n",
      "            if picture==False:\n",
      "                continue\n",
      "            #ignore story is content less than 200 letters. Else write to DataFrame and return\n",
      "            if len(cont) >200:\n",
      "                dicts=[{'published':entry['published'],'title':entry['title'],'link':entry['link'],\n",
      "                        'picture':pic,'height':hgt,\n",
      "                        'width':wid,'description':entry['title_detail'].value,'content':cont,'source':\"guardian\"}]\n",
      "                df = df.append(dicts, ignore_index=True)\n",
      "            #break\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.2 continued..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_cnbc(feed,date,url):\n",
      "    \"\"\"\n",
      "    This functions parses ONLY CNBC feeds.\n",
      "    input: feed, max last modified date, url\n",
      "    output: Dataframe with 1 row for each story.\n",
      "    ['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    \n",
      "    \"\"\"    \n",
      "    columns=['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    df=pd.DataFrame(columns=columns)\n",
      "    for entry in feed.entries:\n",
      "        #check for new story\n",
      "        if entry.get('published_parsed') > date:\n",
      "            data = requests.get(entry['link']).text\n",
      "            dom = web.Element(data)\n",
      "            cont=''\n",
      "            pic=None\n",
      "            picture=False\n",
      "            wid=0\n",
      "            hgt=0\n",
      "            #extract image\n",
      "            for p in dom.by_tag('img'):\n",
      "                if hasattr(p,'src'):\n",
      "                    if \"jpg\" in str(p.attributes['src']):\n",
      "                        if \"240x160\" in str(p.attributes['src']):\n",
      "                            pic= p.attributes['src']\n",
      "                            wid= 240\n",
      "                            hgt= 160\n",
      "                            picture=True\n",
      "                            break\n",
      "            if picture==False:\n",
      "                continue\n",
      "\n",
      "            #extract content\n",
      "            for div in dom.by_tag('div'):\n",
      "                if hasattr(div,'id'):\n",
      "                    try:\n",
      "                        if str(div.attributes['id'])==\"article_body\":\n",
      "                            for p in div.by_tag('p'):\n",
      "                                cont+= re.sub('<[^<]+?>', '', p.content)\n",
      "                    except:\n",
      "                        continue\n",
      "\n",
      "            #ignore story is content less than 200 letters. Else write to DataFrame and return\n",
      "            if len(cont) >200:\n",
      "                dicts=[{'published':entry['published'],'title':entry['title'],'link':entry['link'],\n",
      "                        'picture':pic,'height':hgt,\n",
      "                        'width':wid,'description':entry['summary_detail'].value,'content':cont,'source':\"cnbc\"}]\n",
      "                df = df.append(dicts, ignore_index=True)\n",
      "            #break\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.2 continued..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_nbc(feed,date,url):\n",
      "    \"\"\"\n",
      "    This functions parses ONLY NBC feeds.\n",
      "    input: feed, max last modified date, url\n",
      "    output: Dataframe with 1 row for each story.\n",
      "    ['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    \n",
      "    \"\"\"    \n",
      "    columns=['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    df=pd.DataFrame(columns=columns)\n",
      "    for entry in feed.entries:\n",
      "        #check for new story\n",
      "        if entry.get('published_parsed') > date:\n",
      "            data = requests.get(entry['link']).text\n",
      "            dom = web.Element(data)\n",
      "            cont=''\n",
      "            pic=None\n",
      "            picture=False\n",
      "            wid=0\n",
      "            hgt=0\n",
      "            #extract image\n",
      "            \n",
      "            for p in dom.by_tag('img'):\n",
      "                if hasattr(p,'src'):\n",
      "                    if \"jpg\" in str(p.attributes['src']): # and \"logo\" not in str(p.attributes['src']):\n",
      "                        if hasattr(p,'height') and hasattr(p,'width'):\n",
      "                            if int(p.attributes['height'])>100:\n",
      "                                pic= p.attributes['src']\n",
      "                                wid= p.attributes['width']\n",
      "                                hgt= p.attributes['height']\n",
      "                                picture=True\n",
      "                                break\n",
      "            if picture==False:\n",
      "                continue\n",
      "\n",
      "            #extract content\n",
      "            for div in dom.by_tag('div'):\n",
      "                if hasattr(div,'class'):\n",
      "                    try:\n",
      "                        if str(div.attributes['class'])==\"articleText\":\n",
      "                            for p in div.by_tag('p'):\n",
      "                                cont+= re.sub('<[^<]+?>', '', p.content)\n",
      "                    except:\n",
      "                        continue\n",
      "\n",
      "            #ignore story is content less than 200 letters. Else write to DataFrame and return\n",
      "            if len(cont) >200:\n",
      "                dicts=[{'published':entry['published'],'title':entry['title'],'link':entry['link'],\n",
      "                        'picture':pic,'height':hgt,\n",
      "                        'width':wid,'description':entry['summary_detail'].value.split('<')[0],'content':cont,'source':\"nbc\"}]\n",
      "                df = df.append(dicts, ignore_index=True)\n",
      "            #break\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.2 continued..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_nytimes(feed,date,url):\n",
      "    \"\"\"\n",
      "    This functions parses ONLY NYTIMES feeds.\n",
      "    input: feed, max last modified date, url\n",
      "    output: Dataframe with 1 row for each story.\n",
      "    ['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    \n",
      "    \"\"\"    \n",
      "    columns=['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "    df=pd.DataFrame(columns=columns)\n",
      "    for entry in feed.entries:\n",
      "        #check for new story\n",
      "        if entry.get('published_parsed') > date:\n",
      "            data = requests.get(entry['link']).text\n",
      "            dom = web.Element(data)\n",
      "            cont=''\n",
      "            pic=None\n",
      "            picture=False\n",
      "            wid=0\n",
      "            hgt=0\n",
      "            #extract image\n",
      "\n",
      "            for p in dom.by_tag('img'):\n",
      "                if hasattr(p,'src'):\n",
      "                    if \"jpg\" in str(p.attributes['src']): # and \"logo\" not in str(p.attributes['src']):\n",
      "                        if hasattr(p,'height') and hasattr(p,'width'):\n",
      "                            if int(p.attributes['height'])>100:\n",
      "                                pic= p.attributes['src']\n",
      "                                wid= p.attributes['width']\n",
      "                                hgt= p.attributes['height']\n",
      "                                picture=True\n",
      "                                break\n",
      "            if picture==False:\n",
      "                continue\n",
      "\n",
      "            #extract content\n",
      "            for div in dom.by_tag('div'):\n",
      "                if hasattr(div,'class'):\n",
      "                    try:\n",
      "                        if str(div.attributes['class'])==\"articleBody\":\n",
      "                            for p in div.by_tag('p'):\n",
      "                                cont+= re.sub('<[^<]+?>', '', p.content)\n",
      "                    except:\n",
      "                        continue\n",
      "\n",
      "            #ignore story is content less than 200 letters. Else write to DataFrame and return\n",
      "            if len(cont) >200:\n",
      "                #print entry\n",
      "                dicts=[{'published':entry['published'],'title':entry['title'],'link':entry['link'],\n",
      "                        'picture':pic,'height':hgt,\n",
      "                        'width':wid,'description':entry['summary_detail'].value.split('<')[0],'content':cont,'source':\"nytimes\"}]\n",
      "                df = df.append(dicts, ignore_index=True)\n",
      "            #break\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.3 Function that will return the date of most recently published story."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each feed, we extract the published date and return the max publish date. This is used to determine if additional stories have been added to the RSS feed since the last time we checked. This along with the ETAG( explained further below) is used to extract only new stories and eliminate duplicate stories. Parts of this code are dervied and modified from stackoverflow."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def max_entry_date(feed):\n",
      "    \"\"\"\n",
      "    This functions returns the date of the latest story in the feed.\n",
      "    input: feed\n",
      "    output: None or max date.\n",
      "    \"\"\"    \n",
      "    #get all published dates \n",
      "    entry_pub_dates = (e.get('published_parsed') for e in feed.entries)\n",
      "    #eliminated nuls\n",
      "    entry_pub_dates = tuple(e for e in entry_pub_dates if e is not None)\n",
      "    #calculate max date and return\n",
      "    if len(entry_pub_dates) > 0:\n",
      "        return max(entry_pub_dates)    \n",
      "    return None\n",
      "\n",
      "def entries_with_dates_after(feed, date,url):\n",
      "    \"\"\"\n",
      "    This functions calls each site parsing routine.\n",
      "    input: feed,last modified date, url\n",
      "    output: Story Dataframe\n",
      "    \"\"\"    \n",
      "    #print url,str(url)\n",
      "    if \"rss.cnn\" in str(url):\n",
      "        #print \"cnn\"\n",
      "        return get_cnn(feed,date,url)\n",
      "    if \"feed://feeds.bbci.co.uk\" in str(url):\n",
      "        #print \"bbc\"\n",
      "        return get_bbc(feed,date,url)\n",
      "    if \"feed://feeds.theguardian.com\" in str(url):\n",
      "        #print \"guardian\"\n",
      "        return get_guardian(feed,date,url)\n",
      "    if \"feed://www.cnbc.com\" in str(url):\n",
      "        #print \"cnbc\"\n",
      "        return get_cnbc(feed,date,url)\n",
      "    if \"feed://feeds.nbcnews.com\" in str(url):\n",
      "        #print \"nbc\"\n",
      "        return get_nbc(feed,date,url)\n",
      "    if \"feed://rss.nytimes.com\" in str(url):\n",
      "        #print \"nyt\"\n",
      "        return get_nytimes(feed,date,url)\n",
      "    \n",
      "               \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.4 Check each feed and get only the latest stories"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some RSS feeds(like CNN) provied an ETAG everytime you read the feed. ETAG is used to determine if new stories are added. status of 200 indicates new stories and status of 304 indicates no new stories.Some feeds do not provide ETAG's and for those feeds we use the publish date. URL dataframe keeps trak of ETAG and Last Publish date."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def get_stories(url,etg,mdfd):\n",
      "    \"\"\"\n",
      "    This function parses each url based on ETAG and Last Modified tag.\n",
      "    input: url,ETAG,last modified date\n",
      "    output: Story Dataframe, ETAG, and Last mdoifed date\n",
      "    \"\"\"   \n",
      "    #print url,etg,mdfd\n",
      "    #Get stories from each feed\n",
      "    try:\n",
      "        feed = feedparser.parse(url,etag=etg,modified=mdfd)\n",
      "    except:\n",
      "        print \"could not parse url\",url\n",
      "        return None,None,None\n",
      "    #process new entries\n",
      "    if feed.status==200:\n",
      "        dfy=entries_with_dates_after(feed, mdfd,url)\n",
      "        try:\n",
      "            feed.etag\n",
      "            return dfy,str(feed.etag),max_entry_date(feed)\n",
      "        except:\n",
      "            print \"This site does not support etag\",url\n",
      "            return dfy,None,max_entry_date(feed)\n",
      "    # no new entries\n",
      "    if feed.status==304:\n",
      "        print feed.debug_message\n",
      "        return None,None,None\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.5 Main function that loops thru each RSS feed, gets new stories, and writes to a DB."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Call the above functions to extract and parse new stories from each RSS feed. Then write new stories to the SQLLITE database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "this code wakes up every 5 minutes and checks for new stories\n",
      "\"\"\"\n",
      "columns=['published', 'title', 'link','picture','height','width','description', 'content','source']\n",
      "dfmast=pd.DataFrame(columns=columns)\n",
      "\n",
      "\n",
      "#while True:\n",
      "for idx in url1.index:\n",
      "    try:\n",
      "        dfint,etag,modified=get_stories(url1.site[idx].replace(\"'\",\"\"),url1.etag[idx],url1.modified[idx])\n",
      "    except:\n",
      "        print \"error with feed\", url1.site[idx]\n",
      "    if dfint is not None:\n",
      "        if dfint.empty:\n",
      "            print \"no new stories from:\",url1.site[idx]\n",
      "            \n",
      "        if not dfint.empty:\n",
      "            dfmast=dfmast.append(dfint,ignore_index=True)\n",
      "            url1.etag[idx]=etag\n",
      "            url1.modified[idx]=modified \n",
      "                #print dfmast.shape\n",
      "            print \" Adding new stories from:\",url1.site[idx]\n",
      "            cnx = sqlite3.connect('/Users/santoshkandi/example.db')\n",
      "            sql.write_frame(dfint, name='testsk23', con=cnx,if_exists='append')\n",
      "    #time.sleep(300)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Adding new stories from: 'feed://rss.cnn.com/rss/cnn_topstories.rss\u2019\n",
        " Adding new stories from:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://rss.cnn.com/rss/cnn_world.rss'\n",
        " Adding new stories from:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://rss.cnn.com/rss/cnn_us.rss'\n",
        "error with feed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://feeds.bbci.co.uk/news/rss.xml'\n",
        " Adding new stories from: 'feed://feeds.bbci.co.uk/news/rss.xml'\n",
        "error with feed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://feeds.bbci.co.uk/news/world/rss.xml'\n",
        " Adding new stories from: 'feed://feeds.bbci.co.uk/news/world/rss.xml'\n",
        " Adding new stories from:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://feeds.theguardian.com/theguardian/uk/rss'\n",
        " Adding new stories from:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://feeds.theguardian.com/theguardian/us/rss'\n",
        "This site does not support etag"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " feed://www.cnbc.com/id/100003114/device/rss/rss.html\n",
        " Adding new stories from: 'feed://www.cnbc.com/id/100003114/device/rss/rss.html'\n",
        "This site does not support etag"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " feed://www.cnbc.com/id/100727362/device/rss/rss.html\n",
        " Adding new stories from: 'feed://www.cnbc.com/id/100727362/device/rss/rss.html'\n",
        " Adding new stories from:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://feeds.nbcnews.com/feeds/topstories'\n",
        " Adding new stories from:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://rss.nytimes.com/services/xml/rss/nyt/US.xml'\n",
        " Adding new stories from:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 'feed://rss.nytimes.com/services/xml/rss/nyt/World.xml'\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.6 Vizualizing number of stories from each source"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfx=dfmast.groupby('source').count()\n",
      "dfx=dfx[['published']]\n",
      "dfx=dfx.reset_index()\n",
      "dfx.rename(columns={dfx.columns[0]:'Source'}, inplace=True)\n",
      "dfx.rename(columns={dfx.columns[1]:'NoofStories'}, inplace=True)\n",
      "print dfx\n",
      "x = np.arange(len(dfx.Source))\n",
      "y=dfx.NoofStories\n",
      "f = plt.figure()\n",
      "ax = f.add_axes([0.1, 0.1, 0.8, 0.8])\n",
      "ax.bar(x, y, align='center')\n",
      "ax.set_xticks(x)\n",
      "ax.set_xticklabels(dfx.Source)\n",
      "plt.xlabel('Source')\n",
      "plt.ylabel('No of Stories')\n",
      "#f.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "     Source  NoofStories\n",
        "0      cnbc           47\n",
        "1       cnn          185\n",
        "2  guardian           33\n",
        "3       nbc            4\n",
        "4   nytimes           39\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "<matplotlib.text.Text at 0x10df03210>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAGUCAYAAACr5y8ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10df6b990>"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 2 - Detecting Trends Using Social Sources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Trend detection is an important component of Trender.io (hence the name). Internet news readers are busy and have a limited attention span. Sites like CNN.com use editorial judgement to determine the news stories that should appear on the frontpage of the website. At Trender.io, we aim to promote news stories that are popular amongst social media users. \n",
      "\n",
      "In this notebook, we outline our discovery process and eventual approach to identifying trending topics."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.1 Getting Trends from Twitter\n",
      "\n",
      "Our initial approach was to simply use the Twitter API to identify trending topics (https://dev.twitter.com/docs). The API allows developers to query trending topics by geographical region. In this section we will connect to Twitter, get the list of places, and query trending topics."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.1.1 Connect to Twitter and List Places\n",
      "\n",
      "We use the Twython library (https://github.com/ryanmcgrath/twython) to connect to Twitter. We register an application and try to list the places for which trending topics are avialable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from twython import Twython"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "APP_KEY = 'XXX'\n",
      "APP_SECRET = 'XXX'\n",
      "\n",
      "twitter = Twython(app_key=APP_KEY, app_secret=APP_SECRET)\n",
      "auth = twitter.get_authentication_tokens()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The locations are identified by Where On Earth ID (WOEID) and a list of places is available from Twitter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "places = pd.read_json(json.dumps(twitter.get_available_trends()))\n",
      "places.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>country</th>\n",
        "      <th>countryCode</th>\n",
        "      <th>name</th>\n",
        "      <th>parentid</th>\n",
        "      <th>placeType</th>\n",
        "      <th>url</th>\n",
        "      <th>woeid</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>       </td>\n",
        "      <td> None</td>\n",
        "      <td> Worldwide</td>\n",
        "      <td>        0</td>\n",
        "      <td> {u'code': 19, u'name': u'Supername'}</td>\n",
        "      <td>    http://where.yahooapis.com/v1/place/1</td>\n",
        "      <td>    1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> Canada</td>\n",
        "      <td>   CA</td>\n",
        "      <td>  Winnipeg</td>\n",
        "      <td> 23424775</td>\n",
        "      <td>       {u'code': 7, u'name': u'Town'}</td>\n",
        "      <td> http://where.yahooapis.com/v1/place/2972</td>\n",
        "      <td> 2972</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> Canada</td>\n",
        "      <td>   CA</td>\n",
        "      <td>    Ottawa</td>\n",
        "      <td> 23424775</td>\n",
        "      <td>       {u'code': 7, u'name': u'Town'}</td>\n",
        "      <td> http://where.yahooapis.com/v1/place/3369</td>\n",
        "      <td> 3369</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> Canada</td>\n",
        "      <td>   CA</td>\n",
        "      <td>    Quebec</td>\n",
        "      <td> 23424775</td>\n",
        "      <td>       {u'code': 7, u'name': u'Town'}</td>\n",
        "      <td> http://where.yahooapis.com/v1/place/3444</td>\n",
        "      <td> 3444</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> Canada</td>\n",
        "      <td>   CA</td>\n",
        "      <td>  Montreal</td>\n",
        "      <td> 23424775</td>\n",
        "      <td>       {u'code': 7, u'name': u'Town'}</td>\n",
        "      <td> http://where.yahooapis.com/v1/place/3534</td>\n",
        "      <td> 3534</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "  country countryCode       name  parentid                             placeType                                       url  woeid\n",
        "0                None  Worldwide         0  {u'code': 19, u'name': u'Supername'}     http://where.yahooapis.com/v1/place/1      1\n",
        "1  Canada          CA   Winnipeg  23424775        {u'code': 7, u'name': u'Town'}  http://where.yahooapis.com/v1/place/2972   2972\n",
        "2  Canada          CA     Ottawa  23424775        {u'code': 7, u'name': u'Town'}  http://where.yahooapis.com/v1/place/3369   3369\n",
        "3  Canada          CA     Quebec  23424775        {u'code': 7, u'name': u'Town'}  http://where.yahooapis.com/v1/place/3444   3444\n",
        "4  Canada          CA   Montreal  23424775        {u'code': 7, u'name': u'Town'}  http://where.yahooapis.com/v1/place/3534   3534"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.1.2 Get Trending Topics\n",
      "For the purposes of exploration, lets pick a place and get the trending topcis right now."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf = int(places[places.name == 'San Francisco'].woeid)\n",
      "\n",
      "place_trends_json = twitter.get_place_trends(id=sf)\n",
      "place_trends = pd.DataFrame(data=place_trends_json[0]['trends'])\n",
      "place_trends.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>events</th>\n",
        "      <th>name</th>\n",
        "      <th>promoted_content</th>\n",
        "      <th>query</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> None</td>\n",
        "      <td>         #CyberMonday</td>\n",
        "      <td> None</td>\n",
        "      <td>         %23CyberMonday</td>\n",
        "      <td>        http://twitter.com/search?q=%23CyberMonday</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> None</td>\n",
        "      <td>         #LoveYourCup</td>\n",
        "      <td> None</td>\n",
        "      <td>         %23LoveYourCup</td>\n",
        "      <td>        http://twitter.com/search?q=%23LoveYourCup</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> None</td>\n",
        "      <td>               #MyBCS</td>\n",
        "      <td> None</td>\n",
        "      <td>               %23MyBCS</td>\n",
        "      <td>              http://twitter.com/search?q=%23MyBCS</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> None</td>\n",
        "      <td>            Tom Daley</td>\n",
        "      <td> None</td>\n",
        "      <td>        %22Tom+Daley%22</td>\n",
        "      <td>       http://twitter.com/search?q=%22Tom+Daley%22</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> None</td>\n",
        "      <td> #MilletinSesiAzizdir</td>\n",
        "      <td> None</td>\n",
        "      <td> %23MilletinSesiAzizdir</td>\n",
        "      <td> http://twitter.com/search?q=%23MilletinSesiAzi...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> None</td>\n",
        "      <td>            #Together</td>\n",
        "      <td> None</td>\n",
        "      <td>            %23Together</td>\n",
        "      <td>           http://twitter.com/search?q=%23Together</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> None</td>\n",
        "      <td>            Christmas</td>\n",
        "      <td> None</td>\n",
        "      <td>              Christmas</td>\n",
        "      <td>             http://twitter.com/search?q=Christmas</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> None</td>\n",
        "      <td>                  HIV</td>\n",
        "      <td> None</td>\n",
        "      <td>                    HIV</td>\n",
        "      <td>                   http://twitter.com/search?q=HIV</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> None</td>\n",
        "      <td>                  OSU</td>\n",
        "      <td> None</td>\n",
        "      <td>                    OSU</td>\n",
        "      <td>                   http://twitter.com/search?q=OSU</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> None</td>\n",
        "      <td>         Ron Burgundy</td>\n",
        "      <td> None</td>\n",
        "      <td>     %22Ron+Burgundy%22</td>\n",
        "      <td>    http://twitter.com/search?q=%22Ron+Burgundy%22</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 122,
       "text": [
        "  events                  name promoted_content                   query                                                url\n",
        "0   None          #CyberMonday             None          %23CyberMonday         http://twitter.com/search?q=%23CyberMonday\n",
        "1   None          #LoveYourCup             None          %23LoveYourCup         http://twitter.com/search?q=%23LoveYourCup\n",
        "2   None                #MyBCS             None                %23MyBCS               http://twitter.com/search?q=%23MyBCS\n",
        "3   None             Tom Daley             None         %22Tom+Daley%22        http://twitter.com/search?q=%22Tom+Daley%22\n",
        "4   None  #MilletinSesiAzizdir             None  %23MilletinSesiAzizdir  http://twitter.com/search?q=%23MilletinSesiAzi...\n",
        "5   None             #Together             None             %23Together            http://twitter.com/search?q=%23Together\n",
        "6   None             Christmas             None               Christmas              http://twitter.com/search?q=Christmas\n",
        "7   None                   HIV             None                     HIV                    http://twitter.com/search?q=HIV\n",
        "8   None                   OSU             None                     OSU                    http://twitter.com/search?q=OSU\n",
        "9   None          Ron Burgundy             None      %22Ron+Burgundy%22     http://twitter.com/search?q=%22Ron+Burgundy%22"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.1.3 Discussion of Twitter Trends\n",
      "\n",
      "The above output presents the trending topics for the San Francisco area on December 2, 2013 1:30 PM EDT. There are several qualitative observations that inform our approach to trend detection:\n",
      "\n",
      "   1. Twitter only returns 10 trending topics for each area. Given that there is some overlap in the trends and that there is often sponsored content that is trending, this is a relatively small number of topics to fill a news site. \n",
      "   2. The topics that Twitter identifies as trending are a mix of words used *within* tweets and popular hashtags. Subjectively, hashtags are of limited value in identifying newsworthy topics (they are often inane memes or promoted by media outlets).\n",
      "   3. In general, the trending topics are related to pop culture gossip and sporting events. While these are popular subsections of news sites, we would like to identify some news stories of import.\n",
      "   4. The Twitter API limits such calls at a rate of 15 per 15 minutes (per user/app). We cannot gather data on enough places in a timely manner using the API without bending the rules of the TOS.\n",
      "\n",
      "**As a result, we investigate methods for identifying trending topics directly from social sources.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.2 Mining Trends from Tweets\n",
      "\n",
      "If we cannot rely on Twitter to provide useful trending topics, can we mine the topics ourselves from public Tweets? In this section we explore this possibility."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.2.1 Streaming Tweets\n",
      "\n",
      "Twitter provides a streaming API for gathering public tweets. Our next plan is to continuously monitor this stream, collect tweets, and identify topics ourselves. The public tweet stream produces a small sample (from 1-8% according to various sources) of the total tweets in the system. We request a filtered stream, which produces (up to) the same number of tweets but contains only those produced by users in Boston, New York, Chicago, San Francisco, and Los Angeles."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the tweet collector runs continuously, it is not a good fit for an iPython notebook. The code that collects, parses, and stores tweets is reproduced below.\n",
      "\n",
      "**Note: We discuss the concepts employed by this code later in this notebook. Code provided here is for reference only.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thecode = open(\"../backend/tweet_collector.py\").read()\n",
      "thehtml = highlight(thecode, PythonLexer(), HtmlFormatter())\n",
      "HTML(thehtml)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">re</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">sqlite3</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">threading</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">Queue</span> <span class=\"kn\">import</span> <span class=\"n\">Queue</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">guess_language</span> <span class=\"kn\">import</span> <span class=\"n\">guess_language</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">shapely.geometry</span> <span class=\"kn\">import</span> <span class=\"n\">Polygon</span><span class=\"p\">,</span> <span class=\"n\">mapping</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">asShape</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">shapely</span> <span class=\"kn\">import</span> <span class=\"n\">speedups</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">twython</span> <span class=\"kn\">import</span> <span class=\"n\">Twython</span><span class=\"p\">,</span> <span class=\"n\">TwythonStreamer</span>\n",
        "\n",
        "<span class=\"k\">if</span> <span class=\"n\">speedups</span><span class=\"o\">.</span><span class=\"n\">available</span><span class=\"p\">:</span>\n",
        "    <span class=\"n\">speedups</span><span class=\"o\">.</span><span class=\"n\">enable</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"n\">APP_KEY</span> <span class=\"o\">=</span> <span class=\"s\">&#39;JckhuMYvTreTPO4zC5fXxA&#39;</span>\n",
        "<span class=\"n\">APP_SECRET</span> <span class=\"o\">=</span> <span class=\"s\">&#39;QSAozi8gvr6urv7UwlHeCbAn83cwQ0TuxtfmbFTgV4&#39;</span>\n",
        "<span class=\"n\">OAUTH_TOKEN</span> <span class=\"o\">=</span> <span class=\"s\">&#39;198988714-Xb5Ze2toM9UDHtLtQkWSvkVmHKMcAhlU5kJnkG3Y&#39;</span>\n",
        "<span class=\"n\">OAUTH_SECRET</span> <span class=\"o\">=</span> <span class=\"s\">&#39;WY4cg2u4Oc8LRjfZmKYs7WDbNdHXVSSTNZjZdvdyn2ROy&#39;</span>\n",
        "\n",
        "<span class=\"n\">WORD_REGEX</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"s\">r&#39;^([a-zA-Z</span><span class=\"se\">\\&#39;</span><span class=\"s\">\\&quot;\\?\\.\\!,]+)$&#39;</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">HASH_REGEX</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"s\">r&#39;^(\\w+)$&#39;</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"n\">AREAS</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">&quot;los_angeles&quot;</span><span class=\"p\">:</span> <span class=\"n\">Polygon</span><span class=\"p\">([(</span><span class=\"o\">-</span><span class=\"mf\">118.66333</span><span class=\"p\">,</span><span class=\"mf\">33.610045</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">118.66333</span><span class=\"p\">,</span><span class=\"mf\">34.415973</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">117.702026</span><span class=\"p\">,</span><span class=\"mf\">34.415973</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">117.702026</span><span class=\"p\">,</span><span class=\"mf\">33.610045</span><span class=\"p\">)]),</span>\n",
        "         <span class=\"s\">&quot;san_francisco&quot;</span><span class=\"p\">:</span> <span class=\"n\">Polygon</span><span class=\"p\">([(</span><span class=\"o\">-</span><span class=\"mf\">122.75</span><span class=\"p\">,</span><span class=\"mf\">36.8</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">122.75</span><span class=\"p\">,</span><span class=\"mf\">37.8</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">121.75</span><span class=\"p\">,</span><span class=\"mf\">37.8</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">121.75</span><span class=\"p\">,</span><span class=\"mf\">36.8</span><span class=\"p\">)]),</span>\n",
        "         <span class=\"s\">&quot;chicago&quot;</span><span class=\"p\">:</span> <span class=\"n\">Polygon</span><span class=\"p\">([(</span><span class=\"o\">-</span><span class=\"mf\">88.253174</span><span class=\"p\">,</span><span class=\"mf\">41.520917</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">88.253174</span><span class=\"p\">,</span><span class=\"mf\">42.122673</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">87.264404</span><span class=\"p\">,</span><span class=\"mf\">42.122673</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">87.264404</span><span class=\"p\">,</span><span class=\"mf\">41.520917</span><span class=\"p\">)]),</span>\n",
        "         <span class=\"s\">&quot;boston&quot;</span><span class=\"p\">:</span> <span class=\"n\">Polygon</span><span class=\"p\">([(</span><span class=\"o\">-</span><span class=\"mf\">71.315002</span><span class=\"p\">,</span><span class=\"mf\">42.234618</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">71.315002</span><span class=\"p\">,</span><span class=\"mf\">42.429539</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">70.927734</span><span class=\"p\">,</span><span class=\"mf\">42.429539</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">70.927734</span><span class=\"p\">,</span><span class=\"mf\">42.234618</span><span class=\"p\">)]),</span>\n",
        "         <span class=\"s\">&quot;new_york&quot;</span><span class=\"p\">:</span> <span class=\"n\">Polygon</span><span class=\"p\">([(</span><span class=\"o\">-</span><span class=\"mf\">74.057465</span><span class=\"p\">,</span><span class=\"mf\">40.579542</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">74.057465</span><span class=\"p\">,</span><span class=\"mf\">40.860564</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">73.766327</span><span class=\"p\">,</span><span class=\"mf\">40.860564</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">73.766327</span><span class=\"p\">,</span><span class=\"mf\">40.579542</span><span class=\"p\">)])}</span>\n",
        "\n",
        "<span class=\"n\">LOCS</span> <span class=\"o\">=</span> <span class=\"s\">&quot;,&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">([</span><span class=\"s\">&quot;,&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">([</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">p</span> <span class=\"ow\">in</span> <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">bounds</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">AREAS</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">()])</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">class</span> <span class=\"nc\">SimpleCollector</span><span class=\"p\">(</span><span class=\"n\">TwythonStreamer</span><span class=\"p\">):</span>    \n",
        "    <span class=\"k\">def</span> <span class=\"nf\">on_success</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">):</span>\n",
        "        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">running_</span><span class=\"p\">:</span>\n",
        "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">disconnect</span><span class=\"p\">()</span>\n",
        "        <span class=\"k\">if</span> <span class=\"s\">&#39;text&#39;</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">queue_</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n",
        "            \n",
        "    <span class=\"k\">def</span> <span class=\"nf\">on_error</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">status_code</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">):</span>\n",
        "        <span class=\"k\">print</span> <span class=\"s\">&quot;TWITTER STREAM ERROR: &quot;</span><span class=\"p\">,</span> <span class=\"n\">status_code</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">disconnect</span><span class=\"p\">()</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">collect_location</span><span class=\"p\">(</span><span class=\"n\">locs</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">stream</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n",
        "    \n",
        "    <span class=\"k\">while</span><span class=\"p\">(</span><span class=\"n\">running_</span><span class=\"p\">):</span>\n",
        "        <span class=\"k\">try</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">stream</span> <span class=\"o\">=</span> <span class=\"n\">SimpleCollector</span><span class=\"p\">(</span><span class=\"n\">APP_KEY</span><span class=\"p\">,</span> <span class=\"n\">APP_SECRET</span><span class=\"p\">,</span> <span class=\"n\">OAUTH_TOKEN</span><span class=\"p\">,</span> <span class=\"n\">OAUTH_SECRET</span><span class=\"p\">)</span>\n",
        "            <span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">statuses</span><span class=\"o\">.</span><span class=\"n\">filter</span><span class=\"p\">(</span><span class=\"n\">locations</span><span class=\"o\">=</span><span class=\"n\">locs</span><span class=\"p\">)</span>\n",
        "        <span class=\"k\">except</span><span class=\"p\">:</span>\n",
        "            <span class=\"k\">print</span> <span class=\"s\">&quot;CAUGHT EXCEPTION, RECONNECTING&quot;</span>\n",
        "            <span class=\"k\">if</span> <span class=\"n\">stream</span><span class=\"p\">:</span>\n",
        "                <span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">disconnect</span><span class=\"p\">()</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">parse_tweets</span><span class=\"p\">():</span>    \n",
        "    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">sqlite3</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span><span class=\"n\">dbfile_</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">cursor</span> <span class=\"o\">=</span> <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>\n",
        "    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;PRAGMA journal_mode = WAL&quot;</span><span class=\"p\">)</span>\n",
        "    \n",
        "    <span class=\"k\">if</span> <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">!=</span> <span class=\"s\">&quot;wal&quot;</span><span class=\"p\">:</span>\n",
        "        <span class=\"k\">print</span> <span class=\"s\">&quot;Could not set journal_mode!&quot;</span>\n",
        "    \n",
        "    <span class=\"k\">while</span><span class=\"p\">(</span><span class=\"n\">running_</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">tweet</span> <span class=\"o\">=</span> <span class=\"n\">queue_</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">()</span>\n",
        "        \n",
        "        <span class=\"c\"># place tweet into a city</span>\n",
        "        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">tweet</span><span class=\"p\">[</span><span class=\"s\">&#39;place&#39;</span><span class=\"p\">]</span> <span class=\"ow\">or</span> <span class=\"ow\">not</span> <span class=\"n\">tweet</span><span class=\"p\">[</span><span class=\"s\">&#39;place&#39;</span><span class=\"p\">][</span><span class=\"s\">&#39;bounding_box&#39;</span><span class=\"p\">]:</span>\n",
        "            <span class=\"k\">continue</span>\n",
        "            \n",
        "        <span class=\"n\">poly</span> <span class=\"o\">=</span> <span class=\"n\">asShape</span><span class=\"p\">(</span><span class=\"n\">tweet</span><span class=\"p\">[</span><span class=\"s\">&#39;place&#39;</span><span class=\"p\">][</span><span class=\"s\">&#39;bounding_box&#39;</span><span class=\"p\">])</span>\n",
        "        <span class=\"n\">city</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">c</span><span class=\"p\">,</span><span class=\"n\">p</span> <span class=\"ow\">in</span> <span class=\"n\">AREAS</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n",
        "            <span class=\"k\">if</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">intersects</span><span class=\"p\">(</span><span class=\"n\">poly</span><span class=\"p\">):</span>\n",
        "                <span class=\"n\">city</span> <span class=\"o\">=</span> <span class=\"n\">c</span>\n",
        "                <span class=\"k\">break</span>\n",
        "        \n",
        "        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">city</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">city</span> <span class=\"o\">=</span> <span class=\"s\">&quot;unknown&quot;</span>\n",
        "        \n",
        "        <span class=\"c\"># parse the tweet for unique words and figure out the language</span>\n",
        "        <span class=\"n\">tweet_text</span> <span class=\"o\">=</span> <span class=\"n\">tweet</span><span class=\"p\">[</span><span class=\"s\">&#39;text&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s\">&#39;utf-8&#39;</span><span class=\"p\">)</span>\n",
        "        <span class=\"n\">cleaned</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">group</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"n\">WORD_REGEX</span><span class=\"o\">.</span><span class=\"n\">match</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">tweet_text</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">())</span> <span class=\"k\">if</span> <span class=\"n\">m</span><span class=\"p\">]</span>\n",
        "        <span class=\"n\">lang</span> <span class=\"o\">=</span> <span class=\"n\">guess_language</span><span class=\"p\">(</span><span class=\"s\">&quot; &quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">cleaned</span><span class=\"p\">))</span>\n",
        "        <span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">translate</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"s\">&#39;.?!,</span><span class=\"se\">\\&quot;</span><span class=\"s\">&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">&quot;&#39;s&quot;</span><span class=\"p\">,</span><span class=\"s\">&#39;&#39;</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">cleaned</span><span class=\"p\">]</span>\n",
        "        \n",
        "        <span class=\"k\">if</span> <span class=\"n\">words</span> <span class=\"ow\">and</span> <span class=\"n\">lang</span> <span class=\"o\">==</span> <span class=\"s\">&#39;en&#39;</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">tstamp</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">())</span>\n",
        "            \n",
        "            <span class=\"c\"># save to sqlite db and to a set of files because we don&#39;t completely </span>\n",
        "            <span class=\"c\"># trust the sqlite db to not be corrupted at some point</span>\n",
        "            <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;INSERT INTO tweets VALUES (?, (SELECT city_id FROM cities WHERE name=?), ?)&quot;</span><span class=\"p\">,</span> \\\n",
        "                           <span class=\"p\">(</span><span class=\"n\">tstamp</span><span class=\"p\">,</span> <span class=\"n\">city</span><span class=\"p\">,</span> <span class=\"s\">&quot; &quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">cleaned</span><span class=\"p\">)))</span>\n",
        "            \n",
        "            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">words</span><span class=\"p\">:</span>\n",
        "                <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;INSERT OR IGNORE INTO vals(val) VALUES (?)&quot;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,))</span>\n",
        "                <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;INSERT INTO word VALUES &quot;</span> \\\n",
        "                               <span class=\"s\">&quot;(?, (SELECT city_id FROM cities WHERE name=?), (SELECT val_id FROM vals WHERE val=?))&quot;</span><span class=\"p\">,</span> \n",
        "                               <span class=\"p\">(</span><span class=\"n\">tstamp</span><span class=\"p\">,</span> <span class=\"n\">city</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">))</span>\n",
        "            \n",
        "            <span class=\"k\">if</span> <span class=\"n\">tweet</span><span class=\"p\">[</span><span class=\"s\">&#39;entities&#39;</span><span class=\"p\">][</span><span class=\"s\">&#39;hashtags&#39;</span><span class=\"p\">]:</span>\n",
        "                <span class=\"n\">tags</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">group</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"n\">HASH_REGEX</span><span class=\"o\">.</span><span class=\"n\">match</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"p\">[</span><span class=\"s\">&#39;text&#39;</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"n\">tweet</span><span class=\"p\">[</span><span class=\"s\">&#39;entities&#39;</span><span class=\"p\">][</span><span class=\"s\">&#39;hashtags&#39;</span><span class=\"p\">])</span> <span class=\"k\">if</span> <span class=\"n\">m</span><span class=\"p\">]</span>\n",
        "                \n",
        "                <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">tags</span><span class=\"p\">:</span>\n",
        "                    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;INSERT OR IGNORE INTO vals(val) VALUES (?)&quot;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">,))</span>\n",
        "                    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;INSERT INTO hash VALUES &quot;</span> \\\n",
        "                                   <span class=\"s\">&quot;(?, (SELECT city_id FROM cities WHERE name=?), (SELECT val_id FROM vals WHERE val=?))&quot;</span><span class=\"p\">,</span> \n",
        "                                   <span class=\"p\">(</span><span class=\"n\">tstamp</span><span class=\"p\">,</span> <span class=\"n\">city</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"p\">))</span>\n",
        "                \n",
        "            <span class=\"k\">if</span> <span class=\"n\">tweet</span><span class=\"p\">[</span><span class=\"s\">&#39;entities&#39;</span><span class=\"p\">][</span><span class=\"s\">&#39;urls&#39;</span><span class=\"p\">]:</span>\n",
        "                <span class=\"n\">urls</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">u</span><span class=\"p\">[</span><span class=\"s\">&#39;expanded_url&#39;</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">u</span> <span class=\"ow\">in</span> <span class=\"n\">tweet</span><span class=\"p\">[</span><span class=\"s\">&#39;entities&#39;</span><span class=\"p\">][</span><span class=\"s\">&#39;urls&#39;</span><span class=\"p\">]]</span>\n",
        "                \n",
        "                <span class=\"k\">for</span> <span class=\"n\">u</span> <span class=\"ow\">in</span> <span class=\"n\">urls</span><span class=\"p\">:</span>\n",
        "                    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;INSERT OR IGNORE INTO vals(val) VALUES (?)&quot;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">u</span><span class=\"p\">,))</span>\n",
        "                    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;INSERT INTO link VALUES &quot;</span> \\\n",
        "                                   <span class=\"s\">&quot;(?, (SELECT city_id FROM cities WHERE name=?), (SELECT val_id FROM vals WHERE val=?))&quot;</span><span class=\"p\">,</span> \n",
        "                                   <span class=\"p\">(</span><span class=\"n\">tstamp</span><span class=\"p\">,</span> <span class=\"n\">city</span><span class=\"p\">,</span> <span class=\"n\">u</span><span class=\"p\">))</span>\n",
        "            \n",
        "            <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">commit</span><span class=\"p\">()</span>\n",
        "    \n",
        "    <span class=\"c\"># done collecting, clean up</span>\n",
        "    <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">commit</span><span class=\"p\">()</span>\n",
        "    <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">make_db</span><span class=\"p\">():</span>\n",
        "    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">sqlite3</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span><span class=\"n\">dbfile_</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">cursor</span> <span class=\"o\">=</span> <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>\n",
        "\n",
        "    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;CREATE TABLE vals (val_id INTEGER PRIMARY KEY ASC, val TEXT NOT NULL UNIQUE)&quot;</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;CREATE TABLE cities (city_id INTEGER PRIMARY KEY ASC, name TEXT NOT NULL UNIQUE)&quot;</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;CREATE TABLE tweets (tstamp INT, city_id INTEGER, tweet TEXT)&quot;</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;CREATE TABLE word (tstamp INT, city_id INTEGER, val_id INTEGER)&quot;</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;CREATE TABLE hash (tstamp INT, city_id INTEGER, val_id INTEGER)&quot;</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;CREATE TABLE link (tstamp INT, city_id INTEGER, val_id INTEGER)&quot;</span><span class=\"p\">)</span>\n",
        "    \n",
        "    <span class=\"k\">for</span> <span class=\"n\">city</span> <span class=\"ow\">in</span> <span class=\"n\">AREAS</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"s\">&quot;unknown&quot;</span><span class=\"p\">]:</span>\n",
        "        <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;INSERT INTO cities(name) VALUES (?)&quot;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">city</span><span class=\"p\">,))</span>\n",
        " \n",
        "    <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n",
        "\n",
        "\n",
        "<span class=\"c\"># horrible global variables. some of these could be params</span>\n",
        "<span class=\"n\">queue_</span> <span class=\"o\">=</span> <span class=\"n\">Queue</span><span class=\"p\">()</span>\n",
        "<span class=\"n\">running_</span> <span class=\"o\">=</span> <span class=\"bp\">True</span>\n",
        "<span class=\"n\">outdir_</span> <span class=\"o\">=</span> <span class=\"s\">&#39;collected&#39;</span>\n",
        "<span class=\"n\">dbfile_</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">outdir_</span><span class=\"p\">,</span> <span class=\"s\">&#39;twitter.db&#39;</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># setup output</span>\n",
        "<span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">outdir_</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">makedirs</span><span class=\"p\">(</span><span class=\"n\">outdir_</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">dbfile_</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">make_db</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"c\"># start a thread for collection and parsing</span>\n",
        "<span class=\"n\">threads</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">threading</span><span class=\"o\">.</span><span class=\"n\">Thread</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">=</span><span class=\"n\">parse_tweets</span><span class=\"p\">),</span>\n",
        "           <span class=\"n\">threading</span><span class=\"o\">.</span><span class=\"n\">Thread</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">=</span><span class=\"n\">collect_location</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">LOCS</span><span class=\"p\">,))]</span>\n",
        "\n",
        "<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">threads</span><span class=\"p\">:</span>\n",
        "    <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">daemon</span> <span class=\"o\">=</span> <span class=\"bp\">True</span>\n",
        "    <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"k\">try</span><span class=\"p\">:</span>\n",
        "    <span class=\"k\">while</span><span class=\"p\">(</span><span class=\"n\">running_</span><span class=\"p\">):</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">threads</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n",
        "<span class=\"k\">except</span><span class=\"p\">:</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&quot;TERMINATING&quot;</span>\n",
        "    <span class=\"n\">running_</span> <span class=\"o\">=</span> <span class=\"bp\">False</span>\n",
        "    \n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<IPython.core.display.HTML at 0x10f5c1410>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.2.2 Parsing Tweets\n",
      "\n",
      "The code above connects to the Twitter streaming API and runs continuously. The data returned from Twitter is parsed and stored into a sqlite database. In this section we describe some of the data cleaning that is applied to the raw stream and how the data is stored.\n",
      "\n",
      "First, lets look at the format of a tweet that is delivered on the stream:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweet_json = {'contributors': None, \n",
      "              'truncated': False, \n",
      "              'text': 'Really did not want to get up for work with boobear sleeping on me and both of us all warm and cozy.', \n",
      "              'in_reply_to_status_id': None, \n",
      "              'id': 405414218251247616, \n",
      "              'favorite_count': 0, \n",
      "              'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', \n",
      "              'retweeted': False, \n",
      "              'coordinates': {'type': 'Point', \n",
      "                              'coordinates': [-117.91525561, 33.8100029]}, \n",
      "              'entities': {'symbols': [], \n",
      "                           'user_mentions': [], \n",
      "                           'hashtags': [], \n",
      "                           'urls': []}, \n",
      "              'in_reply_to_screen_name': None, \n",
      "              'id_str': '405414218251247616', \n",
      "              'retweet_count': 0, \n",
      "              'in_reply_to_user_id': None, \n",
      "              'favorited': False, \n",
      "              'user': {'follow_request_sent': None, \n",
      "                       'profile_use_background_image': True, \n",
      "                       'default_profile_image': False, \n",
      "                       'id': 37504668, \n",
      "                       'verified': False, \n",
      "                       'profile_image_url_https': 'https://pbs.twimg.com/profile_images/378800000691977015/82a7bc7ca05c9365f7cff19988e881d1_normal.jpeg', \n",
      "                       'profile_sidebar_fill_color': '493045', \n",
      "                       'profile_text_color': '28072B', \n",
      "                       'followers_count': 88, \n",
      "                       'profile_sidebar_border_color': 'FFFFFF', \n",
      "                       'id_str': '37504668', \n",
      "                       'profile_background_color': '000000', \n",
      "                       'listed_count': 3, \n",
      "                       'profile_background_image_url_https': 'https://si0.twimg.com/profile_background_images/378800000110202273/774537a2417bf817472f76b4d89770e7.jpeg', \n",
      "                       'utc_offset': -28800, \n",
      "                       'statuses_count': 1798, \n",
      "                       'description': 'Film Enthusiast, 714, Disneylander, Addicted to Quotes, and IN LOVE with my friends :)', \n",
      "                       'friends_count': 187, \n",
      "                       'location': 'Orange, California', \n",
      "                       'profile_link_color': 'F05858', \n",
      "                       'profile_image_url': 'http://pbs.twimg.com/profile_images/378800000691977015/82a7bc7ca05c9365f7cff19988e881d1_normal.jpeg', \n",
      "                       'following': None, \n",
      "                       'geo_enabled': True, \n",
      "                       'profile_banner_url': 'https://pbs.twimg.com/profile_banners/37504668/1384574968', \n",
      "                       'profile_background_image_url': 'http://a0.twimg.com/profile_background_images/378800000110202273/774537a2417bf817472f76b4d89770e7.jpeg', \n",
      "                       'name': 'Allie Slagle', \n",
      "                       'lang': 'en', \n",
      "                       'profile_background_tile': True, \n",
      "                       'favourites_count': 72, \n",
      "                       'screen_name': 'AllieSlagle', \n",
      "                       'notifications': None, \n",
      "                       'url': None, \n",
      "                       'created_at': 'Sun May 03 21:19:14 +0000 2009', \n",
      "                       'contributors_enabled': False, \n",
      "                       'time_zone': 'Pacific Time (US & Canada)', \n",
      "                       'protected': False, \n",
      "                       'default_profile': False, \n",
      "                       'is_translator': False}, \n",
      "              'geo': {'type': 'Point', \n",
      "                      'coordinates': [33.8100029, -117.91525561]}, \n",
      "              'in_reply_to_user_id_str': None, \n",
      "              'lang': 'en', \n",
      "              'created_at': 'Tue Nov 26 19:14:16 +0000 2013', \n",
      "              'filter_level': 'medium', \n",
      "              'in_reply_to_status_id_str': None, \n",
      "              'place': {'full_name': 'Anaheim, CA', \n",
      "                        'url': 'https://api.twitter.com/1.1/geo/id/0c2e6999105f8070.json', \n",
      "                        'country': 'United States', \n",
      "                        'place_type': 'city', \n",
      "                        'bounding_box': {'type': 'Polygon', \n",
      "                                         'coordinates': [[[-118.017597, 33.788835], [-118.017597, 33.881456], [-117.674604, 33.881456], [-117.674604, 33.788835]]]}, \n",
      "                        'contained_within': [], \n",
      "                        'country_code': 'US', \n",
      "                        'attributes': {}, \n",
      "                        'id': '0c2e6999105f8070', \n",
      "                        'name': 'Anaheim'}}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Conveniently, Twitter returns a JSON object with tons of information. We are only interested in a subset of this information, namely the tweet text, hash tags, links, and location.\n",
      "\n",
      "The tweet undergoes a series of parsing steps."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.1.1 Geolocation\n",
      "\n",
      "The location of the tweet is extracted using the 'place' information. When connecting to Twitter, we requested a filtered stream for five cities (using lat/lon bounding boxes). We would like to categorize this tweet into a city, but we cannot use the name because our bounding boxes actually cover a number of cities and towns. For example, the tweet above is from Anaheim, but it is on our stream because it overlaps with our Los Angeles bounding box.\n",
      "\n",
      "To categorize the city, we see if the bounding box of the city overlaps with one of our requested filters. We use the <tt>shapely</tt> library to work with geometric objects. For example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from shapely.geometry import Polygon, mapping, shape, asShape\n",
      "\n",
      "# our bounding box filters, passed to Twitter when we setup the stream\n",
      "AREAS = {\"los_angeles\": Polygon([(-118.66333,33.610045), (-118.66333,34.415973), (-117.702026,34.415973), (-117.702026,33.610045)]),\n",
      "         \"san_francisco\": Polygon([(-122.75,36.8), (-122.75,37.8), (-121.75,37.8), (-121.75,36.8)]),\n",
      "         \"chicago\": Polygon([(-88.253174,41.520917), (-88.253174,42.122673), (-87.264404,42.122673), (-87.264404,41.520917)]),\n",
      "         \"boston\": Polygon([(-71.315002,42.234618), (-71.315002,42.429539), (-70.927734,42.429539), (-70.927734,42.234618)]),\n",
      "         \"new_york\": Polygon([(-74.057465,40.579542), (-74.057465,40.860564), (-73.766327,40.860564), (-73.766327,40.579542)])}\n",
      "\n",
      "# extract polygon from tweet and check for overlap with our bounding boxes\n",
      "tweet_poly = asShape(tweet_json['place']['bounding_box'])\n",
      "for c,p in AREAS.items():\n",
      "    print c, p.intersects(tweet_poly)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "new_york False\n",
        "boston False\n",
        "los_angeles True\n",
        "san_francisco False\n",
        "chicago False\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.1.2 Filter out Non-Words\n",
      "\n",
      "Tweets often contain a mixture of words, URLs, hashtags, and usernames (denoted by the '@' prefix). For our purposes, we would like to filter out everything that is not a word. We will review hashtags and links in a later step.\n",
      "\n",
      "For this we turn to regular expressions. We run the raw tweet through a fairly conservative regex that only allows letters and some punctuation. As a result, we are left with bare words. One downside to this approach is that we may aggressively strip out content that does not match the word filter (such as words adjacent to parentheses). This is not too much of a concern, because we anticipate a large enough volume of tweets that these lost words should not impact trend identification.\n",
      "\n",
      "We demonstrate this filtering on a few collected tweets, below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "WORD_REGEX = re.compile(r'^([a-zA-Z\\'\\\"\\?\\.\\!,]+)$')\n",
      "\n",
      "tweets = [\"@HighInLA @ibhye how much would those run man cuz I need comfy shoes man\",\n",
      "          \"la voy a subir al face me re cabe esa foto je :)\",\n",
      "          \"Really did not want to get up for work with boobear sleeping on me and both of us all warm and cozy.\",\n",
      "          \"Passing by the place I met my best friend \\U0001f49a St. Joseph \\U0001f64c\",\n",
      "          \"Philly was fun. New York was fun. Toronto was fun. Kalamazoo was fun. But I'm glad to be back home after 5 months away \\U0001f49b\\U0001f303\"]\n",
      "\n",
      "cleaned = []\n",
      "\n",
      "for tweet in tweets:\n",
      "    tweet_text = tweet.encode('utf-8')\n",
      "    clean = \" \".join([m.group() for m in (WORD_REGEX.match(t) for t in tweet_text.split()) if m])\n",
      "    cleaned.append(clean)\n",
      "    print clean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "how much would those run man cuz I need comfy shoes man\n",
        "la voy a subir al face me re cabe esa foto je\n",
        "Really did not want to get up for work with boobear sleeping on me and both of us all warm and cozy.\n",
        "Passing by the place I met my best friend St. Joseph\n",
        "Philly was fun. New York was fun. Toronto was fun. Kalamazoo was fun. But I'm glad to be back home after months away\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.1.3 Language Detection\n",
      "\n",
      "With the words extracted, we can move on to language detection. For the moment, we are only concerned with English language trends. We could rely on the 'lang' attribute of the tweet JSON, but this is not a good indication of the language used in the actual tweet (it is an account setting). \n",
      "\n",
      "For language detection, we use a package called <tt>guess_language</tt>. We are using the more modern version (https://pypi.python.org/pypi/guess_language-spirit), which is an actively maintained version of the older package that confusingly uses the same Python package name.\n",
      "\n",
      "As we can see, the package does a decent job of determining the language of the tweet. This method has the added benefit of misidentifying obnoxiously abbreviated text as a foreign or unknown language."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from guess_language import guess_language\n",
      "\n",
      "# mostly english tweets\n",
      "for tweet in cleaned:\n",
      "    print guess_language(tweet)\n",
      "\n",
      "# useful wrong guess or unknown language for inane tweets\n",
      "print guess_language(\"tha sweeet ass bball game tho, lololol wut\")\n",
      "print guess_language(\"omg ahhhh\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "en\n",
        "ca\n",
        "en\n",
        "en\n",
        "en\n",
        "nso\n",
        "UNKNOWN\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.2.1.4 Storage\n",
      "\n",
      "We store the tweets into a common repository so that other parts of our system can act on the content. We could just store the cleaned tweet text and let the followon procedures deal with re-parsing it. Rather, we store the tweet contents in a few different ways to facilitate subsequent retrieval.\n",
      "\n",
      "We store the data in a sqlite database, which will allow for simultaneous access by the continuously running tweet collector and the analysis code. Since the public Twitter stream does not return that many tweets, we believe that this dataset will not grow so large that sqlite becomes a limitation.\n",
      "\n",
      "The most useful tables are defined as follows:\n",
      "\n",
      "   1. A <tt>tweets</tt> table that stores the complete cleaned tweets as UTF-8 strings. This includes any punctutation that survived the word filter. The UNIX timestamp of the tweet is stored to enable temporal queries.\n",
      "   2. A <tt>word</tt> table that stores individual words (stripped of adjacent punctuation). Words are accompanied by the UNIX timestamp of the occurance. The <tt>(tstamp, word)</tt> pairs are not unique. This table is intended to support word frequency queries over different time periods.\n",
      "   3. <tt>hash</tt> and <tt>link</tt> tables, organized in the same way as the <tt>word</tt> table to facilitate similar temporal analysis.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.3 Keyword Selection\n",
      "\n",
      "Now that we are collecting tweets continnuously, how do we analize their contents to determine trending topics?\n",
      "\n",
      "**For the analysis in this section, we will run queries against the sqlite database. The database itself may or may not be included with this report, due to size and TOS restrictions. The results of some queries are stored as JSON and provided in the <tt>data</tt> directory. We load this data for demonstration purposes rather than querying the database directly.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.3.1.1 Raw Word Frequency\n",
      "\n",
      "Let's start by looking at word frequency. If a topic is trending, then it should appear multiple times over a short time span. We query the word frequencies over an hour's worth of tweets (at approximately 1:30 PM EDT on December 2, 2013)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# THIS CODE WAS RUN AGAINST THE TWITTER DATABASE TO PRODUCE tweet_word_freq.json\n",
      "\n",
      "# a generic frequency query that can be used to get frequencies of words, hashtags, or links.\n",
      "# it does not contain a city filter, so we will get results from all cities\n",
      "# FREQ_QUERY = \"SELECT vals.val,COUNT(*) FROM %s tab,vals WHERE \" \\\n",
      "#              \"tab.val_id = vals.val_id AND \" \\\n",
      "#              \"tab.tstamp BETWEEN ? AND ? GROUP BY vals.val ORDER BY COUNT(*) DESC\"\n",
      "\n",
      "# db = sqlite3.connect('twitter.db')\n",
      "# cursor = db.cursor()\n",
      "# cursor.execute(\"PRAGMA journal_mode = WAL\")\n",
      "\n",
      "# if cursor.fetchone()[0] != \"wal\":\n",
      "#     print \"Could not set journal_mode!\"\n",
      "\n",
      "# now = int(time.time())\n",
      "# freq = [row for row in cursor.execute(FREQ_QUERY % \"word\", (now - 3600, now))]\n",
      "\n",
      "# with open('data/tweet_word_freq_hour.json', 'w') as fp:\n",
      "#     json.dump(freq, fp)\n",
      "\n",
      "# freq = [row for row in cursor.execute(FREQ_QUERY % \"word\", (now - 604800, now - 3600))]\n",
      "\n",
      "# with open('data/tweet_word_freq_week.json', 'w') as fp:\n",
      "#     json.dump(freq, fp)\n",
      "\n",
      "# db.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('data/tweet_word_freq_hour.json', 'r') as fp:\n",
      "    word_freq_hour = pd.DataFrame(json.load(fp), columns=[\"word\", \"freq\"])\n",
      "\n",
      "word_freq_hour['pct'] = word_freq_hour.freq / float(np.sum(word_freq_hour.freq)) * 100\n",
      "word_freq_hour.set_index(\"word\", inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at the raw word frequency over one hour. In addition to the count, we have determined the percentage that each word contributes the total document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print word_freq_hour.count()\n",
      "word_freq_hour.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "freq    12819\n",
        "pct     12819\n",
        "dtype: int64\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>freq</th>\n",
        "      <th>pct</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>word</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>i</th>\n",
        "      <td> 4259</td>\n",
        "      <td> 3.299325</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>the</th>\n",
        "      <td> 3887</td>\n",
        "      <td> 3.011148</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>to</th>\n",
        "      <td> 3461</td>\n",
        "      <td> 2.681138</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>a</th>\n",
        "      <td> 2504</td>\n",
        "      <td> 1.939777</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>you</th>\n",
        "      <td> 2017</td>\n",
        "      <td> 1.562512</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "      freq       pct\n",
        "word                \n",
        "i     4259  3.299325\n",
        "the   3887  3.011148\n",
        "to    3461  2.681138\n",
        "a     2504  1.939777\n",
        "you   2017  1.562512"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we would expect, common English words are most frequently used. In order to find trending topics, we can compare the frequency of words that appear in an hour's wirht of tweets to a larger corpus - say, a week's worth of tweets. The common English words should appear with roughly the same frequency in both bodies of text, but recent topics should stand out with a higher frequency in recent tweets.\n",
      "\n",
      "To test this hypothesis, we gather a baseline corpus and compute the word frequencies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('data/tweet_word_freq_week.json', 'r') as fp:\n",
      "    word_freq_week = pd.DataFrame(json.load(fp), columns=[\"word\", \"freq_base\"])\n",
      "\n",
      "word_freq_week['pct_base'] = word_freq_week.freq_base / float(np.sum(word_freq_week.freq_base)) * 100\n",
      "word_freq_week.set_index(\"word\", inplace=True)\n",
      "\n",
      "print word_freq_week.count()\n",
      "word_freq_week.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "freq_base    197617\n",
        "pct_base     197617\n",
        "dtype: int64\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>freq_base</th>\n",
        "      <th>pct_base</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>word</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>i</th>\n",
        "      <td> 603681</td>\n",
        "      <td> 3.487153</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>the</th>\n",
        "      <td> 513689</td>\n",
        "      <td> 2.967315</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>to</th>\n",
        "      <td> 459114</td>\n",
        "      <td> 2.652064</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>a</th>\n",
        "      <td> 325250</td>\n",
        "      <td> 1.878801</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>my</th>\n",
        "      <td> 301638</td>\n",
        "      <td> 1.742407</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 105,
       "text": [
        "      freq_base  pct_base\n",
        "word                     \n",
        "i        603681  3.487153\n",
        "the      513689  2.967315\n",
        "to       459114  2.652064\n",
        "a        325250  1.878801\n",
        "my       301638  1.742407"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How do we identify trending words? Let's start by visualizing the data. For the words that appear in both data sets, if we plot the frequency per hour versus the frequency per week as a scatter plot, we may see some outliers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# forget about words that do not appear frequently enough to be considered trends\n",
      "word_freq = word_freq_hour[word_freq_hour.freq > 10]\n",
      "word_freq = word_freq.join(word_freq_week)\n",
      "\n",
      "# normalize the frequency within the time period\n",
      "word_freq['pct'] /= np.max(word_freq['pct'])\n",
      "word_freq['pct_base'] /= np.max(word_freq['pct_base'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot([0,1], [0,1], color='k', alpha=0.3)\n",
      "plt.scatter(word_freq.pct_base, word_freq.pct, alpha=0.5)\n",
      "plt.xlabel(\"Normalized Word Frequency (last week)\")\n",
      "plt.ylabel(\"Normalized Word Frequency (last hour)\")\n",
      "\n",
      "remove_border()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAGHCAYAAAA5h8/lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhmEYhtFwrEPHMAzDMAyj4f4P1XhSUn/WBaYAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x110408550>"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The plot shows that most words appear in both documents with similar frequencies. There are some deviations from the ideal line, but it is diffucult to determine if these are trending words. Unfortunately, the resolution of the plot is not that helpful. \n",
      "\n",
      "The words we are after will have relatively low normalized percentages (close to 0). If we instead plot the ratio between the two, those words that appear many times more over the last hour than they do in the last week should be visible as outliers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(range(len(word_freq.pct_base)), word_freq.pct / word_freq.pct_base, alpha=0.5)\n",
      "plt.xlabel(\"Word Seen in Last Hour\")\n",
      "plt.ylabel(\"Frequency Factor\")\n",
      "\n",
      "remove_border()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAGHCAYAAAA9ch/YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQq8X8VkK5JthvwCgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10a6d13d0>"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this plot, we can see that there are some words that appear 5-50 times more frequently in the past hour than they do in the past week. We can identify these words in the data with a simple threshold:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outliers = word_freq[word_freq.pct / word_freq.pct_base > 5]\n",
      "outliers.head(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>freq</th>\n",
        "      <th>pct</th>\n",
        "      <th>freq_base</th>\n",
        "      <th>pct_base</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>word</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>monday</th>\n",
        "      <td> 192</td>\n",
        "      <td> 0.045081</td>\n",
        "      <td> 4176</td>\n",
        "      <td> 0.006918</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>cyber</th>\n",
        "      <td> 126</td>\n",
        "      <td> 0.029584</td>\n",
        "      <td> 1208</td>\n",
        "      <td> 0.002001</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>semester</th>\n",
        "      <td>  50</td>\n",
        "      <td> 0.011740</td>\n",
        "      <td> 1064</td>\n",
        "      <td> 0.001763</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>tom</th>\n",
        "      <td>  38</td>\n",
        "      <td> 0.008922</td>\n",
        "      <td>  984</td>\n",
        "      <td> 0.001630</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>finals</th>\n",
        "      <td>  35</td>\n",
        "      <td> 0.008218</td>\n",
        "      <td>  744</td>\n",
        "      <td> 0.001232</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>walker</th>\n",
        "      <td>  30</td>\n",
        "      <td> 0.007044</td>\n",
        "      <td>  671</td>\n",
        "      <td> 0.001112</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>daley</th>\n",
        "      <td>  27</td>\n",
        "      <td> 0.006340</td>\n",
        "      <td>  211</td>\n",
        "      <td> 0.000350</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mondays</th>\n",
        "      <td>  25</td>\n",
        "      <td> 0.005870</td>\n",
        "      <td>  412</td>\n",
        "      <td> 0.000682</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>presentation</th>\n",
        "      <td>  24</td>\n",
        "      <td> 0.005635</td>\n",
        "      <td>  322</td>\n",
        "      <td> 0.000533</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crash</th>\n",
        "      <td>  19</td>\n",
        "      <td> 0.004461</td>\n",
        "      <td>  490</td>\n",
        "      <td> 0.000812</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>doctors</th>\n",
        "      <td>  17</td>\n",
        "      <td> 0.003992</td>\n",
        "      <td>  404</td>\n",
        "      <td> 0.000669</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>android</th>\n",
        "      <td>  15</td>\n",
        "      <td> 0.003522</td>\n",
        "      <td>  413</td>\n",
        "      <td> 0.000684</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>auburn</th>\n",
        "      <td>  13</td>\n",
        "      <td> 0.003052</td>\n",
        "      <td>  157</td>\n",
        "      <td> 0.000260</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>eataly</th>\n",
        "      <td>  13</td>\n",
        "      <td> 0.003052</td>\n",
        "      <td>   39</td>\n",
        "      <td> 0.000065</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>awakening</th>\n",
        "      <td>  11</td>\n",
        "      <td> 0.002583</td>\n",
        "      <td>   38</td>\n",
        "      <td> 0.000063</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>tax</th>\n",
        "      <td>  11</td>\n",
        "      <td> 0.002583</td>\n",
        "      <td>  257</td>\n",
        "      <td> 0.000426</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 124,
       "text": [
        "              freq       pct  freq_base  pct_base\n",
        "word                                             \n",
        "monday         192  0.045081       4176  0.006918\n",
        "cyber          126  0.029584       1208  0.002001\n",
        "semester        50  0.011740       1064  0.001763\n",
        "tom             38  0.008922        984  0.001630\n",
        "finals          35  0.008218        744  0.001232\n",
        "walker          30  0.007044        671  0.001112\n",
        "daley           27  0.006340        211  0.000350\n",
        "mondays         25  0.005870        412  0.000682\n",
        "presentation    24  0.005635        322  0.000533\n",
        "crash           19  0.004461        490  0.000812\n",
        "doctors         17  0.003992        404  0.000669\n",
        "android         15  0.003522        413  0.000684\n",
        "auburn          13  0.003052        157  0.000260\n",
        "eataly          13  0.003052         39  0.000065\n",
        "awakening       11  0.002583         38  0.000063\n",
        "tax             11  0.002583        257  0.000426"
       ]
      }
     ],
     "prompt_number": 124
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Qualitatively, these extracted trending words seem to make some sense. Some observations:\n",
      "\n",
      "   1. At the time of this analysis, it is Cyber Monday, so the top trending words make sense.\n",
      "   2. The news of Paul Walker's untimely death in a car crash is relatively new, so his name is trending.\n",
      "   3. It is the end of the semester for college students, and they are beginning to gripe about final exams.\n",
      "   4. Tom Daley (some sort of diving star) came out as gay.\n",
      "   5. Auburn beat Alabama in college football on Saturday in a spectacular fashion and it is still being talked about.\n",
      "   6. Eataly is a new restaurant in Chicago, which is one of our selected cities. \n",
      "   \n",
      "Some of these trends align with those obtained from Twitter's API. However, there are some issues with using the raw word frequency to determine trending keywords:\n",
      "\n",
      "   1. How should the baseline document be constructed? Is a week's worth of tweets sufficient? Should we use multiple baselines?\n",
      "   2. What factor is significant? We arbitrarily chose to filter out words that appeared fewer than 10 times in an hour and had a frequency factor less than 5. We would need to identify a more robust method for choosing these parameters. \n",
      "   3. We only consider unigrams (single words). We would like to produce trending topics that are bigrams so that names are preserved (e.g. 'paul walker' instead of 'paul' and 'walker').\n",
      "   4. What about words that are new in the past hour? Our current method would only identify trending words that also appear in the baseline document. This means that a topic must be at least an hour old before becoming a trend in our system."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.3.1.2 Single Document Keyword Generation\n",
      "\n",
      "We turn our attention to the academic literature in search of a solution. One appealing approach claims to be able to extract keywords from a single document. That is, they do not require a baseline corpus to identify keywords (both unigrams and bigrams). The paper (a book chapter) is:\n",
      "\n",
      "Rose, S., & Engel, D., & Cramer, N., & Cowley, W. (2010). Automatic Keyword Extraction from Individual Documents. In Berry, M. W., & Kogan, J. (Ed.) *Text Mining: Applications and Theory* (pp. 1-20). Hoboken, NJ: John Wiley & Sons, Ltd.\n",
      "http://onlinelibrary.wiley.com/doi/10.1002/9780470689646.ch1/summary\n",
      "   \n",
      "The method identified, nicknamed RAKE, extracts keywords (after filtering out common \"stop words\") using word frequency and degree (adjacency to other words) in a single document. In theory, we should be able to treat an hour's worth of tweets as a document and see if RAKE can identify key words and phrases.\n",
      "\n",
      "We found an open source implementation of RAKE () and modified it slightly. We added some stop words for twitter (e.g. RT, omg, wtf) and refactored the code for easier invocation. We demonstrate how RAKE could be used on the cleaned tweets from the above experiment. First, we extract the tweets from the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TWEET_QUERY = \"SELECT tweet FROM tweets WHERE tstamp BETWEEN ? AND ?\"\n",
      "\n",
      "# db = sqlite3.connect('twitter.db')\n",
      "# cursor = db.cursor()\n",
      "# cursor.execute(\"PRAGMA journal_mode = WAL\")\n",
      "\n",
      "# if cursor.fetchone()[0] != \"wal\":\n",
      "#     print \"Could not set journal_mode!\"\n",
      "\n",
      "# now = int(time.time())\n",
      "# tweets = [row[0] for row in cursor.execute(TWEET_QUERY, (now - 3600, now))]\n",
      "\n",
      "# db.close()\n",
      "\n",
      "# with open('data/tweets.json', 'w') as fp:\n",
      "#     json.dump(tweets, fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('data/tweets.json', 'r') as fp:\n",
      "    tweets_hour = json.load(fp)\n",
      "    \n",
      "# construct a document from the tweets. add punctuation if missing so that RAKE\n",
      "# will break the document into sentences.\n",
      "PUNC = re.compile(r'[^a-zA-Z]$')\n",
      "doc = \" \".join([tweet + '.' if m else tweet for tweet,m in ((t, PUNC.match(t)) for t in tweets_hour)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import RAKE.rake as rake\n",
      "\n",
      "rake_kw = rake.extractKeywords(doc, stopPath='../backend/twitter_stoplist.txt')\n",
      "\n",
      "# print out the top 20 results that are bigrams or unigrams\n",
      "count = 0\n",
      "for r in rake_kw:\n",
      "    if len(r[0].split()) <= 2:\n",
      "        print r[0]\n",
      "        \n",
      "        count += 1\n",
      "        if count >= 20:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "groupies fightin\n",
        "repeat goodmorning\n",
        "repeat\n",
        "los angeles\n",
        "sac transfers\n",
        "artificial turf\n",
        "san francisco\n",
        "location se\n",
        "san lorenzo\n",
        "golden diaper\n",
        "los al\n",
        "cat luna\n",
        "culinary arts\n",
        "judge froeberg\n",
        "pacific ocean\n",
        "groupies\n",
        "george washington\n",
        "martha stewart\n",
        "synthetic division\n",
        "counting stars\n"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hmmmmm, these results are not exaclty what we were hoping for. In fact, they seem to be downright nonsense. Some thoughts:\n",
      "\n",
      "   1. It is possible that the implementation is not correct, though it was tested using the example text from the paper.\n",
      "   2. More likely, RAKE is not intended to handle large documents and/or documents with a wide variety of themes. Its intended purpose is to extract keywords from *academic paper abstracts*. Abstracts from academic papers have the benefit of being short and information dense, so this method might make more sense in that context.\n",
      "   \n",
      "Rather than investigate the problem and potentially modify the appraoch, we choose to move on to other literature in search of a solution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.3.1.3 Back to Document Frequency Methods\n",
      "\n",
      "Our experiment with RAKE did not go so well, so we turn our attention back to methods based on word frequency. After some reading, this paper proves potentially useful:\n",
      "\n",
      "Benhardus, James, and Kalita, Jugal. \"Streaming trend detection in Twitter\" *International Journal of Web Based Communities*. 9.1 (2013): 122-139.\n",
      "http://inderscience.metapress.com/content/906V117647682257\n",
      "\n",
      "The authors identify two (very similar) methods that use well known NLP concepts to detect trends in a Twitter stream. The methods, the Trending Score (TS) and Term Frequency - Inverse Document Frequency product (TF-IDF) rely on a small document composed of recent tweets (from which we want to identify trending topics) and a large corpus of historical tweets, broken into multiple, evenly sized documents. We will not reproduce the explanation of these computations here (they are available in the supplied pdf files in the <tt>papers</tt> directory of this submission). Both methods rely on word frequency calculations within the recent document and across the corpus of historical documents."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We periodically produce keywords from the sqlite database using these two methods. Once per hour we generate a set of baseline documents using the complete set of historcial tweets (minus the most recent hour). The baseline corpus is broken into one-hour documents and the metrics needed for determining the TS and TF-IDF scores are computed (e.g. term frequency, document frequency, average normalized term frequency, and inverse document frequency). The result is stored as a single json document that is ingested by a more frequent keyword generation script.\n",
      "\n",
      "The code for creating the baseline file is provided in the <tt>backend</tt> directory and is reproduced below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thecode = open(\"../backend/make_tweet_baseline.py\").read()\n",
      "thehtml = highlight(thecode, PythonLexer(), HtmlFormatter())\n",
      "HTML(thehtml)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">math</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">operator</span> <span class=\"kn\">import</span> <span class=\"n\">itemgetter</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">sqlite3</span>\n",
        "\n",
        "<span class=\"n\">FREQ_QUERY</span> <span class=\"o\">=</span> <span class=\"s\">&quot;SELECT vals.val,COUNT(*) FROM </span><span class=\"si\">%s</span><span class=\"s\"> tab,vals WHERE &quot;</span> \\\n",
        "             <span class=\"s\">&quot;tab.val_id = vals.val_id AND &quot;</span> \\\n",
        "             <span class=\"s\">&quot;tab.tstamp BETWEEN ? AND ? GROUP BY vals.val ORDER BY COUNT(*) DESC&quot;</span>\n",
        "\n",
        "\n",
        "<span class=\"n\">dbfile</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"s\">&#39;collected&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;twitter.db&#39;</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">dbfile</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&quot;There is no DB!&quot;</span>\n",
        "    <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">exit</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">sqlite3</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span><span class=\"n\">dbfile</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">cursor</span> <span class=\"o\">=</span> <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>\n",
        "<span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;PRAGMA journal_mode = WAL&quot;</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"k\">if</span> <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">!=</span> <span class=\"s\">&quot;wal&quot;</span><span class=\"p\">:</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&quot;Could not set journal_mode!&quot;</span>\n",
        "\n",
        "<span class=\"c\"># get timestamp of oldest data</span>\n",
        "<span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;SELECT tstamp FROM word ORDER BY tstamp ASC LIMIT 1&quot;</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">oldest</span> <span class=\"o\">=</span> <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "<span class=\"n\">now</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">())</span>\n",
        "\n",
        "<span class=\"n\">terms</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">()</span>\n",
        "<span class=\"n\">docs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "<span class=\"n\">hours</span> <span class=\"o\">=</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">now</span> <span class=\"o\">-</span> <span class=\"mi\">3600</span><span class=\"p\">,</span> <span class=\"n\">oldest</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">3600</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># make a document per hour of historical tweets and compute metrics</span>\n",
        "<span class=\"k\">for</span> <span class=\"n\">end</span><span class=\"p\">,</span><span class=\"n\">start</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">hours</span><span class=\"p\">,</span><span class=\"n\">hours</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]):</span>\n",
        "    <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">row</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"n\">FREQ_QUERY</span> <span class=\"o\">%</span> <span class=\"s\">&#39;word&#39;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">start</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"p\">))</span> <span class=\"k\">if</span> <span class=\"n\">row</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span>\n",
        "    <span class=\"n\">nword</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">itemgetter</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">f</span><span class=\"p\">)))</span>\n",
        "    <span class=\"n\">terms</span> <span class=\"o\">|=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">itemgetter</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">f</span><span class=\"p\">))</span>\n",
        "    \n",
        "    <span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">w</span><span class=\"p\">,</span><span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">doc</span><span class=\"p\">[</span><span class=\"n\">w</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">&#39;tf&#39;</span><span class=\"p\">:</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"s\">&#39;tf_norm&#39;</span><span class=\"p\">:</span> <span class=\"n\">c</span> <span class=\"o\">/</span> <span class=\"n\">nword</span> <span class=\"o\">*</span> <span class=\"mi\">1000000</span><span class=\"p\">}</span>\n",
        "    <span class=\"n\">docs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"n\">ndocs</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">docs</span><span class=\"p\">))</span>\n",
        "<span class=\"n\">freq</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">&#39;ndocs&#39;</span><span class=\"p\">:</span> <span class=\"n\">ndocs</span><span class=\"p\">,</span> <span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">:</span> <span class=\"p\">{}}</span>\n",
        "\n",
        "<span class=\"c\"># make global dict of metrics per term</span>\n",
        "<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">terms</span><span class=\"p\">:</span>\n",
        "    <span class=\"n\">tf</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n",
        "    <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n",
        "    <span class=\"n\">tf_norm</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n",
        "    \n",
        "    <span class=\"k\">for</span> <span class=\"n\">d</span> <span class=\"ow\">in</span> <span class=\"n\">docs</span><span class=\"p\">:</span>\n",
        "        <span class=\"k\">if</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">d</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">tf</span> <span class=\"o\">+=</span> <span class=\"n\">d</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">][</span><span class=\"s\">&#39;tf&#39;</span><span class=\"p\">]</span>\n",
        "            <span class=\"n\">tf_norm</span> <span class=\"o\">+=</span> <span class=\"n\">d</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">][</span><span class=\"s\">&#39;tf_norm&#39;</span><span class=\"p\">]</span>\n",
        "            <span class=\"n\">df</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n",
        "            \n",
        "    <span class=\"n\">freq</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">][</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">&#39;tf&#39;</span><span class=\"p\">:</span> <span class=\"n\">tf</span><span class=\"p\">,</span> <span class=\"s\">&#39;df&#39;</span><span class=\"p\">:</span> <span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"s\">&#39;atf_norm&#39;</span><span class=\"p\">:</span> <span class=\"n\">tf_norm</span> <span class=\"o\">/</span> <span class=\"n\">ndocs</span><span class=\"p\">,</span> <span class=\"s\">&#39;idf&#39;</span><span class=\"p\">:</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">ndocs</span> <span class=\"o\">/</span> <span class=\"n\">df</span><span class=\"p\">)}</span>\n",
        "\n",
        "<span class=\"n\">fp</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">&#39;twitter_baseline.json&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;w&#39;</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">freq</span><span class=\"p\">,</span> <span class=\"n\">fp</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "<IPython.core.display.HTML at 0x10f5f8450>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Every 10 minutes, we launch a process that determines the current trending topics from the latest Twitter stream. The process involves:\n",
      "\n",
      "   1. Loading the stop word list to filter out common and nonsense words. This list is the SmartStoplist from RAKE that is augmented with some common Twitter words (e.g. RT, omg, lmao).\n",
      "   2. Extracting an hour's worth of tweets from our sqlite database and extracting the terms. Terms include unigrams and bigrams that are constructed by considering pairs of words in each sentence with pairs that contain stop words excluded.\n",
      "   3. Compute the term frequency in the last hour for each identified term.\n",
      "   4. Load the baseline file and compute the TS and TF-IDF score for each term identified above.\n",
      "   5. Sort the terms by score and export the top 50 in a JSON file.\n",
      "   \n",
      "The code for computing keywords from the Twitter database and the baseline file is included in the <tt>backend</tt> directory and is reproduced below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thecode = open(\"../backend/tweet_keywords.py\").read()\n",
      "thehtml = highlight(thecode, PythonLexer(), HtmlFormatter())\n",
      "HTML(thehtml)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">glob</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">re</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">operator</span> <span class=\"kn\">import</span> <span class=\"n\">itemgetter</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">collections</span> <span class=\"kn\">import</span> <span class=\"n\">Counter</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">sqlite3</span>\n",
        "\n",
        "<span class=\"n\">AREAS</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">&quot;boston&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;new_york&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;los_angeles&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;san_francisco&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;chicago&quot;</span><span class=\"p\">]</span>\n",
        "<span class=\"n\">DAYS</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">&#39;sunday&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;monday&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;tuesday&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;wednesday&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;thursday&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;friday&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;saturday&#39;</span><span class=\"p\">]</span>\n",
        "\n",
        "<span class=\"n\">TWEET_QUERY</span> <span class=\"o\">=</span> <span class=\"s\">&quot;SELECT tweet FROM tweets WHERE tstamp BETWEEN ? AND ?&quot;</span>\n",
        "\n",
        "<span class=\"n\">SENTENCE_DELIM</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"s\">u&#39;[.!?,;:</span><span class=\"se\">\\t\\\\</span><span class=\"s\">-</span><span class=\"se\">\\\\</span><span class=\"s\">&quot;</span><span class=\"se\">\\\\</span><span class=\"s\">(</span><span class=\"se\">\\\\</span><span class=\"s\">)</span><span class=\"se\">\\u2019\\u2013</span><span class=\"s\">]&#39;</span><span class=\"p\">)</span>\n",
        "    \n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">extract_terms</span><span class=\"p\">(</span><span class=\"n\">tweet</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">global</span> <span class=\"n\">stoplist</span>\n",
        "    <span class=\"n\">unigrams</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "    <span class=\"n\">bigrams</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "    <span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"n\">SENTENCE_DELIM</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">tweet</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">())</span>\n",
        "    <span class=\"n\">nwords</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n",
        "    \n",
        "    <span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n",
        "        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">sentence</span><span class=\"p\">:</span> \n",
        "            <span class=\"k\">continue</span>\n",
        "        \n",
        "        <span class=\"c\"># unigrams</span>\n",
        "        <span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">&quot;&#39;s&quot;</span><span class=\"p\">,</span> <span class=\"s\">&#39;&#39;</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">sentence</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()]</span>\n",
        "        <span class=\"n\">nwords</span> <span class=\"o\">+=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">)</span>\n",
        "        <span class=\"n\">unigrams</span><span class=\"o\">.</span><span class=\"n\">extend</span><span class=\"p\">([</span><span class=\"n\">w</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">words</span> <span class=\"k\">if</span> <span class=\"n\">w</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">stoplist</span><span class=\"p\">])</span>\n",
        "        \n",
        "        <span class=\"c\"># bigrams</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">w1</span><span class=\"p\">,</span><span class=\"n\">w2</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">words</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]):</span>\n",
        "            <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"ow\">not</span> <span class=\"n\">w1</span> <span class=\"ow\">in</span> <span class=\"n\">stoplist</span><span class=\"p\">)</span> <span class=\"ow\">and</span> <span class=\"p\">(</span><span class=\"ow\">not</span> <span class=\"n\">w2</span> <span class=\"ow\">in</span> <span class=\"n\">stoplist</span><span class=\"p\">):</span>\n",
        "                <span class=\"n\">bigrams</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"s\">&quot;</span><span class=\"si\">%s</span><span class=\"s\"> </span><span class=\"si\">%s</span><span class=\"s\">&quot;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">w1</span><span class=\"p\">,</span> <span class=\"n\">w2</span><span class=\"p\">))</span>\n",
        "            \n",
        "    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">unigrams</span><span class=\"p\">,</span> <span class=\"n\">bigrams</span><span class=\"p\">,</span> <span class=\"n\">nwords</span><span class=\"p\">)</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">compute_keywords</span><span class=\"p\">(</span><span class=\"n\">dbcursor</span><span class=\"p\">,</span> <span class=\"n\">start</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"p\">,</span> <span class=\"n\">limit</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">global</span> <span class=\"n\">baseline</span>\n",
        "    <span class=\"k\">global</span> <span class=\"n\">thresh</span>\n",
        "    \n",
        "    <span class=\"c\"># compute average metrics from baseline to deal with unknown terms</span>\n",
        "    <span class=\"n\">idf_mean</span> <span class=\"o\">=</span> <span class=\"mf\">0.</span>\n",
        "    <span class=\"n\">atf_norm_mean</span> <span class=\"o\">=</span> <span class=\"mf\">0.</span>\n",
        "    \n",
        "    <span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">():</span>\n",
        "        <span class=\"n\">idf_mean</span> <span class=\"o\">+=</span> <span class=\"n\">m</span><span class=\"p\">[</span><span class=\"s\">&#39;idf&#39;</span><span class=\"p\">]</span>\n",
        "        <span class=\"n\">atf_norm_mean</span> <span class=\"o\">+=</span> <span class=\"n\">m</span><span class=\"p\">[</span><span class=\"s\">&#39;atf_norm&#39;</span><span class=\"p\">]</span>\n",
        "        \n",
        "    <span class=\"n\">idf_mean</span> <span class=\"o\">/=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">])</span>\n",
        "    <span class=\"n\">atf_norm_mean</span> <span class=\"o\">/=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">])</span>\n",
        "    \n",
        "    <span class=\"c\"># get tweets for the time period requested</span>\n",
        "    <span class=\"n\">tweets</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">row</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">dbcursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"n\">TWEET_QUERY</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">start</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"p\">))]</span>\n",
        "    <span class=\"n\">count</span> <span class=\"o\">=</span> <span class=\"n\">Counter</span><span class=\"p\">()</span>\n",
        "    <span class=\"n\">nwords</span> <span class=\"o\">=</span> <span class=\"mf\">0.</span>\n",
        "    \n",
        "    <span class=\"c\"># extract terms from each tweet and update document metrics</span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">tweets</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">uni</span><span class=\"p\">,</span><span class=\"n\">bi</span><span class=\"p\">,</span><span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">extract_terms</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n",
        "        <span class=\"n\">count</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">(</span><span class=\"n\">uni</span> <span class=\"o\">+</span> <span class=\"n\">bi</span><span class=\"p\">)</span>\n",
        "        <span class=\"n\">nwords</span> <span class=\"o\">+=</span> <span class=\"n\">n</span>\n",
        "    \n",
        "    <span class=\"c\"># filter out terms that do not appear that often</span>\n",
        "    <span class=\"n\">freq</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">c</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">count</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()</span> <span class=\"k\">if</span> <span class=\"n\">c</span> <span class=\"o\">&gt;=</span> <span class=\"n\">thresh</span><span class=\"p\">]</span>\n",
        "    \n",
        "    <span class=\"c\"># try to promote bigrams by removing the unigrams that comprise them</span>\n",
        "    <span class=\"n\">del_unigrams</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">([</span><span class=\"n\">s</span> <span class=\"k\">for</span> <span class=\"n\">sublist</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">freq</span> <span class=\"k\">if</span> <span class=\"s\">&quot; &quot;</span> <span class=\"ow\">in</span> <span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">sublist</span><span class=\"p\">])</span>\n",
        "    <span class=\"n\">freq</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">c</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">freq</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">del_unigrams</span><span class=\"p\">]</span>\n",
        "    \n",
        "    <span class=\"c\"># first method - compute a trendings core using the normalized term frequency from this</span>\n",
        "    <span class=\"c\"># document and the average normalized term frequency from the baseline</span>\n",
        "    <span class=\"n\">ts</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">([(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">c</span> <span class=\"o\">/</span> <span class=\"n\">nwords</span> <span class=\"o\">*</span> <span class=\"mi\">1000000</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">][</span><span class=\"n\">k</span><span class=\"p\">][</span><span class=\"s\">&#39;atf_norm&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">]</span> <span class=\"k\">else</span> <span class=\"n\">atf_norm_mean</span><span class=\"p\">))</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">freq</span><span class=\"p\">],</span> \n",
        "                <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">itemgetter</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)[:</span><span class=\"n\">limit</span><span class=\"p\">]</span>\n",
        "    \n",
        "    <span class=\"c\"># second method - use tf-idf scores to determine term popularity</span>\n",
        "    <span class=\"n\">tf_idf</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">([(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">c</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">][</span><span class=\"n\">k</span><span class=\"p\">][</span><span class=\"s\">&#39;idf&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">]</span> <span class=\"k\">else</span> <span class=\"n\">idf_mean</span><span class=\"p\">))</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">freq</span><span class=\"p\">],</span> \n",
        "                    <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">itemgetter</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)[:</span><span class=\"n\">limit</span><span class=\"p\">]</span>\n",
        "    \n",
        "    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s\">&#39;ts&#39;</span><span class=\"p\">:</span> <span class=\"n\">ts</span><span class=\"p\">,</span> <span class=\"s\">&#39;tf_idf&#39;</span><span class=\"p\">:</span> <span class=\"n\">tf_idf</span><span class=\"p\">}</span>\n",
        "    \n",
        "\n",
        "<span class=\"n\">limit</span> <span class=\"o\">=</span> <span class=\"mi\">50</span>\n",
        "<span class=\"n\">thresh</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n",
        "<span class=\"n\">dbfile</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"s\">&#39;collected&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;twitter.db&#39;</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">baselinefile</span> <span class=\"o\">=</span> <span class=\"s\">&#39;twitter_baseline.json&#39;</span>\n",
        "<span class=\"n\">stoplistfile</span> <span class=\"o\">=</span> <span class=\"s\">&#39;twitter_stoplist.txt&#39;</span>\n",
        "\n",
        "<span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">dbfile</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&quot;MISSING TWITTER DB!&quot;</span>\n",
        "    <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">exit</span><span class=\"p\">()</span>\n",
        "    \n",
        "<span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">baselinefile</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&quot;MISSING TWITTER BASELINE!&quot;</span>\n",
        "    <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">exit</span><span class=\"p\">()</span>\n",
        "    \n",
        "<span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">stoplistfile</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&quot;MISSING TWITTER STOPLIST!&quot;</span>\n",
        "    <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">exit</span><span class=\"p\">()</span>\n",
        "    \n",
        "<span class=\"n\">fp</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">baselinefile</span><span class=\"p\">,</span> <span class=\"s\">&#39;r&#39;</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">baseline</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">fp</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"n\">stoplist</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">()</span>\n",
        "<span class=\"n\">fp</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">stoplistfile</span><span class=\"p\">,</span> <span class=\"s\">&#39;r&#39;</span><span class=\"p\">)</span>\n",
        "<span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">readlines</span><span class=\"p\">():</span>\n",
        "    <span class=\"n\">stoplist</span> <span class=\"o\">|=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">rstrip</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">())</span>\n",
        "<span class=\"n\">stoplist</span> <span class=\"o\">|=</span> <span class=\"nb\">set</span><span class=\"p\">([</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">&quot;&#39;&quot;</span><span class=\"p\">,</span><span class=\"s\">&#39;&#39;</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">stoplist</span> <span class=\"k\">if</span> <span class=\"s\">&quot;&#39;&quot;</span> <span class=\"ow\">in</span> <span class=\"n\">s</span><span class=\"p\">])</span>\n",
        "<span class=\"n\">stoplist</span> <span class=\"o\">|=</span> <span class=\"nb\">set</span><span class=\"p\">([</span><span class=\"n\">s</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span> <span class=\"k\">if</span> <span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;metrics&#39;</span><span class=\"p\">][</span><span class=\"n\">s</span><span class=\"p\">][</span><span class=\"s\">&#39;df&#39;</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">baseline</span><span class=\"p\">[</span><span class=\"s\">&#39;ndocs&#39;</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"mf\">0.8</span><span class=\"p\">])</span>\n",
        "<span class=\"n\">stoplist</span> <span class=\"o\">|=</span> <span class=\"nb\">set</span><span class=\"p\">([</span><span class=\"n\">s</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">DAYS</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"n\">s</span> <span class=\"o\">+</span> <span class=\"s\">&#39;s&#39;</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">DAYS</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"n\">s</span> <span class=\"o\">+</span> <span class=\"s\">&quot;&#39;s&quot;</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">DAYS</span><span class=\"p\">]])</span>\n",
        "<span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">sqlite3</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span><span class=\"n\">dbfile</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">cursor</span> <span class=\"o\">=</span> <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>\n",
        "<span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s\">&quot;PRAGMA journal_mode = WAL&quot;</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"k\">if</span> <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">!=</span> <span class=\"s\">&quot;wal&quot;</span><span class=\"p\">:</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&quot;Could not set journal_mode!&quot;</span>\n",
        "\n",
        "<span class=\"n\">now</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">())</span>\n",
        "<span class=\"k\">print</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">compute_keywords</span><span class=\"p\">(</span><span class=\"n\">cursor</span><span class=\"p\">,</span> <span class=\"n\">now</span> <span class=\"o\">-</span> <span class=\"mi\">3600</span><span class=\"p\">,</span> <span class=\"n\">now</span><span class=\"p\">,</span> <span class=\"n\">limit</span><span class=\"p\">))</span>\n",
        "\n",
        "<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "<IPython.core.display.HTML at 0x10fe144d0>"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at the results of these computations. We load the output of the keyword generation script collected around 4:00 PM EDT on December 2, 2013."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('data/twitter_keywords.json', 'r') as fp:\n",
      "    tweet_kw_json = json.load(fp)\n",
      "\n",
      "tweet_kw = pd.DataFrame({'ts': [kw for kw,score in tweet_kw_json['ts']],\n",
      "                         'tf-idf': [kw for kw,score in tweet_kw_json['tf_idf']]})\n",
      "tweet_kw.head(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>tf-idf</th>\n",
        "      <th>ts</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>       paul walker</td>\n",
        "      <td>              sark</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>         tom daley</td>\n",
        "      <td>       directioner</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> horror convention</td>\n",
        "      <td>       paul walker</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>   steve sarkisian</td>\n",
        "      <td>         tom daley</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>       directioner</td>\n",
        "      <td> horror convention</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>              sark</td>\n",
        "      <td>   steve sarkisian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>    mercy endureth</td>\n",
        "      <td>           carroll</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>  mariska hargitay</td>\n",
        "      <td>    mercy endureth</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>             cyber</td>\n",
        "      <td>  mariska hargitay</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td>      directioners</td>\n",
        "      <td>      directioners</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>               edc</td>\n",
        "      <td>               edc</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>             drone</td>\n",
        "      <td>             cyber</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>           carroll</td>\n",
        "      <td>             drone</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>        sharkeisha</td>\n",
        "      <td>          seahawks</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>            drones</td>\n",
        "      <td>             comic</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>            finals</td>\n",
        "      <td>            drones</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>               usc</td>\n",
        "      <td>            finals</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>             comic</td>\n",
        "      <td>             prove</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>              geno</td>\n",
        "      <td>          shipping</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>             prove</td>\n",
        "      <td>              geno</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 158,
       "text": [
        "               tf-idf                 ts\n",
        "0         paul walker               sark\n",
        "1           tom daley        directioner\n",
        "2   horror convention        paul walker\n",
        "3     steve sarkisian          tom daley\n",
        "4         directioner  horror convention\n",
        "5                sark    steve sarkisian\n",
        "6      mercy endureth            carroll\n",
        "7    mariska hargitay     mercy endureth\n",
        "8               cyber   mariska hargitay\n",
        "9        directioners       directioners\n",
        "10                edc                edc\n",
        "11              drone              cyber\n",
        "12            carroll              drone\n",
        "13         sharkeisha           seahawks\n",
        "14             drones              comic\n",
        "15             finals             drones\n",
        "16                usc             finals\n",
        "17              comic              prove\n",
        "18               geno           shipping\n",
        "19              prove               geno"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above output shows the results of both methods. They are quite similar, with minor variations in the ordering of terms. A few observations:\n",
      "\n",
      "   1. We now have bigrams! There are a few caveats with how bigrams are handled in the current code.\n",
      "      1. The stoplist contains days of the week, so Cyber Monday was not considered a bigram. Oh well. Adding days of the week (and relative times like 'tomorrow') prevented trends from tweets like 'I hate Mondays' and 'So glad its Friday'. \n",
      "      2. The baseline script does not produce metrics for bigrams. As a result, to compute the TS and TF-IDF scores for bigrams we use the average of the metrics from the baseline corpus. Fortunately the average metrics are low (since there are lots of words that only appear a few times compared to common words). This means that an unknown term (like a bigram or a new word) must have a high term frequency in the recent tweets to appear as a trending topic. This is a decent compromise with an acceptable result. A better approach might be to include bigrams in the baseline (making it significantly larger).\n",
      "   2. The top trends mostly make sense. Searching Google News for these terms produces meaningful articles. As before, we see that Paul Walker, Cyber Monday, sporting events and teams, and the Amazon Drone program are making news.\n",
      "   3. As before, local results (e.g. the Long Beach Horror Convention) become trending topics when the locations are restricted."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.3.2 Twitter Trends Discussion\n",
      "\n",
      "Overall, these trending topics are qualitatively better than those provided by Twitter in that we can generate more topics and we are not including hash tags. The topics that people discuss on Twitter cannot be helped, so we are stuck with the celebrity gossip and sports trends. Perhaps having access to more data or doing a different analysis (not based on term frequency) would extract different information, but it does not appear as though Twitter is used as a signal for more serious news unless it is a large event (e.g. a national election or natural disaster)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.4 Wikipedia Trend Detection\n",
      "\n",
      "In this section we explore another source for trending topics - Wikipedia. Given that this site allows anyone to edit articles, and that articles exist for important events and people, we believe that wiki access patterns can indicate trending topics. For example, after the news of Paul Walker's death is made public, we would expect to see more traffic to his wiki page (along with access to the Fast and the Furious movie pages). Fortunately, Wikipedia has an open API with some useful information. This leaves us two options:\n",
      "\n",
      "   1. Use recent changes (edits) to the wiki to determine trends. Hotter topics will likely have more edits, and like word frequency in Twitter, we should see spikes for newsworthy pages. There are a few problems with using edits. First, the total number of edits over a short period of tiem does not necessarily indicate a popular page. Some editors commit many small changes that could throw off this metric. Second, a relatively small number of people edit WIkipedia. The vast majority just read, and this metric will not capture that. Ideally, we would like to find articles that have some discussion or elevated level of collaboration, indicating that multiple people are interested in editing the page. \n",
      "   2. Use page views as a metric for page popularity and look for spikes in viewership. An approach similar to TF-IDF could be used to determine when a page is spiking in popularity."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.4.1 Using Recent Changes\n",
      "\n",
      "Wikipedia's API (http://www.mediawiki.org/wiki/API:Main_page) allows a client to query the list of recent changes over a given time period. The API for recent changes is quite flexible (https://www.mediawiki.org/wiki/API:Recentchanges), allowing different properties to be queried in various language wikis. Let's explore the form of the results.\n",
      "\n",
      "The following code requests the changes to wiki pages for the English site over the past twelve hours, excluding administrative pages (e.g. Talk and User), minor edits, bot edits, and non-edits (e.g. new pages, deletions). The results shown are from December 3, 2013 12:00 PM EDT."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bymin = []\n",
      "\n",
      "# can only get 500 changes per query, so the time period cannot be that long\n",
      "now = int(time.time())\n",
      "mins = range(now, now - (3600 * 12), -60)\n",
      "ranges = zip(mins, mins[1:])\n",
      "\n",
      "for start,end in ranges:\n",
      "    options = {'format': 'json', \n",
      "               'action': 'query', \n",
      "               'list': 'recentchanges', \n",
      "               'rctype': 'edit',\n",
      "               'rcshow': '!minor|!bot',\n",
      "               'rcnamespace': 0,\n",
      "               'rcprop': 'timestamp|title|ids|flags|sizes|user',\n",
      "               'rclimit': 500,\n",
      "               'rcstart': '%d' % start,\n",
      "               'rcend': '%d' % end}\n",
      "\n",
      "    r = requests.get(\"http://en.wikipedia.org/w/api.php\", params=options)\n",
      "    \n",
      "    if r.status_code == 200:\n",
      "        changes = json.loads(r.text)\n",
      "    else:\n",
      "        print \"Error invoking api: %d\" % r.status_code\n",
      "        break\n",
      "    \n",
      "    bymin.append(pd.DataFrame(changes['query']['recentchanges']))\n",
      "    \n",
      "lastday = pd.concat(bymin)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 179
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The form of the JSON returned is as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print json.dumps(changes['query']['recentchanges'][0], indent=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\n",
        "    \"newlen\": 46564, \n",
        "    \"rcid\": 620787333, \n",
        "    \"pageid\": 37982259, \n",
        "    \"title\": \"2013 Seattle Seahawks season\", \n",
        "    \"timestamp\": \"2013-12-03T04:56:44Z\", \n",
        "    \"revid\": 584317455, \n",
        "    \"old_revid\": 584258908, \n",
        "    \"user\": \"SeahawkCountry\", \n",
        "    \"ns\": 0, \n",
        "    \"type\": \"edit\", \n",
        "    \"oldlen\": 46545\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 180
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the length of the article (before and after) to determine if an edit is substantial. In practice, this may not be that helpful (and is not enabled in our collection script, reproduced below)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter out edits that are not substantial\n",
      "lastday['pctchange'] = np.abs((lastday.newlen - lastday.oldlen) * 100. / lastday.oldlen)\n",
      "major = lastday[lastday.pctchange >= 1.]\n",
      "major.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>anon</th>\n",
        "      <th>newlen</th>\n",
        "      <th>ns</th>\n",
        "      <th>old_revid</th>\n",
        "      <th>oldlen</th>\n",
        "      <th>pageid</th>\n",
        "      <th>rcid</th>\n",
        "      <th>revid</th>\n",
        "      <th>timestamp</th>\n",
        "      <th>title</th>\n",
        "      <th>type</th>\n",
        "      <th>user</th>\n",
        "      <th>pctchange</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> NaN</td>\n",
        "      <td>  5341</td>\n",
        "      <td> 0</td>\n",
        "      <td> 578509060</td>\n",
        "      <td>  4686</td>\n",
        "      <td>  3834664</td>\n",
        "      <td> 620877951</td>\n",
        "      <td> 584387116</td>\n",
        "      <td> 2013-12-03T16:54:40Z</td>\n",
        "      <td> Alpena High School (Michigan)</td>\n",
        "      <td> edit</td>\n",
        "      <td>   MrNiceGuy1113</td>\n",
        "      <td> 13.977806</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> NaN</td>\n",
        "      <td>   233</td>\n",
        "      <td> 0</td>\n",
        "      <td> 584386267</td>\n",
        "      <td>   182</td>\n",
        "      <td> 41259338</td>\n",
        "      <td> 620877944</td>\n",
        "      <td> 584387110</td>\n",
        "      <td> 2013-12-03T16:54:39Z</td>\n",
        "      <td>      Indiahoma Public Schools</td>\n",
        "      <td> edit</td>\n",
        "      <td>       Jinkinson</td>\n",
        "      <td> 28.021978</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> NaN</td>\n",
        "      <td>  1350</td>\n",
        "      <td> 0</td>\n",
        "      <td> 576911228</td>\n",
        "      <td>  1318</td>\n",
        "      <td>   298402</td>\n",
        "      <td> 620877943</td>\n",
        "      <td> 584387109</td>\n",
        "      <td> 2013-12-03T16:54:39Z</td>\n",
        "      <td>    Hyderabad (disambiguation)</td>\n",
        "      <td> edit</td>\n",
        "      <td>    Randhirreddy</td>\n",
        "      <td>  2.427921</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>    </td>\n",
        "      <td>  3141</td>\n",
        "      <td> 0</td>\n",
        "      <td> 584386962</td>\n",
        "      <td>  3177</td>\n",
        "      <td> 32844505</td>\n",
        "      <td> 620877939</td>\n",
        "      <td> 584387105</td>\n",
        "      <td> 2013-12-03T16:54:36Z</td>\n",
        "      <td>                  Supertourism</td>\n",
        "      <td> edit</td>\n",
        "      <td>    92.37.123.63</td>\n",
        "      <td>  1.133144</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> NaN</td>\n",
        "      <td> 14424</td>\n",
        "      <td> 0</td>\n",
        "      <td> 570756353</td>\n",
        "      <td> 12804</td>\n",
        "      <td>   129353</td>\n",
        "      <td> 620877922</td>\n",
        "      <td> 584387087</td>\n",
        "      <td> 2013-12-03T16:54:26Z</td>\n",
        "      <td>                Harrison, Ohio</td>\n",
        "      <td> edit</td>\n",
        "      <td> DemocraticLuntz</td>\n",
        "      <td> 12.652296</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 181,
       "text": [
        "   anon  newlen  ns  old_revid  oldlen    pageid       rcid      revid             timestamp                          title  type             user  pctchange\n",
        "2   NaN    5341   0  578509060    4686   3834664  620877951  584387116  2013-12-03T16:54:40Z  Alpena High School (Michigan)  edit    MrNiceGuy1113  13.977806\n",
        "8   NaN     233   0  584386267     182  41259338  620877944  584387110  2013-12-03T16:54:39Z       Indiahoma Public Schools  edit        Jinkinson  28.021978\n",
        "9   NaN    1350   0  576911228    1318    298402  620877943  584387109  2013-12-03T16:54:39Z     Hyderabad (disambiguation)  edit     Randhirreddy   2.427921\n",
        "11         3141   0  584386962    3177  32844505  620877939  584387105  2013-12-03T16:54:36Z                   Supertourism  edit     92.37.123.63   1.133144\n",
        "20  NaN   14424   0  570756353   12804    129353  620877922  584387087  2013-12-03T16:54:26Z                 Harrison, Ohio  edit  DemocraticLuntz  12.652296"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In addition, we can determine which articles have more discussion/collaboration and use that information as a sorting criteria. The number of collaborators is determined by the number of unique editors (including anonymous editors) that contribute to an article. This can help filter out articles that have many changes due to a single author making multiple small changes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nchanges = pd.DataFrame(lastday.groupby('pageid').size(), columns= ['nchanges'])\n",
      "nchanges['title'] = [t[0] for t in lastday.groupby('pageid')['title'].unique()]\n",
      "\n",
      "# look for contentious articles\n",
      "nchanges['neditors'] = lastday.groupby('pageid').user.nunique()\n",
      "nchanges = nchanges[nchanges.neditors >= 7]\n",
      "\n",
      "nchanges.sort('nchanges', ascending=False, inplace=True)\n",
      "nchanges.head(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>nchanges</th>\n",
        "      <th>title</th>\n",
        "      <th>neditors</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pageid</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>15326144</th>\n",
        "      <td> 63</td>\n",
        "      <td>                                   Neymar</td>\n",
        "      <td> 49</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41181297</th>\n",
        "      <td> 43</td>\n",
        "      <td>                               Euromaidan</td>\n",
        "      <td> 12</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3771977 </th>\n",
        "      <td> 43</td>\n",
        "      <td>                              STAR Sports</td>\n",
        "      <td> 14</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41238127</th>\n",
        "      <td> 37</td>\n",
        "      <td>                   Quark (cryptocurrency)</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20004766</th>\n",
        "      <td> 24</td>\n",
        "      <td>                  Tangible symbol systems</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>38053955</th>\n",
        "      <td> 22</td>\n",
        "      <td>                           Deaths in 2013</td>\n",
        "      <td> 17</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>913782  </th>\n",
        "      <td> 21</td>\n",
        "      <td>                            Brian Griffin</td>\n",
        "      <td> 10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>36831635</th>\n",
        "      <td> 21</td>\n",
        "      <td>                          Miss Earth 2013</td>\n",
        "      <td>  9</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1073152 </th>\n",
        "      <td> 20</td>\n",
        "      <td>                                    Clara</td>\n",
        "      <td>  9</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5230750 </th>\n",
        "      <td> 19</td>\n",
        "      <td>                          English exonyms</td>\n",
        "      <td> 10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8355    </th>\n",
        "      <td> 18</td>\n",
        "      <td>                               December 3</td>\n",
        "      <td> 11</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8052761 </th>\n",
        "      <td> 15</td>\n",
        "      <td>       List of best-selling game consoles</td>\n",
        "      <td>  8</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41226911</th>\n",
        "      <td> 14</td>\n",
        "      <td>            2013 Glasgow helicopter crash</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>36825285</th>\n",
        "      <td> 13</td>\n",
        "      <td>              List of Tamil films of 2013</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41034170</th>\n",
        "      <td> 12</td>\n",
        "      <td> Croatian constitutional referendum, 2013</td>\n",
        "      <td>  8</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12094780</th>\n",
        "      <td> 10</td>\n",
        "      <td>                           Tetragrammaton</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>898745  </th>\n",
        "      <td> 10</td>\n",
        "      <td>                            Carlos Finlay</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41257756</th>\n",
        "      <td> 10</td>\n",
        "      <td>            Artrave: The Artpop Ball Tour</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>34164547</th>\n",
        "      <td>  8</td>\n",
        "      <td>                       Frozen (2013 film)</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39922676</th>\n",
        "      <td>  7</td>\n",
        "      <td>               2014 Malaysia Super League</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 185,
       "text": [
        "          nchanges                                     title  neditors\n",
        "pageid                                                                \n",
        "15326144        63                                    Neymar        49\n",
        "41181297        43                                Euromaidan        12\n",
        "3771977         43                               STAR Sports        14\n",
        "41238127        37                    Quark (cryptocurrency)         7\n",
        "20004766        24                   Tangible symbol systems         7\n",
        "38053955        22                            Deaths in 2013        17\n",
        "913782          21                             Brian Griffin        10\n",
        "36831635        21                           Miss Earth 2013         9\n",
        "1073152         20                                     Clara         9\n",
        "5230750         19                           English exonyms        10\n",
        "8355            18                                December 3        11\n",
        "8052761         15        List of best-selling game consoles         8\n",
        "41226911        14             2013 Glasgow helicopter crash         7\n",
        "36825285        13               List of Tamil films of 2013         7\n",
        "41034170        12  Croatian constitutional referendum, 2013         8\n",
        "12094780        10                            Tetragrammaton         7\n",
        "898745          10                             Carlos Finlay         7\n",
        "41257756        10             Artrave: The Artpop Ball Tour         7\n",
        "34164547         8                        Frozen (2013 film)         7\n",
        "39922676         7                2014 Malaysia Super League         7"
       ]
      }
     ],
     "prompt_number": 185
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These results are encouraging. There are a number of events and topics that are not identified by Twitter users (e.g. helicopter crash, Euromaidan, constiutional referendum) but also some popular culture items (e.g. Brian Griffin, Miss Earth 2013). If we add these topics to those mined from Twitter, we can get more breadth.\n",
      "\n",
      "This is the approach we take for detecting trends from Wikipedia. The code we use, which runs every 30 minutes, is provided in the <tt>backend</tt> directory and is reproduced below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thecode = open(\"../backend/wiki_keywords.py\").read()\n",
      "thehtml = highlight(thecode, PythonLexer(), HtmlFormatter())\n",
      "HTML(thehtml)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">re</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"kn\">as</span> <span class=\"nn\">pd</span>\n",
        "\n",
        "<span class=\"n\">SENTENCE_DELIM</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"s\">u&#39;[.!?,;:</span><span class=\"se\">\\t\\\\</span><span class=\"s\">-</span><span class=\"se\">\\\\</span><span class=\"s\">&quot;</span><span class=\"se\">\\\\</span><span class=\"s\">(</span><span class=\"se\">\\\\</span><span class=\"s\">)</span><span class=\"se\">\\u2019\\u2013</span><span class=\"s\">]&#39;</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">extract_terms</span><span class=\"p\">(</span><span class=\"n\">tweet</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">global</span> <span class=\"n\">stoplist</span>\n",
        "    <span class=\"n\">unigrams</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "    <span class=\"n\">bigrams</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "    <span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"n\">SENTENCE_DELIM</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">tweet</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">())</span>\n",
        "    \n",
        "    <span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n",
        "        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">sentence</span><span class=\"p\">:</span> \n",
        "            <span class=\"k\">continue</span>\n",
        "        \n",
        "        <span class=\"c\"># unigrams</span>\n",
        "        <span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">&quot;&#39;s&quot;</span><span class=\"p\">,</span> <span class=\"s\">&#39;&#39;</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">sentence</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()]</span>\n",
        "        <span class=\"n\">unigrams</span><span class=\"o\">.</span><span class=\"n\">extend</span><span class=\"p\">([</span><span class=\"n\">w</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">words</span> <span class=\"k\">if</span> <span class=\"n\">w</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">stoplist</span><span class=\"p\">])</span>\n",
        "        \n",
        "        <span class=\"c\"># bigrams</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">w1</span><span class=\"p\">,</span><span class=\"n\">w2</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">words</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]):</span>\n",
        "            <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"ow\">not</span> <span class=\"n\">w1</span> <span class=\"ow\">in</span> <span class=\"n\">stoplist</span><span class=\"p\">)</span> <span class=\"ow\">and</span> <span class=\"p\">(</span><span class=\"ow\">not</span> <span class=\"n\">w2</span> <span class=\"ow\">in</span> <span class=\"n\">stoplist</span><span class=\"p\">):</span>\n",
        "                <span class=\"n\">bigrams</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"s\">&quot;</span><span class=\"si\">%s</span><span class=\"s\"> </span><span class=\"si\">%s</span><span class=\"s\">&quot;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">w1</span><span class=\"p\">,</span> <span class=\"n\">w2</span><span class=\"p\">))</span>\n",
        "            \n",
        "    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">unigrams</span><span class=\"p\">,</span> <span class=\"n\">bigrams</span><span class=\"p\">)</span>\n",
        "\n",
        "\n",
        "<span class=\"n\">stoplistfile</span> <span class=\"o\">=</span> <span class=\"s\">&#39;wiki_stoplist.txt&#39;</span>\n",
        "    \n",
        "<span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">stoplistfile</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&quot;MISSING STOPLIST!&quot;</span>\n",
        "    <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">exit</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"n\">stoplist</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">()</span>\n",
        "<span class=\"n\">fp</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">stoplistfile</span><span class=\"p\">,</span> <span class=\"s\">&#39;r&#39;</span><span class=\"p\">)</span>\n",
        "<span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">readlines</span><span class=\"p\">():</span>\n",
        "    <span class=\"n\">stoplist</span> <span class=\"o\">|=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">rstrip</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">())</span>\n",
        "<span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n",
        "\n",
        "\n",
        "<span class=\"c\"># can only gt 500 changes per query, so shorten time period </span>\n",
        "<span class=\"c\"># to one minute and iterate over desired period.</span>\n",
        "<span class=\"n\">period</span> <span class=\"o\">=</span> <span class=\"mi\">3600</span> <span class=\"o\">*</span> <span class=\"mi\">12</span> <span class=\"c\"># one day</span>\n",
        "<span class=\"n\">now</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">())</span>\n",
        "<span class=\"n\">mins</span> <span class=\"o\">=</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">now</span><span class=\"p\">,</span> <span class=\"n\">now</span> <span class=\"o\">-</span> <span class=\"n\">period</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">60</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">ranges</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">mins</span><span class=\"p\">,</span> <span class=\"n\">mins</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:])</span>\n",
        "<span class=\"n\">bymin</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "\n",
        "<span class=\"k\">for</span> <span class=\"n\">start</span><span class=\"p\">,</span><span class=\"n\">end</span> <span class=\"ow\">in</span> <span class=\"n\">ranges</span><span class=\"p\">:</span>\n",
        "    <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">&#39;format&#39;</span><span class=\"p\">:</span> <span class=\"s\">&#39;json&#39;</span><span class=\"p\">,</span> \n",
        "               <span class=\"s\">&#39;action&#39;</span><span class=\"p\">:</span> <span class=\"s\">&#39;query&#39;</span><span class=\"p\">,</span> \n",
        "               <span class=\"s\">&#39;list&#39;</span><span class=\"p\">:</span> <span class=\"s\">&#39;recentchanges&#39;</span><span class=\"p\">,</span> \n",
        "               <span class=\"s\">&#39;rctype&#39;</span><span class=\"p\">:</span> <span class=\"s\">&#39;edit&#39;</span><span class=\"p\">,</span>\n",
        "               <span class=\"s\">&#39;rcshow&#39;</span><span class=\"p\">:</span> <span class=\"s\">&#39;!minor|!bot&#39;</span><span class=\"p\">,</span>\n",
        "               <span class=\"s\">&#39;rcnamespace&#39;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n",
        "               <span class=\"s\">&#39;rcprop&#39;</span><span class=\"p\">:</span> <span class=\"s\">&#39;timestamp|title|ids|flags|sizes|user&#39;</span><span class=\"p\">,</span>\n",
        "               <span class=\"s\">&#39;rclimit&#39;</span><span class=\"p\">:</span> <span class=\"mi\">500</span><span class=\"p\">,</span>\n",
        "               <span class=\"s\">&#39;rcstart&#39;</span><span class=\"p\">:</span> <span class=\"s\">&#39;</span><span class=\"si\">%d</span><span class=\"s\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">start</span><span class=\"p\">,</span>\n",
        "               <span class=\"s\">&#39;rcend&#39;</span><span class=\"p\">:</span> <span class=\"s\">&#39;</span><span class=\"si\">%d</span><span class=\"s\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">end</span><span class=\"p\">}</span>\n",
        "\n",
        "    <span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">&quot;http://en.wikipedia.org/w/api.php&quot;</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"n\">options</span><span class=\"p\">)</span>\n",
        "    \n",
        "    <span class=\"k\">if</span> <span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">status_code</span> <span class=\"o\">==</span> <span class=\"mi\">200</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">changes</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
        "        <span class=\"k\">print</span> <span class=\"s\">&quot;Error invoking api: </span><span class=\"si\">%d</span><span class=\"s\">&quot;</span> <span class=\"o\">%</span> <span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">status_code</span>\n",
        "        <span class=\"k\">break</span>\n",
        "    \n",
        "    <span class=\"n\">bymin</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">changes</span><span class=\"p\">[</span><span class=\"s\">&#39;query&#39;</span><span class=\"p\">][</span><span class=\"s\">&#39;recentchanges&#39;</span><span class=\"p\">]))</span>\n",
        "    \n",
        "<span class=\"n\">recent</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">concat</span><span class=\"p\">(</span><span class=\"n\">bymin</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># filter out edits that are not substantial</span>\n",
        "<span class=\"c\">#recent[&#39;pctchange&#39;] = np.abs((recent.newlen - recent.oldlen) * 100. / recent.oldlen)</span>\n",
        "<span class=\"c\">#recent = recent[lastday.pctchange &gt;= 1.]</span>\n",
        "\n",
        "<span class=\"n\">nchanges</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">recent</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s\">&#39;pageid&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">(),</span> <span class=\"n\">columns</span><span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">&#39;nchanges&#39;</span><span class=\"p\">])</span>\n",
        "<span class=\"n\">nchanges</span><span class=\"p\">[</span><span class=\"s\">&#39;title&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">recent</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s\">&#39;pageid&#39;</span><span class=\"p\">)[</span><span class=\"s\">&#39;title&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">unique</span><span class=\"p\">()]</span>\n",
        "\n",
        "<span class=\"c\"># look for contentious articles</span>\n",
        "<span class=\"n\">nchanges</span><span class=\"p\">[</span><span class=\"s\">&#39;neditors&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">recent</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s\">&#39;pageid&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">user</span><span class=\"o\">.</span><span class=\"n\">nunique</span><span class=\"p\">()</span>\n",
        "<span class=\"n\">nchanges</span> <span class=\"o\">=</span> <span class=\"n\">nchanges</span><span class=\"p\">[</span><span class=\"n\">nchanges</span><span class=\"o\">.</span><span class=\"n\">neditors</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">5</span><span class=\"p\">]</span>\n",
        "<span class=\"n\">nchanges</span> <span class=\"o\">=</span> <span class=\"n\">nchanges</span><span class=\"o\">.</span><span class=\"n\">sort</span><span class=\"p\">(</span><span class=\"s\">&#39;nchanges&#39;</span><span class=\"p\">,</span> <span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)[:</span><span class=\"mi\">50</span><span class=\"p\">]</span>\n",
        "\n",
        "<span class=\"n\">keywords</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">()</span>\n",
        "<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">nchanges</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">:</span>\n",
        "    <span class=\"n\">uni</span><span class=\"p\">,</span><span class=\"n\">bi</span> <span class=\"o\">=</span> <span class=\"n\">extract_terms</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">keywords</span> <span class=\"o\">|=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">uni</span> <span class=\"o\">+</span> <span class=\"n\">bi</span><span class=\"p\">)</span>\n",
        "    \n",
        "<span class=\"c\"># try to promote bigrams by removing the unigrams that comprise them</span>\n",
        "<span class=\"n\">keywords</span> <span class=\"o\">-=</span> <span class=\"nb\">set</span><span class=\"p\">([</span><span class=\"n\">s</span> <span class=\"k\">for</span> <span class=\"n\">sublist</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">keywords</span> <span class=\"k\">if</span> <span class=\"s\">&quot; &quot;</span> <span class=\"ow\">in</span> <span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">sublist</span><span class=\"p\">])</span>\n",
        "\n",
        "<span class=\"k\">print</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">([</span><span class=\"n\">k</span> <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">keywords</span><span class=\"p\">])</span>\n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "<IPython.core.display.HTML at 0x10f5f83d0>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output of this process (from 4:00 PM EDT on December 2, 2013) is shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('data/wikipedia_keywords.json', 'r') as fp:\n",
      "    wiki_kw = json.load(fp)\n",
      "    print json.dumps(wiki_kw, indent=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[\n",
        "    \"star sports\", \n",
        "    \"2013 film\", \n",
        "    \"deaths\", \n",
        "    \"2013 ukraine\", \n",
        "    \"selangor\", \n",
        "    \"william burges\", \n",
        "    \"tom daley\", \n",
        "    \"walking dead\", \n",
        "    \"doctor\", \n",
        "    \"paul walker\", \n",
        "    \"ukraine pro\", \n",
        "    \"maria callas\", \n",
        "    \"cyber monday\", \n",
        "    \"steve sarkisian\", \n",
        "    \"european union\", \n",
        "    \"union protests\", \n",
        "    \"frozen\", \n",
        "    \"maria bello\", \n",
        "    \"football association\"\n",
        "]\n"
       ]
      }
     ],
     "prompt_number": 187
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.4.2 Using Page Views\n",
      "\n",
      "We quickly explore the possibility of using Wikipedia page views to determine trending topics. Page view statistics are not available via the API, however, they are made available as hourly data dumps (http://dumps.wikimedia.org). We download one such dump from 4:00 PM EDT on December 2, 2013 (not included due to its size and avilability) and explore the data within. The format is described at http://dumps.wikimedia.org/other/pagecounts-raw."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import urllib\n",
      "\n",
      "# urllib.urlretrieve('http://dumps.wikimedia.org/other/pagecounts-raw/2013/2013-12/pagecounts-20131202-190002.gz', \\\n",
      "#                    'data/pagecounts-20131202-190002.gz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 192
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "\n",
      "fp = gzip.open('data/pagecounts-20131202-190002.gz', 'rb')\n",
      "\n",
      "en_views = []\n",
      "for line in fp:\n",
      "    parts = line.rstrip().split()\n",
      "    if parts[0].lower() == 'en':\n",
      "        en_views.append((parts[1], parts[2]))\n",
      "        \n",
      "fp.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 194
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "viewed = pd.DataFrame(en_views, columns=['title', 'count'])\n",
      "viewed['count'] = viewed['count'].astype(int)\n",
      "viewed.sort('count', ascending=False, inplace=True)\n",
      "viewed = viewed[~viewed['title'].str.contains('Main_Page|Special:|Talk:|User:|File:|Media:|index.html|undefined|Undefined')]\n",
      "viewed.head(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>title</th>\n",
        "      <th>count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>1306288</th>\n",
        "      <td>                             Maria_Callas</td>\n",
        "      <td> 125799</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1506885</th>\n",
        "      <td>                              Paul_Walker</td>\n",
        "      <td>  47816</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>308467 </th>\n",
        "      <td>                            Carlos_Finlay</td>\n",
        "      <td>  26893</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>415043 </th>\n",
        "      <td> Climatic_Research_Unit_email_controversy</td>\n",
        "      <td>   8525</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1577508</th>\n",
        "      <td>                                Pyrolysis</td>\n",
        "      <td>   7859</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2040165</th>\n",
        "      <td>                                Tom_Daley</td>\n",
        "      <td>   6861</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2014249</th>\n",
        "      <td>             The_Walking_Dead_(TV_series)</td>\n",
        "      <td>   6607</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1079097</th>\n",
        "      <td>                                     Java</td>\n",
        "      <td>   5270</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1306267</th>\n",
        "      <td>                              Maria_Bello</td>\n",
        "      <td>   5209</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>881241 </th>\n",
        "      <td>                                        G</td>\n",
        "      <td>   5054</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2014265</th>\n",
        "      <td>              The_Walking_Dead_(season_4)</td>\n",
        "      <td>   4673</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>238159 </th>\n",
        "      <td>                                  Bitcoin</td>\n",
        "      <td>   4374</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1548863</th>\n",
        "      <td>                       Porsche_Carrera_GT</td>\n",
        "      <td>   4347</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>603678 </th>\n",
        "      <td>                       Fast_%26_Furious_7</td>\n",
        "      <td>   4067</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1498820</th>\n",
        "      <td>                               Parrotfish</td>\n",
        "      <td>   3941</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>460668 </th>\n",
        "      <td>                             Cyber_Monday</td>\n",
        "      <td>   3813</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>157144 </th>\n",
        "      <td>                             Armaan_Kohli</td>\n",
        "      <td>   3811</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>598185 </th>\n",
        "      <td>                                 Facebook</td>\n",
        "      <td>   3619</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>921743 </th>\n",
        "      <td>                              Goo_(album)</td>\n",
        "      <td>   3550</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1987717</th>\n",
        "      <td>                 The_Fast_and_the_Furious</td>\n",
        "      <td>   3381</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 200,
       "text": [
        "                                            title   count\n",
        "1306288                              Maria_Callas  125799\n",
        "1506885                               Paul_Walker   47816\n",
        "308467                              Carlos_Finlay   26893\n",
        "415043   Climatic_Research_Unit_email_controversy    8525\n",
        "1577508                                 Pyrolysis    7859\n",
        "2040165                                 Tom_Daley    6861\n",
        "2014249              The_Walking_Dead_(TV_series)    6607\n",
        "1079097                                      Java    5270\n",
        "1306267                               Maria_Bello    5209\n",
        "881241                                          G    5054\n",
        "2014265               The_Walking_Dead_(season_4)    4673\n",
        "238159                                    Bitcoin    4374\n",
        "1548863                        Porsche_Carrera_GT    4347\n",
        "603678                         Fast_%26_Furious_7    4067\n",
        "1498820                                Parrotfish    3941\n",
        "460668                               Cyber_Monday    3813\n",
        "157144                               Armaan_Kohli    3811\n",
        "598185                                   Facebook    3619\n",
        "921743                                Goo_(album)    3550\n",
        "1987717                  The_Fast_and_the_Furious    3381"
       ]
      }
     ],
     "prompt_number": 200
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wikipedia page view frequency, without any TF-IDF analysis, is already promising. It identifies several of the familiar topics (Paul Walker's death) in addition to some not yet seen (Bitcoin). It is possible to do a TF-IDF computation if a baseline is built from the historical page view data. We leave this as future work, having generated enough topics as a proof-of-concept."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 3 - Identifying Themes in Scraped Stories"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've fed a number of documents into our collection at this point. We would like to be able to look and see if there are any themes of trending stories in these collected stories."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "# Ignore depreciation warnings (living in the present with this course!)\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
      "                        module=\"pandas\", lineno=570)\n",
      "\n",
      "\n",
      "base_df = pd.read_csv('cnn.csv')\n",
      "base_df.head(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>published</th>\n",
        "      <th>title</th>\n",
        "      <th>link</th>\n",
        "      <th>content</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Mon, 18 Nov 2013 21:30:12 EST</td>\n",
        "      <td> Is that an addict standing next to you?</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/OG...</td>\n",
        "      <td> &lt;em&gt;&lt;strong&gt;Editor's note:&lt;/strong&gt; Patrick R....</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> Mon, 18 Nov 2013 20:24:21 EST</td>\n",
        "      <td>    Warren vs. Clinton is a false choice</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/iB...</td>\n",
        "      <td> &lt;em&gt;&lt;strong&gt;Editor's note:&lt;/strong&gt; &lt;a href=\"h...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 122,
       "text": [
        "                       published                                    title  \\\n",
        "0  Mon, 18 Nov 2013 21:30:12 EST  Is that an addict standing next to you?   \n",
        "1  Mon, 18 Nov 2013 20:24:21 EST     Warren vs. Clinton is a false choice   \n",
        "\n",
        "                                                link  \\\n",
        "0  http://rss.cnn.com/~r/rss/cnn_topstories/~3/OG...   \n",
        "1  http://rss.cnn.com/~r/rss/cnn_topstories/~3/iB...   \n",
        "\n",
        "                                             content  \n",
        "0  <em><strong>Editor's note:</strong> Patrick R....  \n",
        "1  <em><strong>Editor's note:</strong> <a href=\"h...  "
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.1 Data cleaning\n",
      "\n",
      "The first thing we want to do is clean our data. Currently the 'content' column has HTML tags that are likely to get in the way when looking for themes. We want to remove these."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Some code to remove HTML tags from content\n",
      "Source: http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
      "'''\n",
      "\n",
      "from HTMLParser import HTMLParser\n",
      "\n",
      "class MLStripper(HTMLParser):\n",
      "    def __init__(self):\n",
      "        self.reset()\n",
      "        self.fed = []\n",
      "    def handle_data(self, d):\n",
      "        self.fed.append(d)\n",
      "    def get_data(self):\n",
      "        return ''.join(self.fed)\n",
      "\n",
      "def strip_tags(html):\n",
      "    s = MLStripper()\n",
      "    s.feed(html)\n",
      "    return s.get_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_tags_from_df(df):\n",
      "    \"\"\"\n",
      "    Takes a dataframe and removes the tags from the 'content' column of each row\n",
      "    Returns treated dataframe\n",
      "    \"\"\"\n",
      "    content = []\n",
      "    for c in df.content:\n",
      "        content.append(strip_tags(c))\n",
      "    df.content = content\n",
      "    return df\n",
      "base_df = strip_tags_from_df(base_df)\n",
      "base_df.head() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>published</th>\n",
        "      <th>title</th>\n",
        "      <th>link</th>\n",
        "      <th>content</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 2013-11-18T21:30:12.000000-0500</td>\n",
        "      <td> Is that an addict standing next to you?</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/OG...</td>\n",
        "      <td> Editor's note: Patrick R. Krill is an attorney...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2013-11-18T20:24:21.000000-0500</td>\n",
        "      <td>    Warren vs. Clinton is a false choice</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/iB...</td>\n",
        "      <td> Editor's note: Julian Zelizer is a professor o...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2013-11-18T20:58:16.000000-0500</td>\n",
        "      <td>                     Obamacare's fixable</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/5g...</td>\n",
        "      <td> Editor's note: Paul Begala, a Democratic strat...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 2013-11-18T22:10:30.000000-0500</td>\n",
        "      <td>     Ken Burns: Memorize Lincoln's words</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/Cc...</td>\n",
        "      <td> Editor's note: Ken Burns has been making films...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 2013-11-18T16:11:41.000000-0500</td>\n",
        "      <td>        Ted Turner wants to go to heaven</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/RA...</td>\n",
        "      <td> Watch \"Ted Turner: The Maverick Man\" on Tuesda...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 277,
       "text": [
        "                         published                                    title  \\\n",
        "0  2013-11-18T21:30:12.000000-0500  Is that an addict standing next to you?   \n",
        "1  2013-11-18T20:24:21.000000-0500     Warren vs. Clinton is a false choice   \n",
        "2  2013-11-18T20:58:16.000000-0500                      Obamacare's fixable   \n",
        "3  2013-11-18T22:10:30.000000-0500      Ken Burns: Memorize Lincoln's words   \n",
        "4  2013-11-18T16:11:41.000000-0500         Ted Turner wants to go to heaven   \n",
        "\n",
        "                                                link  \\\n",
        "0  http://rss.cnn.com/~r/rss/cnn_topstories/~3/OG...   \n",
        "1  http://rss.cnn.com/~r/rss/cnn_topstories/~3/iB...   \n",
        "2  http://rss.cnn.com/~r/rss/cnn_topstories/~3/5g...   \n",
        "3  http://rss.cnn.com/~r/rss/cnn_topstories/~3/Cc...   \n",
        "4  http://rss.cnn.com/~r/rss/cnn_topstories/~3/RA...   \n",
        "\n",
        "                                             content  \n",
        "0  Editor's note: Patrick R. Krill is an attorney...  \n",
        "1  Editor's note: Julian Zelizer is a professor o...  \n",
        "2  Editor's note: Paul Begala, a Democratic strat...  \n",
        "3  Editor's note: Ken Burns has been making films...  \n",
        "4  Watch \"Ted Turner: The Maverick Man\" on Tuesda...  "
       ]
      }
     ],
     "prompt_number": 277
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.2 Finding key themes\n",
      "\n",
      "We want to look through the content of recent stories published and find the themes that were not occuring previously. Let's start off by taking a bag of words for all recent stories and identifying the key words for these as a whole."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "import operator\n",
      "\n",
      "def create_bag_of_words(df):\n",
      "    \"\"\"\n",
      "    Takes a dataframe and returns a bag of words for all stories\n",
      "    \"\"\"\n",
      "    text = df.content\n",
      "    vectorizer = CountVectorizer(min_df=0)\n",
      "    vectorizer.fit(text)\n",
      "    x = vectorizer.transform(text)\n",
      "    x = x.toarray()\n",
      "    return vectorizer, x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def word_frequency(w, t):\n",
      "    \"\"\"\n",
      "    Takes words and transform and returns an array of tuples with words and their frequency \n",
      "    \"\"\"\n",
      "    words_count = {}\n",
      "    feature_names = w.get_feature_names()\n",
      "    for n in range(len(feature_names)):\n",
      "        c = 0\n",
      "        for w in range(len(t)):\n",
      "            c += t[w][n]\n",
      "        words_count[feature_names[n]] = c\n",
      "    return sorted(words_count.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
      "\n",
      "def find_word_frequencies(df):\n",
      "    words, transform = create_bag_of_words(df)\n",
      "    return word_frequency(words, transform)\n",
      "\n",
      "frequencies = find_word_frequencies(base_df)\n",
      "print frequencies[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'the', 22380)\n"
       ]
      }
     ],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Most common words in stories:\\n\\nCount           Word\\n\", \"-\"*30\n",
      "for w in frequencies[0:5]:\n",
      "    print w[0].ljust(15), w[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most common words in stories:\n",
        "\n",
        "Count           Word\n",
        "------------------------------\n",
        "the             22380\n",
        "to              11932\n",
        "and             11241\n",
        "of              10593\n",
        "in              7945\n"
       ]
      }
     ],
     "prompt_number": 245
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To make our lives easier let's create a dataframe to contain this data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_df = pd.DataFrame([n[0] for n in frequencies], columns=['word'])\n",
      "words_df['freq'] = [n[1] for n in frequencies]\n",
      "words_df.set_index(\"word\", inplace=True)\n",
      "words_df['pct'] = words_df.freq / float(np.sum(words_df.freq)) * 100\n",
      "words_df['pct'] /= np.max(words_df['pct'])\n",
      "\n",
      "words_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>freq</th>\n",
        "      <th>pct</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>word</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>the</th>\n",
        "      <td> 22380</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>to</th>\n",
        "      <td> 11932</td>\n",
        "      <td> 0.533155</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>and</th>\n",
        "      <td> 11241</td>\n",
        "      <td> 0.502279</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>of</th>\n",
        "      <td> 10593</td>\n",
        "      <td> 0.473324</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>in</th>\n",
        "      <td>  7945</td>\n",
        "      <td> 0.355004</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 246,
       "text": [
        "       freq       pct\n",
        "word                 \n",
        "the   22380  1.000000\n",
        "to    11932  0.533155\n",
        "and   11241  0.502279\n",
        "of    10593  0.473324\n",
        "in     7945  0.355004"
       ]
      }
     ],
     "prompt_number": 246
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This provides us with the count and percentage for all stories. Now we want to find the percentages for only the most recent words, and find the largest changes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "current = np.datetime64(datetime.datetime.now())\n",
      "import time\n",
      "import pytz\n",
      "\n",
      "filtered_df = base_df.copy()\n",
      "\n",
      "# Let's only consider the past 14 days\n",
      "recent_period = current - np.timedelta64(14, 'D')\n",
      "month_numbers = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
      "                 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
      "\n",
      "# Need to convert date format to comparative type\n",
      "def change_to_timedelta64(s):\n",
      "    if len(s) > 1:\n",
      "        return False\n",
      "    year   = int(s[12:16])\n",
      "    month  = s[8:11]\n",
      "    day    = int(s[5:7])\n",
      "    hour   = int(s[17:19])\n",
      "    minute = int(s[20:22])\n",
      "    second = int(s[23:25])\n",
      "    tz     = s[26:29]\n",
      "    if tz == 'EDT':\n",
      "        tz = 'US/Eastern'\n",
      "    zone = pytz.timezone(tz)\n",
      "    month_num = month_numbers[month]\n",
      "    time = datetime.datetime(year, month_num, day, hour, minute, second, tzinfo=zone)\n",
      "    return np.datetime64(time)\n",
      "\n",
      "if not isinstance(filtered_df.published[0], np.datetime64):\n",
      "    times = []\n",
      "    for p in filtered_df.published:\n",
      "        times.append(change_to_timedelta64(p))\n",
      "    filtered_df.published = times\n",
      "\n",
      "filtered_df = base_df.sort(columns=\"published\", ascending=False)\n",
      "filtered_df = filtered_df[filtered_df.published < recent_period]\n",
      "filtered_df.head()\n",
      "print base_df.shape\n",
      "print filtered_df.shape\n",
      "filtered_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(675, 4)\n",
        "(305, 4)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>published</th>\n",
        "      <th>title</th>\n",
        "      <th>link</th>\n",
        "      <th>content</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>172</th>\n",
        "      <td> 2013-11-14T16:48:31.000000-0500</td>\n",
        "      <td>         The health care fix won't work</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_us/~3/-5RdmVq7S9...</td>\n",
        "      <td> Editor's note: Aaron E. Carroll is a professor...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>164</th>\n",
        "      <td> 2013-11-14T14:26:09.000000-0500</td>\n",
        "      <td>       Study: Ancient dogs domesticated</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_us/~3/OsiPhQtnEu...</td>\n",
        "      <td> (CNN) -- Every dog has its day, and scientists...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>173</th>\n",
        "      <td> 2013-11-14T14:25:13.000000-0500</td>\n",
        "      <td>           Obama 'frustrated and angry'</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_us/~3/EhK8BMQSUB...</td>\n",
        "      <td> (CNN) -- As the story of the Obamacare website...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>433</th>\n",
        "      <td> 2013-11-14T14:09:11.000000-0500</td>\n",
        "      <td> TSA defends behavior detection program</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_travel/~3/zNrSvM...</td>\n",
        "      <td> Washington (CNN) -- Longer lines at airport ch...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>284</th>\n",
        "      <td> 2013-11-14T11:48:13.000000-0500</td>\n",
        "      <td>             PlayStation 4 vs. Xbox One</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_tech/~3/NW61_fBr...</td>\n",
        "      <td> (CNN) -- The last time a new PlayStation or Xb...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 278,
       "text": [
        "                           published                                   title  \\\n",
        "172  2013-11-14T16:48:31.000000-0500          The health care fix won't work   \n",
        "164  2013-11-14T14:26:09.000000-0500        Study: Ancient dogs domesticated   \n",
        "173  2013-11-14T14:25:13.000000-0500            Obama 'frustrated and angry'   \n",
        "433  2013-11-14T14:09:11.000000-0500  TSA defends behavior detection program   \n",
        "284  2013-11-14T11:48:13.000000-0500              PlayStation 4 vs. Xbox One   \n",
        "\n",
        "                                                  link  \\\n",
        "172  http://rss.cnn.com/~r/rss/cnn_us/~3/-5RdmVq7S9...   \n",
        "164  http://rss.cnn.com/~r/rss/cnn_us/~3/OsiPhQtnEu...   \n",
        "173  http://rss.cnn.com/~r/rss/cnn_us/~3/EhK8BMQSUB...   \n",
        "433  http://rss.cnn.com/~r/rss/cnn_travel/~3/zNrSvM...   \n",
        "284  http://rss.cnn.com/~r/rss/cnn_tech/~3/NW61_fBr...   \n",
        "\n",
        "                                               content  \n",
        "172  Editor's note: Aaron E. Carroll is a professor...  \n",
        "164  (CNN) -- Every dog has its day, and scientists...  \n",
        "173  (CNN) -- As the story of the Obamacare website...  \n",
        "433  Washington (CNN) -- Longer lines at airport ch...  \n",
        "284  (CNN) -- The last time a new PlayStation or Xb...  "
       ]
      }
     ],
     "prompt_number": 278
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frequencies = find_word_frequencies(filtered_df)\n",
      "print frequencies[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'the', 10627)\n"
       ]
      }
     ],
     "prompt_number": 279
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = []\n",
      "words = [w[0] for w in frequencies]\n",
      "for w in words_df.index:\n",
      "    if w in words:\n",
      "        pos = [i for i,x in enumerate(words) if x == w]\n",
      "        count = frequencies[pos[0]][1]\n",
      "        counts.append(count)\n",
      "    else:\n",
      "        counts.append(0)\n",
      "\n",
      "words_df['recent_freq'] = counts\n",
      "\n",
      "words_df['recent_pct'] = words_df.recent_freq / float(np.sum(words_df.recent_freq)) * 100\n",
      "words_df['recent_pct'] /= np.max(words_df['recent_pct'])\n",
      "words_df['pct_change'] = words_df['recent_pct'] - words_df['pct']\n",
      "words_df['pct_inc'] = words_df['recent_pct']/words_df['pct']\n",
      "\n",
      "words_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>freq</th>\n",
        "      <th>pct</th>\n",
        "      <th>recent_freq</th>\n",
        "      <th>recent_pct</th>\n",
        "      <th>pct_change</th>\n",
        "      <th>pct_inc</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>word</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>the</th>\n",
        "      <td> 22380</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 10627</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>to</th>\n",
        "      <td> 11932</td>\n",
        "      <td> 0.533155</td>\n",
        "      <td>  5976</td>\n",
        "      <td> 0.562341</td>\n",
        "      <td> 0.029187</td>\n",
        "      <td> 1.054743</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>and</th>\n",
        "      <td> 11241</td>\n",
        "      <td> 0.502279</td>\n",
        "      <td>  5622</td>\n",
        "      <td> 0.529030</td>\n",
        "      <td> 0.026751</td>\n",
        "      <td> 1.053259</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>of</th>\n",
        "      <td> 10593</td>\n",
        "      <td> 0.473324</td>\n",
        "      <td>  5130</td>\n",
        "      <td> 0.482733</td>\n",
        "      <td> 0.009408</td>\n",
        "      <td> 1.019877</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>in</th>\n",
        "      <td>  7945</td>\n",
        "      <td> 0.355004</td>\n",
        "      <td>  3693</td>\n",
        "      <td> 0.347511</td>\n",
        "      <td>-0.007493</td>\n",
        "      <td> 0.978892</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 282,
       "text": [
        "       freq       pct  recent_freq  recent_pct  pct_change   pct_inc\n",
        "word                                                                \n",
        "the   22380  1.000000        10627    1.000000    0.000000  1.000000\n",
        "to    11932  0.533155         5976    0.562341    0.029187  1.054743\n",
        "and   11241  0.502279         5622    0.529030    0.026751  1.053259\n",
        "of    10593  0.473324         5130    0.482733    0.009408  1.019877\n",
        "in     7945  0.355004         3693    0.347511   -0.007493  0.978892"
       ]
      }
     ],
     "prompt_number": 282
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_df[words_df.freq>20].sort(columns=['pct_inc', 'pct'], ascending=False).head(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>freq</th>\n",
        "      <th>pct</th>\n",
        "      <th>recent_freq</th>\n",
        "      <th>recent_pct</th>\n",
        "      <th>pct_change</th>\n",
        "      <th>pct_inc</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>word</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>aeg</th>\n",
        "      <td> 40</td>\n",
        "      <td> 0.001787</td>\n",
        "      <td> 40</td>\n",
        "      <td> 0.003764</td>\n",
        "      <td> 0.001977</td>\n",
        "      <td> 2.105957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>belly</th>\n",
        "      <td> 34</td>\n",
        "      <td> 0.001519</td>\n",
        "      <td> 34</td>\n",
        "      <td> 0.003199</td>\n",
        "      <td> 0.001680</td>\n",
        "      <td> 2.105957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>leisure</th>\n",
        "      <td> 28</td>\n",
        "      <td> 0.001251</td>\n",
        "      <td> 28</td>\n",
        "      <td> 0.002635</td>\n",
        "      <td> 0.001384</td>\n",
        "      <td> 2.105957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>interesting</th>\n",
        "      <td> 23</td>\n",
        "      <td> 0.001028</td>\n",
        "      <td> 23</td>\n",
        "      <td> 0.002164</td>\n",
        "      <td> 0.001137</td>\n",
        "      <td> 2.105957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>conklin</th>\n",
        "      <td> 23</td>\n",
        "      <td> 0.001028</td>\n",
        "      <td> 23</td>\n",
        "      <td> 0.002164</td>\n",
        "      <td> 0.001137</td>\n",
        "      <td> 2.105957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bullied</th>\n",
        "      <td> 22</td>\n",
        "      <td> 0.000983</td>\n",
        "      <td> 22</td>\n",
        "      <td> 0.002070</td>\n",
        "      <td> 0.001087</td>\n",
        "      <td> 2.105957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>poly</th>\n",
        "      <td> 24</td>\n",
        "      <td> 0.001072</td>\n",
        "      <td> 24</td>\n",
        "      <td> 0.002258</td>\n",
        "      <td> 0.001186</td>\n",
        "      <td> 2.105957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>penguin</th>\n",
        "      <td> 24</td>\n",
        "      <td> 0.001072</td>\n",
        "      <td> 24</td>\n",
        "      <td> 0.002258</td>\n",
        "      <td> 0.001186</td>\n",
        "      <td> 2.105957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>alexis</th>\n",
        "      <td> 62</td>\n",
        "      <td> 0.002770</td>\n",
        "      <td> 60</td>\n",
        "      <td> 0.005646</td>\n",
        "      <td> 0.002876</td>\n",
        "      <td> 2.038022</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>knight</th>\n",
        "      <td> 27</td>\n",
        "      <td> 0.001206</td>\n",
        "      <td> 26</td>\n",
        "      <td> 0.002447</td>\n",
        "      <td> 0.001240</td>\n",
        "      <td> 2.027958</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mullins</th>\n",
        "      <td> 26</td>\n",
        "      <td> 0.001162</td>\n",
        "      <td> 25</td>\n",
        "      <td> 0.002352</td>\n",
        "      <td> 0.001191</td>\n",
        "      <td> 2.024958</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>yosemite</th>\n",
        "      <td> 22</td>\n",
        "      <td> 0.000983</td>\n",
        "      <td> 21</td>\n",
        "      <td> 0.001976</td>\n",
        "      <td> 0.000993</td>\n",
        "      <td> 2.010231</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bully</th>\n",
        "      <td> 22</td>\n",
        "      <td> 0.000983</td>\n",
        "      <td> 21</td>\n",
        "      <td> 0.001976</td>\n",
        "      <td> 0.000993</td>\n",
        "      <td> 2.010231</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mermaid</th>\n",
        "      <td> 21</td>\n",
        "      <td> 0.000938</td>\n",
        "      <td> 20</td>\n",
        "      <td> 0.001882</td>\n",
        "      <td> 0.000944</td>\n",
        "      <td> 2.005673</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>morsy</th>\n",
        "      <td> 41</td>\n",
        "      <td> 0.001832</td>\n",
        "      <td> 39</td>\n",
        "      <td> 0.003670</td>\n",
        "      <td> 0.001838</td>\n",
        "      <td> 2.003227</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>murray</th>\n",
        "      <td> 39</td>\n",
        "      <td> 0.001743</td>\n",
        "      <td> 37</td>\n",
        "      <td> 0.003482</td>\n",
        "      <td> 0.001739</td>\n",
        "      <td> 1.997959</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>polyamory</th>\n",
        "      <td> 33</td>\n",
        "      <td> 0.001475</td>\n",
        "      <td> 31</td>\n",
        "      <td> 0.002917</td>\n",
        "      <td> 0.001443</td>\n",
        "      <td> 1.978323</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bike</th>\n",
        "      <td> 32</td>\n",
        "      <td> 0.001430</td>\n",
        "      <td> 30</td>\n",
        "      <td> 0.002823</td>\n",
        "      <td> 0.001393</td>\n",
        "      <td> 1.974334</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stanton</th>\n",
        "      <td> 25</td>\n",
        "      <td> 0.001117</td>\n",
        "      <td> 23</td>\n",
        "      <td> 0.002164</td>\n",
        "      <td> 0.001047</td>\n",
        "      <td> 1.937480</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>motorcycle</th>\n",
        "      <td> 25</td>\n",
        "      <td> 0.001117</td>\n",
        "      <td> 23</td>\n",
        "      <td> 0.002164</td>\n",
        "      <td> 0.001047</td>\n",
        "      <td> 1.937480</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 294,
       "text": [
        "             freq       pct  recent_freq  recent_pct  pct_change   pct_inc\n",
        "word                                                                      \n",
        "aeg            40  0.001787           40    0.003764    0.001977  2.105957\n",
        "belly          34  0.001519           34    0.003199    0.001680  2.105957\n",
        "leisure        28  0.001251           28    0.002635    0.001384  2.105957\n",
        "interesting    23  0.001028           23    0.002164    0.001137  2.105957\n",
        "conklin        23  0.001028           23    0.002164    0.001137  2.105957\n",
        "bullied        22  0.000983           22    0.002070    0.001087  2.105957\n",
        "poly           24  0.001072           24    0.002258    0.001186  2.105957\n",
        "penguin        24  0.001072           24    0.002258    0.001186  2.105957\n",
        "alexis         62  0.002770           60    0.005646    0.002876  2.038022\n",
        "knight         27  0.001206           26    0.002447    0.001240  2.027958\n",
        "mullins        26  0.001162           25    0.002352    0.001191  2.024958\n",
        "yosemite       22  0.000983           21    0.001976    0.000993  2.010231\n",
        "bully          22  0.000983           21    0.001976    0.000993  2.010231\n",
        "mermaid        21  0.000938           20    0.001882    0.000944  2.005673\n",
        "morsy          41  0.001832           39    0.003670    0.001838  2.003227\n",
        "murray         39  0.001743           37    0.003482    0.001739  1.997959\n",
        "polyamory      33  0.001475           31    0.002917    0.001443  1.978323\n",
        "bike           32  0.001430           30    0.002823    0.001393  1.974334\n",
        "stanton        25  0.001117           23    0.002164    0.001047  1.937480\n",
        "motorcycle     25  0.001117           23    0.002164    0.001047  1.937480"
       ]
      }
     ],
     "prompt_number": 294
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notably, the first of these words that we've found have only been used in the latest stories. It's hard though to see if there is actually any theme behind these."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.3 Making code generic\n",
      "\n",
      "Below is a version of the code which has been made generic to run on the server."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pygments import highlight\n",
      "from pygments.lexers import PythonLexer\n",
      "from pygments.formatters import HtmlFormatter\n",
      "from IPython.display import HTML\n",
      "import urllib\n",
      "\n",
      "thecode = open(\"themes_exploration.py\").read()\n",
      "thehtml=highlight(thecode, PythonLexer(), HtmlFormatter())\n",
      "HTML(thehtml)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"kn\">as</span> <span class=\"nn\">pd</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">pytz</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_extraction.text</span> <span class=\"kn\">import</span> <span class=\"n\">CountVectorizer</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">operator</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">warnings</span>\n",
        "<span class=\"kn\">from</span> <span class=\"nn\">HTMLParser</span> <span class=\"kn\">import</span> <span class=\"n\">HTMLParser</span>\n",
        "\n",
        "<span class=\"c\"># Ignore depreciation warnings (living in the present with this course!)</span>\n",
        "<span class=\"n\">warnings</span><span class=\"o\">.</span><span class=\"n\">filterwarnings</span><span class=\"p\">(</span><span class=\"s\">&quot;ignore&quot;</span><span class=\"p\">,</span> <span class=\"n\">category</span><span class=\"o\">=</span><span class=\"ne\">DeprecationWarning</span><span class=\"p\">,</span>\n",
        "                        <span class=\"n\">module</span><span class=\"o\">=</span><span class=\"s\">&quot;pandas&quot;</span><span class=\"p\">,</span> <span class=\"n\">lineno</span><span class=\"o\">=</span><span class=\"mi\">570</span><span class=\"p\">)</span>\n",
        "\n",
        "\n",
        "<span class=\"n\">base_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">&#39;cnn.csv&#39;</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">base_df</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"sd\">&#39;&#39;&#39;</span>\n",
        "<span class=\"sd\">Some code to remove HTML tags from content</span>\n",
        "<span class=\"sd\">Source: http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python</span>\n",
        "<span class=\"sd\">&#39;&#39;&#39;</span>\n",
        "<span class=\"k\">class</span> <span class=\"nc\">MLStripper</span><span class=\"p\">(</span><span class=\"n\">HTMLParser</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fed</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">handle_data</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">):</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fed</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">get_data</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
        "        <span class=\"k\">return</span> <span class=\"s\">&#39;&#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fed</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">strip_tags</span><span class=\"p\">(</span><span class=\"n\">html</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">MLStripper</span><span class=\"p\">()</span>\n",
        "    <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">feed</span><span class=\"p\">(</span><span class=\"n\">html</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">get_data</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">strip_tags_from_df</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&quot;&quot;&quot;</span>\n",
        "<span class=\"sd\">    Takes a dataframe and removes the tags from the &#39;content&#39; column of each row</span>\n",
        "<span class=\"sd\">    Returns treated dataframe</span>\n",
        "<span class=\"sd\">    &quot;&quot;&quot;</span>\n",
        "    <span class=\"n\">content</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">content</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">strip_tags</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">))</span>\n",
        "    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">content</span> <span class=\"o\">=</span> <span class=\"n\">content</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">df</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">create_bag_of_words</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&quot;&quot;&quot;</span>\n",
        "<span class=\"sd\">    Takes a dataframe and returns a bag of words for all stories</span>\n",
        "<span class=\"sd\">    &quot;&quot;&quot;</span>\n",
        "    <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">content</span>\n",
        "    <span class=\"n\">vectorizer</span> <span class=\"o\">=</span> <span class=\"n\">CountVectorizer</span><span class=\"p\">(</span><span class=\"n\">min_df</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">vectorizer</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">vectorizer</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">toarray</span><span class=\"p\">()</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">vectorizer</span><span class=\"p\">,</span> <span class=\"n\">x</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">word_frequency</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&quot;&quot;&quot;</span>\n",
        "<span class=\"sd\">    Takes words and transform and returns an array of tuples with words and their frequency </span>\n",
        "<span class=\"sd\">    &quot;&quot;&quot;</span>\n",
        "    <span class=\"n\">words_count</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n",
        "    <span class=\"n\">feature_names</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">get_feature_names</span><span class=\"p\">()</span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">feature_names</span><span class=\"p\">)):</span>\n",
        "        <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)):</span>\n",
        "            <span class=\"n\">c</span> <span class=\"o\">+=</span> <span class=\"n\">t</span><span class=\"p\">[</span><span class=\"n\">w</span><span class=\"p\">][</span><span class=\"n\">n</span><span class=\"p\">]</span>\n",
        "        <span class=\"n\">words_count</span><span class=\"p\">[</span><span class=\"n\">feature_names</span><span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"n\">c</span>\n",
        "    <span class=\"k\">return</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">words_count</span><span class=\"o\">.</span><span class=\"n\">iteritems</span><span class=\"p\">(),</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">itemgetter</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">find_word_frequencies</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">create_bag_of_words</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">word_frequency</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">change_to_timedelta64</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">):</span>\n",
        "\t<span class=\"c\"># Convert date format to comparative type (text to date)</span>\n",
        "    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n",
        "        <span class=\"k\">return</span> <span class=\"bp\">False</span>\n",
        "    <span class=\"n\">year</span>   <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">12</span><span class=\"p\">:</span><span class=\"mi\">16</span><span class=\"p\">])</span>\n",
        "    <span class=\"n\">month</span>  <span class=\"o\">=</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">:</span><span class=\"mi\">11</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">day</span>    <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">:</span><span class=\"mi\">7</span><span class=\"p\">])</span>\n",
        "    <span class=\"n\">hour</span>   <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">17</span><span class=\"p\">:</span><span class=\"mi\">19</span><span class=\"p\">])</span>\n",
        "    <span class=\"n\">minute</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">20</span><span class=\"p\">:</span><span class=\"mi\">22</span><span class=\"p\">])</span>\n",
        "    <span class=\"n\">second</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">23</span><span class=\"p\">:</span><span class=\"mi\">25</span><span class=\"p\">])</span>\n",
        "    <span class=\"n\">tz</span>     <span class=\"o\">=</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"mi\">26</span><span class=\"p\">:</span><span class=\"mi\">29</span><span class=\"p\">]</span>\n",
        "    <span class=\"k\">if</span> <span class=\"n\">tz</span> <span class=\"o\">==</span> <span class=\"s\">&#39;EDT&#39;</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">tz</span> <span class=\"o\">=</span> <span class=\"s\">&#39;US/Eastern&#39;</span>\n",
        "    <span class=\"n\">zone</span> <span class=\"o\">=</span> <span class=\"n\">pytz</span><span class=\"o\">.</span><span class=\"n\">timezone</span><span class=\"p\">(</span><span class=\"n\">tz</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">month_num</span> <span class=\"o\">=</span> <span class=\"n\">month_numbers</span><span class=\"p\">[</span><span class=\"n\">month</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">datetime</span><span class=\"p\">(</span><span class=\"n\">year</span><span class=\"p\">,</span> <span class=\"n\">month_num</span><span class=\"p\">,</span> <span class=\"n\">day</span><span class=\"p\">,</span> <span class=\"n\">hour</span><span class=\"p\">,</span> <span class=\"n\">minute</span><span class=\"p\">,</span> <span class=\"n\">second</span><span class=\"p\">,</span> <span class=\"n\">tzinfo</span><span class=\"o\">=</span><span class=\"n\">zone</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">datetime64</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"p\">)</span>\n",
        "\n",
        "\n",
        "<span class=\"n\">recent_period</span> <span class=\"o\">=</span> <span class=\"n\">current</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">timedelta64</span><span class=\"p\">(</span><span class=\"mi\">14</span><span class=\"p\">,</span> <span class=\"s\">&#39;D&#39;</span><span class=\"p\">)</span> <span class=\"c\"># Period we&#39;re considering</span>\n",
        "<span class=\"n\">month_numbers</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">&#39;Jan&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s\">&#39;Feb&#39;</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s\">&#39;Mar&#39;</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s\">&#39;Apr&#39;</span><span class=\"p\">:</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s\">&#39;May&#39;</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s\">&#39;Jun&#39;</span><span class=\"p\">:</span> <span class=\"mi\">6</span><span class=\"p\">,</span>\n",
        "                 <span class=\"s\">&#39;Jul&#39;</span><span class=\"p\">:</span> <span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"s\">&#39;Aug&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"s\">&#39;Sep&#39;</span><span class=\"p\">:</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"s\">&#39;Oct&#39;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"s\">&#39;Nov&#39;</span><span class=\"p\">:</span> <span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"s\">&#39;Dec&#39;</span><span class=\"p\">:</span> <span class=\"mi\">12</span><span class=\"p\">}</span> <span class=\"c\"># Months dict</span>\n",
        "<span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">datetime64</span><span class=\"p\">(</span><span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">now</span><span class=\"p\">())</span> <span class=\"c\"># Current date</span>\n",
        "\n",
        "<span class=\"c\"># Remove tags and find word frequencies</span>\n",
        "<span class=\"n\">base_df</span> <span class=\"o\">=</span> <span class=\"n\">strip_tags_from_df</span><span class=\"p\">(</span><span class=\"n\">base_df</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">frequencies</span> <span class=\"o\">=</span> <span class=\"n\">find_word_frequencies</span><span class=\"p\">(</span><span class=\"n\">base_df</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># Make a dataframe with frequencies and calculate percentage of word usage</span>\n",
        "<span class=\"n\">words_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">([</span><span class=\"n\">n</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"n\">frequencies</span><span class=\"p\">],</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">&#39;word&#39;</span><span class=\"p\">])</span>\n",
        "<span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;freq&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"n\">frequencies</span><span class=\"p\">]</span>\n",
        "<span class=\"n\">words_df</span><span class=\"o\">.</span><span class=\"n\">set_index</span><span class=\"p\">(</span><span class=\"s\">&quot;word&quot;</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;pct&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">words_df</span><span class=\"o\">.</span><span class=\"n\">freq</span> <span class=\"o\">/</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">words_df</span><span class=\"o\">.</span><span class=\"n\">freq</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"mi\">100</span>\n",
        "<span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;pct&#39;</span><span class=\"p\">]</span> <span class=\"o\">/=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;pct&#39;</span><span class=\"p\">])</span>\n",
        "\n",
        "<span class=\"c\"># Make a second df where we&#39;ll keep only the most recent stories</span>\n",
        "<span class=\"n\">filtered_df</span> <span class=\"o\">=</span> <span class=\"n\">base_df</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n",
        "\n",
        "<span class=\"c\"># Convert dates from string to dates (if it isn&#39;t already datetime_64 format)</span>\n",
        "<span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">filtered_df</span><span class=\"o\">.</span><span class=\"n\">published</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">datetime64</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">times</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "    <span class=\"k\">for</span> <span class=\"n\">p</span> <span class=\"ow\">in</span> <span class=\"n\">filtered_df</span><span class=\"o\">.</span><span class=\"n\">published</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">times</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">change_to_timedelta64</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">))</span>\n",
        "    <span class=\"n\">filtered_df</span><span class=\"o\">.</span><span class=\"n\">published</span> <span class=\"o\">=</span> <span class=\"n\">times</span>\n",
        "\n",
        "<span class=\"c\"># Filter the dataframe</span>\n",
        "<span class=\"n\">filtered_df</span> <span class=\"o\">=</span> <span class=\"n\">base_df</span><span class=\"o\">.</span><span class=\"n\">sort</span><span class=\"p\">(</span><span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"s\">&quot;published&quot;</span><span class=\"p\">,</span> <span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n",
        "<span class=\"n\">filtered_df</span> <span class=\"o\">=</span> <span class=\"n\">filtered_df</span><span class=\"p\">[</span><span class=\"n\">filtered_df</span><span class=\"o\">.</span><span class=\"n\">published</span> <span class=\"o\">&lt;</span> <span class=\"n\">recent_period</span><span class=\"p\">]</span>\n",
        "\n",
        "<span class=\"c\"># Find new word frequencies</span>\n",
        "<span class=\"n\">frequencies</span> <span class=\"o\">=</span> <span class=\"n\">find_word_frequencies</span><span class=\"p\">(</span><span class=\"n\">filtered_df</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># Match up word counts with those used in full_df (accounting for ones where count=0)</span>\n",
        "<span class=\"n\">counts</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "<span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">frequencies</span><span class=\"p\">]</span>\n",
        "<span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">words_df</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">:</span>\n",
        "    <span class=\"k\">if</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">words</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">pos</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span><span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">x</span> <span class=\"o\">==</span> <span class=\"n\">w</span><span class=\"p\">]</span>\n",
        "        <span class=\"n\">count</span> <span class=\"o\">=</span> <span class=\"n\">frequencies</span><span class=\"p\">[</span><span class=\"n\">pos</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]][</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
        "        <span class=\"n\">counts</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">count</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
        "        <span class=\"n\">counts</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># Add the column for frequency</span>\n",
        "<span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;recent_freq&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">counts</span>\n",
        "\n",
        "<span class=\"c\"># Calculate percentages and change in percentage</span>\n",
        "<span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;recent_pct&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">words_df</span><span class=\"o\">.</span><span class=\"n\">recent_freq</span> <span class=\"o\">/</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">words_df</span><span class=\"o\">.</span><span class=\"n\">recent_freq</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"mi\">100</span>\n",
        "<span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;recent_pct&#39;</span><span class=\"p\">]</span> <span class=\"o\">/=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;recent_pct&#39;</span><span class=\"p\">])</span>\n",
        "<span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;pct_change&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;recent_pct&#39;</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;pct&#39;</span><span class=\"p\">]</span>\n",
        "<span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;pct_inc&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;recent_pct&#39;</span><span class=\"p\">]</span><span class=\"o\">/</span><span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"s\">&#39;pct&#39;</span><span class=\"p\">]</span>\n",
        "\n",
        "<span class=\"c\"># This is output to use in analysis</span>\n",
        "<span class=\"n\">themes_output</span> <span class=\"o\">=</span> <span class=\"n\">words_df</span><span class=\"p\">[</span><span class=\"n\">words_df</span><span class=\"o\">.</span><span class=\"n\">freq</span><span class=\"o\">&gt;</span><span class=\"mi\">20</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">sort</span><span class=\"p\">(</span><span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">&#39;pct_inc&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;pct&#39;</span><span class=\"p\">],</span> <span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<IPython.core.display.HTML at 0x1066bd490>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 4 - Quantifying Bias"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the main parts of trender.io is figuring out the top stories to display on the webpage. The main purpose of this site is that we want to display the most unbiased news from major sources. This notebook will take news stories from top news sources, like CNN and BBC, and keywords generated from trending news topics and trending social media, like Twitter and Wikipedia, and rank them based on different bias conditions outlined below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import feedparser\n",
      "import json\n",
      "#from future import Future\n",
      "import numpy as np\n",
      "import networkx as nx\n",
      "import requests\n",
      "from pattern import web\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import re\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from collections import defaultdict\n",
      "import json\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib as mpl\n",
      "\n",
      "#colorbrewer2 Dark2 qualitative color table\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'white'\n",
      "rcParams['patch.facecolor'] = dark2_colors[0]\n",
      "rcParams['font.family'] = 'StixGeneral'\n",
      "\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()\n",
      "        \n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.1 Identifying Positive and Negative Words\n",
      "The initial data we worked with was the CNN data pulled in from the CNN RSS feeds. First, we wanted to read in the CNN csv file into a pandas DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read the CSV and get shape info\n",
      "df1=pd.read_csv('cnn.csv')\n",
      "print df1.shape\n",
      "df1.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(675, 4)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>published</th>\n",
        "      <th>title</th>\n",
        "      <th>link</th>\n",
        "      <th>content</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Mon, 18 Nov 2013 21:30:12 EST</td>\n",
        "      <td> Is that an addict standing next to you?</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/OG...</td>\n",
        "      <td> &lt;em&gt;&lt;strong&gt;Editor's note:&lt;/strong&gt; Patrick R....</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> Mon, 18 Nov 2013 20:24:21 EST</td>\n",
        "      <td>    Warren vs. Clinton is a false choice</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/iB...</td>\n",
        "      <td> &lt;em&gt;&lt;strong&gt;Editor's note:&lt;/strong&gt; &lt;a href=\"h...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> Mon, 18 Nov 2013 20:58:16 EST</td>\n",
        "      <td>                     Obamacare's fixable</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/5g...</td>\n",
        "      <td> &lt;em&gt;&lt;strong&gt;Editor's note:&lt;/strong&gt; Paul Begal...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> Mon, 18 Nov 2013 22:10:30 EST</td>\n",
        "      <td>     Ken Burns: Memorize Lincoln's words</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/Cc...</td>\n",
        "      <td> &lt;em&gt;&lt;strong&gt;Editor's note:&lt;/strong&gt; Ken Burns ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> Mon, 18 Nov 2013 16:11:41 EST</td>\n",
        "      <td>        Ted Turner wants to go to heaven</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/RA...</td>\n",
        "      <td> &lt;em&gt;Watch \"Ted Turner: The Maverick Man\" on Tu...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "                       published                                    title                                               link                                            content\n",
        "0  Mon, 18 Nov 2013 21:30:12 EST  Is that an addict standing next to you?  http://rss.cnn.com/~r/rss/cnn_topstories/~3/OG...  <em><strong>Editor's note:</strong> Patrick R....\n",
        "1  Mon, 18 Nov 2013 20:24:21 EST     Warren vs. Clinton is a false choice  http://rss.cnn.com/~r/rss/cnn_topstories/~3/iB...  <em><strong>Editor's note:</strong> <a href=\"h...\n",
        "2  Mon, 18 Nov 2013 20:58:16 EST                      Obamacare's fixable  http://rss.cnn.com/~r/rss/cnn_topstories/~3/5g...  <em><strong>Editor's note:</strong> Paul Begal...\n",
        "3  Mon, 18 Nov 2013 22:10:30 EST      Ken Burns: Memorize Lincoln's words  http://rss.cnn.com/~r/rss/cnn_topstories/~3/Cc...  <em><strong>Editor's note:</strong> Ken Burns ...\n",
        "4  Mon, 18 Nov 2013 16:11:41 EST         Ted Turner wants to go to heaven  http://rss.cnn.com/~r/rss/cnn_topstories/~3/RA...  <em>Watch \"Ted Turner: The Maverick Man\" on Tu..."
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that we have a dataframe with columns published, title, link, and content. We are interested in what the story is actually saying, so we now should look at the content of the news story in order to rank all of them. Now that we have our CNN data, we can try to extract words from each news story. These words will be used to analyze how biased the story is."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_words(df):\n",
      "#Given: a dataframe, we want to extract each word in each story\n",
      "#Returns: list of lists containing words of each story\n",
      "\n",
      "    allwords=[]\n",
      "    for x in df['content']:\n",
      "        words = re.findall(r'\\b\\S+\\b', x)\n",
      "        allwords.append(words)\n",
      "    return allwords\n",
      "\n",
      "all_story_words = extract_words(df1)\n",
      "all_story_words[0][0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "[\"em><strong>Editor's\",\n",
        " 'note:</strong',\n",
        " 'Patrick',\n",
        " 'R',\n",
        " 'Krill',\n",
        " 'is',\n",
        " 'an',\n",
        " 'attorney',\n",
        " 'clinician',\n",
        " 'and',\n",
        " 'board-certified',\n",
        " 'licensed',\n",
        " 'alcohol',\n",
        " 'and',\n",
        " 'drug',\n",
        " 'counselor',\n",
        " 'He',\n",
        " 'is',\n",
        " 'the',\n",
        " 'director']"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at a plot of the number of words per story just to see how the length of stories vary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_lengths=[]\n",
      "for j in all_story_words:\n",
      "    all_lengths.append(len(j))\n",
      "\n",
      "\n",
      "plt.hist(all_lengths,bins=100,log='x',color='blue')\n",
      "plt.xlabel('number of words')\n",
      "plt.ylabel('frequency')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAGECAYAAAC/E71LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQIdAACA5Qh0AAAAlvt/6pEh8wLykz4AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b0b04d0>"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we want to import the list of positive and negative words in order to compare our news stories. We will determine our main source of news bias based on this. These lists of positive and negative words were generated by Wilson et al. and are from source: http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-3/. We want to import these files and make two lists, poswords and negwords, which contain the positive and negative words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "negtxt='http://www.unc.edu/~ncaren/haphazard/negative.txt'\n",
      "postxt='http://www.unc.edu/~ncaren/haphazard/positive.txt'\n",
      "files=['negative.txt','positive.txt']\n",
      "path='http://www.unc.edu/~ncaren/haphazard/'\n",
      "for file_name in files:\n",
      "    urllib.urlretrieve(path+file_name,file_name)\n",
      "negwords = open(\"negative.txt\").read()\n",
      "negwords = negwords.split('\\n')\n",
      "poswords = open(\"positive.txt\").read()\n",
      "poswords = poswords.split('\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have this list of positive and negative words, we can compare our list of words to these positive and negative words. We will go through each word to see if it is a positive or negative word and at the end, display the total number of positive and negative words for each story. \n",
      "\n",
      "*NOTE: This was before I made a general function that did this for each news site. Now the function analyze_words will do this in the next part.*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Given: all_story_words with list of lists containing every word\n",
      "#Output: Dataframe with two columns: one containing positive words and one containing negative words\n",
      "allneg=[]\n",
      "allpos=[]\n",
      "for a in all_story_words:\n",
      "    negative_words=[]\n",
      "    positive_words=[]\n",
      "    for w in a:\n",
      "        if w in poswords:\n",
      "            positive_words.append(w)\n",
      "        if w in negwords:\n",
      "            negative_words.append(w)\n",
      "    allpos.append([0] if not positive_words else positive_words)\n",
      "    allneg.append([0] if not negative_words else negative_words)\n",
      "posneg=pd.DataFrame({'pos':allpos,'neg':allneg})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Append all the words in a positive and negative list to output to site wordle.net to construct a wordle.\n",
      "pos_list=[]\n",
      "neg_list=[]\n",
      "for p in posneg['pos']:\n",
      "    for p1 in p:\n",
      "        pos_list.append(p1)\n",
      "print pos_list[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['sure', 'boldly', 'might', 'respect', 'might']\n"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have this DataFrame with all the positive and negative words in each story. Let's see how this looks using a wordle:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div align=\"center\">\n",
      "<img src=\"https://trenderio.s3.amazonaws.com/pos_words_new.png\" width=\"800\" />\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we notice that one of the biggest words is 'just'. The interesting part of this is that 'just' can be used as an emotional word to mean 'what is morally right and fair' as well as an adverb meaning 'recently' or 'in this exact manner.' According to Merriam-Webster, just's primary definition is one meaning 'morally right and fair', so we include it here as a positive word even though it can be used in both meanings. The full definitions of 'just' can be found at their website: http://www.merriam-webster.com/dictionary/just.\n",
      "\n",
      "We can also construct a similar word cloud for the negative words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n in posneg['neg']:\n",
      "    for n1 in n:\n",
      "        neg_list.append(n1)\n",
      "print neg_list[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['embattled', 'erratic', 'addiction', 'trying', 'disease']\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div align=\"center\">\n",
      "<img src=\"https://trenderio.s3.amazonaws.com/neg_words_new.png\" width=\"800\" />\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.2 Identifying Bias\n",
      "We are now working on keyword bias and site bias. Now that we practiced with the CNN data, we can now use the stories from all the news sites that we collected. We want to see how many positive and negative words are used per site. In this part, we will construct our whole ranking system and assign a bias to each news story based on the site bias, the news story bias that contains the keyword, the keyword bias among all news sites, and the overall bias of the particular news site we are analyzing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we want to write a simple function that counts the number of positive and negative words in a dataframe. We can use this function later to calculate the bias of each news site. This first function will take the positive and negative words and display:\n",
      "\n",
      "**1)** The number of positive words\n",
      "\n",
      "**2)** The number of negative words\n",
      "\n",
      "**3)** The total number of words\n",
      "\n",
      "**4)** The positive bias (total number positive words divided by total number words)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def number_of_pos_neg(df):\n",
      "    numPos=0\n",
      "    numNeg=0\n",
      "    for l in df['pos']:\n",
      "        numPos=numPos+len(l)\n",
      "    for m in df['neg']:\n",
      "        numNeg=numNeg+len(m)\n",
      "    pos_bias_percentage = 0 if numPos+numNeg==0 else float(numPos)/float(numPos+numNeg)\n",
      "    return 'Number Positive Words ' ,numPos, 'Number Negative Words ' ,numNeg, 'Total', numPos+numNeg,'bias',pos_bias_percentage\n",
      "\n",
      "print number_of_pos_neg(posneg)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Number Positive Words ', 14301, 'Number Negative Words ', 9880, 'Total', 24181, 'bias', 0.5914147471155039)\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a quick look at what is contained in our overall dataframe with all our news stories."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfall=pd.read_csv('stories.csv')\n",
      "print dfall.shape\n",
      "dfall.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1574, 9)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>published</th>\n",
        "      <th>title</th>\n",
        "      <th>link</th>\n",
        "      <th>picture</th>\n",
        "      <th>height</th>\n",
        "      <th>width</th>\n",
        "      <th>description</th>\n",
        "      <th>content</th>\n",
        "      <th>source</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 2013-12-06 22:58:53</td>\n",
        "      <td>      Life with apartheid often brutal</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/DW...</td>\n",
        "      <td> http://i2.cdn.turner.com/cnn/dam/assets/130625...</td>\n",
        "      <td> 360</td>\n",
        "      <td> 640</td>\n",
        "      <td> For South Africans who survived the decades of...</td>\n",
        "      <td> (CNN) -- Ellen Moshweu was just trying to go t...</td>\n",
        "      <td> cnn</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2013-12-07 01:14:23</td>\n",
        "      <td>         Not all comments are positive</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/Z1...</td>\n",
        "      <td> http://i2.cdn.turner.com/cnn/dam/assets/120225...</td>\n",
        "      <td> 120</td>\n",
        "      <td> 214</td>\n",
        "      <td> Within minutes of the news of his death, the b...</td>\n",
        "      <td> Editor's note: Share your stories, memories an...</td>\n",
        "      <td> cnn</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2013-12-07 01:20:28</td>\n",
        "      <td>     Opinion: My first black president</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/tj...</td>\n",
        "      <td> http://i2.cdn.turner.com/cnn/dam/assets/131206...</td>\n",
        "      <td> 360</td>\n",
        "      <td> 640</td>\n",
        "      <td> For Roxanne Jones, traveling to South Africa i...</td>\n",
        "      <td> Editor's note: Roxanne Jones is a founding edi...</td>\n",
        "      <td> cnn</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 2013-12-07 00:52:13</td>\n",
        "      <td>    Amanda Bynes home for the holidays</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/Pl...</td>\n",
        "      <td> http://i2.cdn.turner.com/cnn/dam/assets/130524...</td>\n",
        "      <td> 360</td>\n",
        "      <td> 640</td>\n",
        "      <td> Actress Amanda Bynes \"is excited to be home wi...</td>\n",
        "      <td> Los Angeles (CNN) -- Actress Amanda Bynes \"is ...</td>\n",
        "      <td> cnn</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 2013-12-06 09:54:04</td>\n",
        "      <td> Taking pics with imaginary girlfriend</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/-M...</td>\n",
        "      <td> http://i2.cdn.turner.com/cnn/dam/assets/131204...</td>\n",
        "      <td> 360</td>\n",
        "      <td> 640</td>\n",
        "      <td>                        Rejoice, single travelers.</td>\n",
        "      <td> (CNN) -- Rejoice, single travelers.Traveling a...</td>\n",
        "      <td> cnn</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "             published                                  title                                               link                                            picture  height  width                                        description                                            content source\n",
        "0  2013-12-06 22:58:53       Life with apartheid often brutal  http://rss.cnn.com/~r/rss/cnn_topstories/~3/DW...  http://i2.cdn.turner.com/cnn/dam/assets/130625...     360    640  For South Africans who survived the decades of...  (CNN) -- Ellen Moshweu was just trying to go t...    cnn\n",
        "1  2013-12-07 01:14:23          Not all comments are positive  http://rss.cnn.com/~r/rss/cnn_topstories/~3/Z1...  http://i2.cdn.turner.com/cnn/dam/assets/120225...     120    214  Within minutes of the news of his death, the b...  Editor's note: Share your stories, memories an...    cnn\n",
        "2  2013-12-07 01:20:28      Opinion: My first black president  http://rss.cnn.com/~r/rss/cnn_topstories/~3/tj...  http://i2.cdn.turner.com/cnn/dam/assets/131206...     360    640  For Roxanne Jones, traveling to South Africa i...  Editor's note: Roxanne Jones is a founding edi...    cnn\n",
        "3  2013-12-07 00:52:13     Amanda Bynes home for the holidays  http://rss.cnn.com/~r/rss/cnn_topstories/~3/Pl...  http://i2.cdn.turner.com/cnn/dam/assets/130524...     360    640  Actress Amanda Bynes \"is excited to be home wi...  Los Angeles (CNN) -- Actress Amanda Bynes \"is ...    cnn\n",
        "4  2013-12-06 09:54:04  Taking pics with imaginary girlfriend  http://rss.cnn.com/~r/rss/cnn_topstories/~3/-M...  http://i2.cdn.turner.com/cnn/dam/assets/131204...     360    640                         Rejoice, single travelers.  (CNN) -- Rejoice, single travelers.Traveling a...    cnn"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's pull out all the stories within each news source. To do that, let's call a function that pulls out stories contained in a news source given:\n",
      "\n",
      "1) the dataframe from the csv\n",
      "\n",
      "2) the news source we want\n",
      "\n",
      "It will output:\n",
      "\n",
      "1) all the words in list format of the news stories\n",
      "\n",
      "2) the news stories themselves (found in the 'content' column of the data frame)\n",
      "\n",
      "3) The links to the news sites so that we can access them later"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def news_content(dfall, newssource):\n",
      "    newssource_stories=[]\n",
      "    newssource_links=[]\n",
      "    for ns in dfall['link'].keys():\n",
      "        if newssource in dfall['link'][ns]:\n",
      "            newssource_stories.append(dfall['content'][ns])\n",
      "            newssource_links.append(dfall['link'][ns])\n",
      "\n",
      "    allwords=[]\n",
      "    for s in range(0,len(newssource_stories)):\n",
      "        words = re.findall(r'\\b\\S+\\b', newssource_stories[s])\n",
      "        allwords.append(words)\n",
      "    return allwords,newssource_stories,newssource_links"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want a way to analyze these words to determine the positive and negative words in each news story. This function is identical to what we did above with the CNN news stories, except now we can pass in any list of words from any news site. The function will pass in all the words it found and search through them to find any and all positive and negative words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze_words(allwords):\n",
      "    allneg=[]\n",
      "    allpos=[]\n",
      "    positive_words=[]\n",
      "    negative_words=[]\n",
      "    for article in allwords:\n",
      "        for word in article:\n",
      "            if word in poswords:\n",
      "                positive_words.append(word)\n",
      "            if word in negwords:\n",
      "                negative_words.append(word)\n",
      "        allpos.append([0] if not positive_words else positive_words)\n",
      "        allneg.append([0] if not negative_words else negative_words)\n",
      "        negative_words=[]\n",
      "        positive_words=[]\n",
      "    return pd.DataFrame({'pos':allpos,'neg':allneg})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's test this function and output the number of positive and negative words for each news source that we want to use. To reiterate, we want to find:\n",
      "\n",
      "1) Total number of positive words per news source\n",
      "\n",
      "2) Total number of negative words per news source\n",
      "\n",
      "3) Total number of words\n",
      "\n",
      "4) The positive bias (positive words divided by total words)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BBCwords,BBCcontent,BBClinks = news_content(dfall, \"bbc\")\n",
      "BBCposneg=analyze_words(BBCwords)\n",
      "number_of_pos_neg(BBCposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "('Number Positive Words ',\n",
        " 6217,\n",
        " 'Number Negative Words ',\n",
        " 4959,\n",
        " 'Total',\n",
        " 11176,\n",
        " 'bias',\n",
        " 0.5562813171080888)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CNNwords,CNNcontent,CNNlinks = news_content(dfall, \"cnn\")\n",
      "CNNposneg=analyze_words(CNNwords)\n",
      "number_of_pos_neg(CNNposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "('Number Positive Words ',\n",
        " 8906,\n",
        " 'Number Negative Words ',\n",
        " 7162,\n",
        " 'Total',\n",
        " 16068,\n",
        " 'bias',\n",
        " 0.554269355240229)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NPRwords,NPRcontent,NPRlinks = news_content(dfall, \"npr\")\n",
      "NPRposneg=analyze_words(NPRwords)\n",
      "number_of_pos_neg(NPRposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "('Number Positive Words ',\n",
        " 0,\n",
        " 'Number Negative Words ',\n",
        " 0,\n",
        " 'Total',\n",
        " 0,\n",
        " 'bias',\n",
        " 0)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NYTIMESwords,NYTIMEScontent, NYTIMESlinks = news_content(dfall, \"nytimes\")\n",
      "NYTIMESposneg=analyze_words(NYTIMESwords)\n",
      "number_of_pos_neg(NYTIMESposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "('Number Positive Words ',\n",
        " 3254,\n",
        " 'Number Negative Words ',\n",
        " 2888,\n",
        " 'Total',\n",
        " 6142,\n",
        " 'bias',\n",
        " 0.52979485509606)"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "GUARDIANwords,GUARDIANcontent,GUARDIANlinks = news_content(dfall, \"guardian\")\n",
      "GUARDIANposneg=analyze_words(GUARDIANwords)\n",
      "number_of_pos_neg(GUARDIANposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "('Number Positive Words ',\n",
        " 5280,\n",
        " 'Number Negative Words ',\n",
        " 4241,\n",
        " 'Total',\n",
        " 9521,\n",
        " 'bias',\n",
        " 0.554563596260897)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CNBCwords,CNBCcontent,CNBClinks = news_content(dfall, \"cnbc\")\n",
      "CNBCposneg=analyze_words(CNBCwords)\n",
      "number_of_pos_neg(CNBCposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "('Number Positive Words ',\n",
        " 4368,\n",
        " 'Number Negative Words ',\n",
        " 2789,\n",
        " 'Total',\n",
        " 7157,\n",
        " 'bias',\n",
        " 0.6103115830655302)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NBCwords,NBCcontent,NBClinks = news_content(dfall, \"nbcnews\")\n",
      "NBCposneg=analyze_words(NBCwords)\n",
      "number_of_pos_neg(NBCposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "('Number Positive Words ',\n",
        " 1973,\n",
        " 'Number Negative Words ',\n",
        " 1634,\n",
        " 'Total',\n",
        " 3607,\n",
        " 'bias',\n",
        " 0.5469919600776268)"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ECONOMISTwords,ECONOMISTcontent,ECONOMISTlinks = news_content(dfall, \"economist\")\n",
      "ECONOMISTposneg=analyze_words(ECONOMISTwords)\n",
      "number_of_pos_neg(ECONOMISTposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "('Number Positive Words ',\n",
        " 0,\n",
        " 'Number Negative Words ',\n",
        " 0,\n",
        " 'Total',\n",
        " 0,\n",
        " 'bias',\n",
        " 0)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "WSJwords,WSJcontent,WSJlinks = news_content(dfall, \"wsj\")\n",
      "WSJposneg=analyze_words(WSJwords)\n",
      "number_of_pos_neg(WSJposneg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "('Number Positive Words ',\n",
        " 0,\n",
        " 'Number Negative Words ',\n",
        " 0,\n",
        " 'Total',\n",
        " 0,\n",
        " 'bias',\n",
        " 0)"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the bias for each news source, we need to find the bias of a keyword for a particular news source. If a news source displays too much bias, we do not want to display this news source. We can find this by making a function keyword_bias that takes in a keyword we want and a site. It then should output:\n",
      "\n",
      "1) The keyword\n",
      "\n",
      "2) The site\n",
      "\n",
      "3) The link along with it's individual news story bias\n",
      "\n",
      "4) The total number of positive words for that keyword\n",
      "\n",
      "5) The total number of words\n",
      "\n",
      "6) The positive bias for the whole news site of that keyword. Note that the closer to 0.5 the bias is, the more it is considered unbiased"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NEWSwords = {'BBC':BBCwords,'CNN':CNNwords,'NPR':NPRwords,'NYTIMES':NYTIMESwords,'GUARDIAN':GUARDIANwords,'CNBC':CNBCwords,'NBC':NBCwords,'ECONOMIST':ECONOMISTwords,'WSJ':WSJwords}\n",
      "NEWSposneg = {'BBC': BBCposneg,'CNN': CNNposneg,'NPR': NPRposneg,'NYTIMES':NYTIMESposneg,'GUARDIAN': GUARDIANposneg,'CNBC':CNBCposneg,'NBC':NBCposneg,'ECONOMIST':ECONOMISTposneg,'WSJ':WSJposneg}\n",
      "NEWScontent = {'BBC':BBCcontent,'CNN':CNNcontent,'NPR':NPRcontent,'NYTIMES':NYTIMEScontent,'GUARDIAN':GUARDIANcontent,'CNBC':CNBCcontent,'NBC':NBCcontent,'ECONOMIST':ECONOMISTcontent,'WSJ':WSJcontent}\n",
      "NEWSlinks = {'BBC':BBClinks,'CNN':CNNlinks,'NPR':NPRlinks,'NYTIMES':NYTIMESlinks,'GUARDIAN':GUARDIANlinks,'CNBC':CNBClinks,'NBC':NBClinks,'ECONOMIST':ECONOMISTlinks,'WSJ':WSJlinks}\n",
      "def keyword_bias(keyword,site):\n",
      "    pos=0\n",
      "    neg=0\n",
      "    links = []\n",
      "    links_bias=[]\n",
      "    for o in range(len(NEWSwords[site])):\n",
      "        if keyword in NEWScontent[site][o]:\n",
      "            links.append(NEWSlinks[site][o])\n",
      "            pos=pos+ len(NEWSposneg[site]['pos'][o])\n",
      "            neg=neg+ len(NEWSposneg[site]['neg'][o])\n",
      "            links_bias.append(float(len(NEWSposneg[site]['pos'][o]))/float(len(NEWSposneg[site]['pos'][o])+len(NEWSposneg[site]['neg'][o])))\n",
      "    bias = 0 if pos+neg==0 else float(pos)/float(pos+neg)\n",
      "    return keyword, site, pd.DataFrame({'bias':links_bias,'links':links}), pos, pos+neg,bias\n",
      "    #return {'links':links,'keyword':keyword,'site':site,'pos':pos,'total':pos+neg,'bias':0 if pos+neg==0 else float(pos)/float(pos+neg)}\n",
      "\n",
      "#The closer to 0 bias is, the more unbiased the article is. If bias is positive, news story is positively biased; \n",
      "#negative means it is negatively biased\n",
      "keyword_bias('nuclear weapon','CNN')[2]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>bias</th>\n",
        "      <th>links</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0.657143</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/uZ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0.723577</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/gh...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.593103</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_topstories/~3/Kb...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 0.653465</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_us/~3/eDCqNq3ZA0...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0.500000</td>\n",
        "      <td> http://rss.cnn.com/~r/rss/cnn_us/~3/VqDg6Oplrz...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "       bias                                              links\n",
        "0  0.657143  http://rss.cnn.com/~r/rss/cnn_topstories/~3/uZ...\n",
        "1  0.723577  http://rss.cnn.com/~r/rss/cnn_topstories/~3/gh...\n",
        "2  0.593103  http://rss.cnn.com/~r/rss/cnn_topstories/~3/Kb...\n",
        "3  0.653465  http://rss.cnn.com/~r/rss/cnn_us/~3/eDCqNq3ZA0...\n",
        "4  0.500000  http://rss.cnn.com/~r/rss/cnn_us/~3/VqDg6Oplrz..."
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's find the keyword bias for all the news sites. We just have to go through all our news sites and run keyword_bias for that keyword and the respective news site. This will return all the news links with their biases. Then we want to calculate the total bias for all those links. We do this in all_site_bias, where we input the keyword and print out the total bias for that keyword among all the news sites."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def all_keyword_bias(keyword):\n",
      "    all_data=[]\n",
      "    for site in NEWSwords.keys():\n",
      "        all_data.append(keyword_bias(keyword,site))\n",
      "    return all_data\n",
      "\n",
      "def all_site_bias(keyword):\n",
      "    keyword_site_bias = 0\n",
      "    everything = all_keyword_bias(keyword)\n",
      "    for e in everything:\n",
      "        keyword_site_bias=keyword_site_bias+e[5]\n",
      "    return float(keyword_site_bias)/float(len(everything))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_site_bias('Obama')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "0.5225483690491758"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#buzzwords = ['NSA','storm','abortion','Obama','Christmas','holidays','Thanksgiving','nuclear']\n",
      "#for bw in buzzwords:\n",
      "#    print all_keyword_bias(bw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to be able to identify the news article that we want to show on our website. We are given an overall news bias and a keyword bias per news source. We are also given the bias for that keyword spanning all news sources. We want to display the news stories that have the most minimum bias. Let us calculate the content bias for each link that displays per keyword. \n",
      "\n",
      "Let's start with the bias of a simple news article. If an article seems really biased, we are less likely to show that article even if the news source is unbiased in general. So we should assign a weight to this bias. Let's have the weight be relatively high: 0.4. \n",
      "\n",
      "Now, we want to analyze the bias of the keyword among all news stories in one source. Say, for example, CNN has, in general, a high bias for the word war. Then we do not want to display CNN right away even if the words within the articles are more or less unbiased. So this bias should have a weight as well: 0.3\n",
      "\n",
      "Further, we want to know the general bias of the keyword among all news articles. Right now, we can just add all the bias together for each news article and divide by the number of news articles. Later, I want to implement a bias that takes into account the total number of positive and negative words since a site with 100 total emotional words will tend to be skewed farther than a site with 1000 emotional words. Weight: 0.2\n",
      "\n",
      "Finally, we want the general bias of the news site to be displayed at well. This should be important, but not as important as what we are searching for. Let us make this bias have a weight of 0.1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets first look at the bias of each article given a key word. These biases will be assigned a weight of 0.4:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for y in all_keyword_bias('Obama'):\n",
      "    print y[2]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       bias                                              links\n",
        "0  0.363636  http://feeds.nbcnews.com/c/35002/f/663303/s/34...\n",
        "1  0.456221  http://feeds.nbcnews.com/c/35002/f/663303/s/34...\n",
        "2  0.622222  http://feeds.nbcnews.com/c/35002/f/663303/s/34...\n",
        "3  0.466667  http://feeds.nbcnews.com/c/35002/f/663303/s/34...\n",
        "       bias                             links\n",
        "0  0.684211  http://www.cnbc.com/id/101229035\n",
        "1  0.600000  http://www.cnbc.com/id/101229716\n",
        "2  0.685714  http://www.cnbc.com/id/101229543\n",
        "3  0.687500  http://www.cnbc.com/id/101212051\n",
        "4  0.307692  http://www.cnbc.com/id/101229358\n",
        "5  0.614035  http://www.cnbc.com/id/101227798\n",
        "6  0.365854  http://www.cnbc.com/id/101227549\n",
        "7  0.343750  http://www.cnbc.com/id/101227095\n",
        "        bias                                              links\n",
        "0   0.676471  http://www.nytimes.com/2013/11/27/us/politics/...\n",
        "1   0.648649  http://www.nytimes.com/2013/11/27/us/justices-...\n",
        "2   0.541667  http://thelede.blogs.nytimes.com/2013/11/25/st...\n",
        "3   0.800000  http://www.nytimes.com/2013/11/26/world/asia/o...\n",
        "4   0.532258  http://www.nytimes.com/2013/11/26/world/middle...\n",
        "5   0.635135  http://www.nytimes.com/2013/11/26/business/new...\n",
        "6   0.600000  http://www.nytimes.com/2013/11/26/us/obama-cal...\n",
        "7   0.647059  http://www.nytimes.com/2013/11/26/us/conservat...\n",
        "8   0.500000  http://www.nytimes.com/video/2013/11/25/us/100...\n",
        "9   0.500000  http://www.nytimes.com/2013/11/26/us/id-verifi...\n",
        "10  0.442857  http://thelede.blogs.nytimes.com/2013/11/26/pr...\n",
        "11  0.700855  http://india.blogs.nytimes.com/2013/11/26/a-ne...\n",
        "12  0.547619  http://www.nytimes.com/2013/11/27/world/asia/u...\n",
        "13  0.800000  http://www.nytimes.com/2013/11/26/world/asia/o...\n",
        "14  0.575758  http://india.blogs.nytimes.com/2013/11/26/imag...\n",
        "15  0.476190  http://india.blogs.nytimes.com/2013/11/26/pare...\n",
        "16  0.543478  http://india.blogs.nytimes.com/2013/11/26/a-co...\n",
        "        bias                                              links\n",
        "0   0.657143  http://rss.cnn.com/~r/rss/cnn_topstories/~3/uZ...\n",
        "1   0.527778  http://rss.cnn.com/~r/rss/cnn_topstories/~3/aF...\n",
        "2   0.516129  http://rss.cnn.com/~r/rss/cnn_topstories/~3/YX...\n",
        "3   0.666667  http://rss.cnn.com/~r/rss/cnn_topstories/~3/X6...\n",
        "4   0.500000  http://rss.cnn.com/~r/rss/cnn_topstories/~3/9f...\n",
        "5   0.723577  http://rss.cnn.com/~r/rss/cnn_topstories/~3/gh...\n",
        "6   0.593103  http://rss.cnn.com/~r/rss/cnn_topstories/~3/Kb...\n",
        "7   0.443662  http://rss.cnn.com/~r/rss/cnn_topstories/~3/vV...\n",
        "8   0.516129  http://rss.cnn.com/~r/rss/cnn_topstories/~3/YX...\n",
        "9   0.652174  http://rss.cnn.com/~r/rss/cnn_world/~3/bUhM-kQ...\n",
        "10  0.653465  http://rss.cnn.com/~r/rss/cnn_us/~3/eDCqNq3ZA0...\n",
        "11  0.416667  http://rss.cnn.com/~r/rss/cnn_us/~3/IQPeLyiULZ...\n",
        "12  0.716216  http://rss.cnn.com/~r/rss/cnn_us/~3/6-SO1_ak_d...\n",
        "13  0.720930  http://rss.cnn.com/~r/rss/cnn_us/~3/Y6FBaF07FX...\n",
        "14  0.611940  http://rss.cnn.com/~r/rss/cnn_us/~3/l0rRUmlOVx...\n",
        "15  0.500000  http://rss.cnn.com/~r/rss/cnn_us/~3/VqDg6Oplrz...\n",
        "16  0.631579  http://rss.cnn.com/~r/rss/cnn_us/~3/zXRundpHn2..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       bias                                              links\n",
        "0  0.787879  http://feeds.theguardian.com/c/34708/f/663828/...\n",
        "1  0.593750  http://feeds.theguardian.com/c/34708/f/663828/...\n",
        "2  0.575758  http://feeds.theguardian.com/c/34708/f/663828/...\n",
        "3  0.681818  http://feeds.theguardian.com/c/34708/f/663828/...\n",
        "4  0.787879  http://feeds.theguardian.com/c/34708/f/663829/...\n",
        "5  0.575758  http://feeds.theguardian.com/c/34708/f/663829/...\n",
        "6  0.727273  http://feeds.theguardian.com/c/34708/f/663829/...\n",
        "7  0.804348  http://feeds.theguardian.com/c/34708/f/663829/...\n",
        "8  0.681818  http://feeds.theguardian.com/c/34708/f/663829/...\n",
        "9  0.518987  http://feeds.theguardian.com/c/34708/f/663829/...\n",
        "Empty DataFrame\n",
        "Columns: [bias, links]\n",
        "Index: []\n",
        "       bias                                              links\n",
        "0  0.555556  http://www.bbc.co.uk/news/world-us-canada-2510...\n",
        "1  0.555556  http://www.bbc.co.uk/news/world-us-canada-2510...\n",
        "        bias                                              links\n",
        "0   0.578431  http://www.economist.com/news/world-week/21590...\n",
        "1   0.515873  http://www.economist.com/news/world-week/21590...\n",
        "2   0.560000  http://www.economist.com/news/world-week/21590...\n",
        "3   0.555556  http://www.economist.com/news/united-states/21...\n",
        "4   0.547009  http://www.economist.com/news/united-states/21...\n",
        "5   0.600000  http://www.economist.com/news/united-states/21...\n",
        "6   0.551724  http://www.economist.com/news/united-states/21...\n",
        "7   0.609524  http://www.economist.com/news/united-states/21...\n",
        "8   0.511628  http://www.economist.com/news/united-states/21...\n",
        "9   0.537313  http://www.economist.com/news/united-states/21...\n",
        "10  0.496503  http://www.economist.com/news/united-states/21...\n",
        "11  0.666667  http://www.economist.com/news/united-states/21...\n",
        "12  0.651163  http://www.economist.com/news/united-states/21...\n",
        "13  0.608696  http://www.economist.com/news/united-states/21...\n",
        "14  0.587500  http://www.economist.com/news/united-states/21...\n",
        "15  0.581560  http://www.economist.com/news/united-states/21...\n",
        "16  0.578947  http://www.economist.com/news/united-states/21...\n",
        "17  0.487179  http://www.economist.com/news/united-states/21...\n",
        "18  0.554455  http://www.economist.com/news/united-states/21...\n",
        "19  0.645669  http://www.economist.com/news/united-states/21..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       bias                                              links\n",
        "0  0.862069  http://www.npr.org/blogs/thetwo-way/2013/11/26...\n",
        "1  0.650794  http://www.npr.org/blogs/health/2013/11/26/247...\n",
        "2  0.716981  http://www.npr.org/blogs/parallels/2013/11/26/...\n",
        "3  0.785714  http://www.npr.org/blogs/thetwo-way/2013/11/26...\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's find bias of keyword among news stories in one source. This will have a weight of 0.3:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for y in all_keyword_bias('Obama'):\n",
      "    print keyword_bias('Obama', y[1])[5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.465373961219\n",
        "0.508\n",
        "0.606981981982\n",
        "0.606858054226\n",
        "0.671052631579\n",
        "0\n",
        "0.555555555556\n",
        "0.565842696629\n",
        "0.723270440252\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want the bias of the word in all the news sites. This will have a weight of 0.2:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print all_site_bias('Obama')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.522548369049\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, calculate the site bias. This has a weight of 0.1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for y in NEWSposneg:\n",
      "    print number_of_pos_neg(NEWSposneg[y])[7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.501661129568\n",
        "0.577516233766\n",
        "0.596233521657\n",
        "0.574421965318\n",
        "0.524969046636\n",
        "0.538461538462\n",
        "0.645807259074\n",
        "0.572300648607\n",
        "0.660869565217\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we put it all together! We will multiply each bias with the links by their respective weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_new_bias=[]\n",
      "for y in all_keyword_bias('Obama'):\n",
      "    new_bias = (all_site_bias('Obama')*0.2+y[2]['bias']*0.4+keyword_bias('Obama', y[1])[5]*0.3+0.1*number_of_pos_neg(NEWSposneg[y[1]])[7])\n",
      "    all_new_bias.append([abs(0.5-x) for x in new_bias.values])\n",
      "\n",
      "for n in range(0,9):\n",
      "    df=pd.DataFrame({'old_bias': [abs(0.5-x) for x in all_keyword_bias('Obama')[n][2]['bias']],'links':all_keyword_bias('Obama')[n][2]['links'],'new_bias':all_new_bias[n]})\n",
      "    result = df.sort_index(by=['new_bias'])\n",
      "    print result\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                               links  new_bias  old_bias\n",
        "3  http://feeds.nbcnews.com/c/35002/f/663303/s/34...  0.019045  0.033333\n",
        "1  http://feeds.nbcnews.com/c/35002/f/663303/s/34...  0.023224  0.043779\n",
        "2  http://feeds.nbcnews.com/c/35002/f/663303/s/34...  0.043177  0.122222\n",
        "0  http://feeds.nbcnews.com/c/35002/f/663303/s/34...  0.060257  0.136364\n",
        "                              links  new_bias  old_bias\n",
        "6  http://www.cnbc.com/id/101227549  0.038997  0.134146\n",
        "7  http://www.cnbc.com/id/101227095  0.047839  0.156250\n",
        "1  http://www.cnbc.com/id/101229716  0.054661  0.100000\n",
        "5  http://www.cnbc.com/id/101227798  0.060275  0.114035\n",
        "4  http://www.cnbc.com/id/101229358  0.062262  0.192308\n",
        "0  http://www.cnbc.com/id/101229035  0.088346  0.184211\n",
        "2  http://www.cnbc.com/id/101229543  0.088947  0.185714\n",
        "3  http://www.cnbc.com/id/101212051  0.089661  0.187500\n",
        "                                                links  new_bias  old_bias\n",
        "10  http://thelede.blogs.nytimes.com/2013/11/26/pr...  0.023370  0.057143\n",
        "15  http://india.blogs.nytimes.com/2013/11/26/pare...  0.036704  0.023810\n",
        "8   http://www.nytimes.com/video/2013/11/25/us/100...  0.046228  0.000000\n",
        "9   http://www.nytimes.com/2013/11/26/us/id-verifi...  0.046228  0.000000\n",
        "4   http://www.nytimes.com/2013/11/26/world/middle...  0.059131  0.032258\n",
        "2   http://thelede.blogs.nytimes.com/2013/11/25/st...  0.062894  0.041667\n",
        "16  http://india.blogs.nytimes.com/2013/11/26/a-co...  0.063619  0.043478\n",
        "12  http://www.nytimes.com/2013/11/27/world/asia/u...  0.065275  0.047619\n",
        "14  http://india.blogs.nytimes.com/2013/11/26/imag...  0.076531  0.075758\n",
        "6   http://www.nytimes.com/2013/11/26/us/obama-cal...  0.086228  0.100000\n",
        "5   http://www.nytimes.com/2013/11/26/business/new...  0.100282  0.135135\n",
        "7   http://www.nytimes.com/2013/11/26/us/conservat...  0.105051  0.147059\n",
        "1   http://www.nytimes.com/2013/11/27/us/justices-...  0.105687  0.148649\n",
        "0   http://www.nytimes.com/2013/11/27/us/politics/...  0.116816  0.176471\n",
        "11  http://india.blogs.nytimes.com/2013/11/26/a-ne...  0.126570  0.200855\n",
        "3   http://www.nytimes.com/2013/11/26/world/asia/o...  0.166228  0.300000\n",
        "13  http://www.nytimes.com/2013/11/26/world/asia/o...  0.166228  0.300000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                                                links  new_bias  old_bias\n",
        "11  http://rss.cnn.com/~r/rss/cnn_us/~3/IQPeLyiULZ...  0.010676  0.083333\n",
        "7   http://rss.cnn.com/~r/rss/cnn_topstories/~3/vV...  0.021474  0.056338\n",
        "4   http://rss.cnn.com/~r/rss/cnn_topstories/~3/9f...  0.044009  0.000000\n",
        "15  http://rss.cnn.com/~r/rss/cnn_us/~3/VqDg6Oplrz...  0.044009  0.000000\n",
        "2   http://rss.cnn.com/~r/rss/cnn_topstories/~3/YX...  0.050461  0.016129\n",
        "8   http://rss.cnn.com/~r/rss/cnn_topstories/~3/YX...  0.050461  0.016129\n",
        "1   http://rss.cnn.com/~r/rss/cnn_topstories/~3/aF...  0.055120  0.027778\n",
        "6   http://rss.cnn.com/~r/rss/cnn_topstories/~3/Kb...  0.081251  0.093103\n",
        "14  http://rss.cnn.com/~r/rss/cnn_us/~3/l0rRUmlOVx...  0.088785  0.111940\n",
        "16  http://rss.cnn.com/~r/rss/cnn_us/~3/zXRundpHn2...  0.096641  0.131579\n",
        "9   http://rss.cnn.com/~r/rss/cnn_world/~3/bUhM-kQ...  0.104879  0.152174\n",
        "10  http://rss.cnn.com/~r/rss/cnn_us/~3/eDCqNq3ZA0...  0.105395  0.153465\n",
        "0   http://rss.cnn.com/~r/rss/cnn_topstories/~3/uZ...  0.106866  0.157143\n",
        "3   http://rss.cnn.com/~r/rss/cnn_topstories/~3/X6...  0.110676  0.166667\n",
        "12  http://rss.cnn.com/~r/rss/cnn_us/~3/6-SO1_ak_d...  0.130496  0.216216\n",
        "13  http://rss.cnn.com/~r/rss/cnn_us/~3/Y6FBaF07FX...  0.132381  0.220930\n",
        "5   http://rss.cnn.com/~r/rss/cnn_topstories/~3/gh...  0.133440  0.223577"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                                               links  new_bias  old_bias\n",
        "9  http://feeds.theguardian.com/c/34708/f/663829/...  0.065917  0.018987\n",
        "2  http://feeds.theguardian.com/c/34708/f/663828/...  0.088625  0.075758\n",
        "5  http://feeds.theguardian.com/c/34708/f/663829/...  0.088625  0.075758\n",
        "1  http://feeds.theguardian.com/c/34708/f/663828/...  0.095822  0.093750\n",
        "3  http://feeds.theguardian.com/c/34708/f/663828/...  0.131050  0.181818\n",
        "8  http://feeds.theguardian.com/c/34708/f/663829/...  0.131050  0.181818\n",
        "6  http://feeds.theguardian.com/c/34708/f/663829/...  0.149231  0.227273\n",
        "0  http://feeds.theguardian.com/c/34708/f/663828/...  0.173474  0.287879\n",
        "4  http://feeds.theguardian.com/c/34708/f/663829/...  0.173474  0.287879\n",
        "7  http://feeds.theguardian.com/c/34708/f/663829/...  0.180061  0.304348"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Empty DataFrame\n",
        "Columns: [links, new_bias, old_bias]\n",
        "Index: []\n",
        "                                               links  new_bias  old_bias\n",
        "0  http://www.bbc.co.uk/news/world-us-canada-2510...  0.057979  0.055556\n",
        "1  http://www.bbc.co.uk/news/world-us-canada-2510...  0.057979  0.055556"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                                                links  new_bias  old_bias\n",
        "17  http://www.economist.com/news/united-states/21...  0.026364  0.012821\n",
        "10  http://www.economist.com/news/united-states/21...  0.030094  0.003497\n",
        "8   http://www.economist.com/news/united-states/21...  0.036144  0.011628\n",
        "1   http://www.economist.com/news/world-week/21590...  0.037842  0.015873\n",
        "9   http://www.economist.com/news/united-states/21...  0.046418  0.037313\n",
        "4   http://www.economist.com/news/united-states/21...  0.050296  0.047009\n",
        "6   http://www.economist.com/news/united-states/21...  0.052182  0.051724\n",
        "18  http://www.economist.com/news/united-states/21...  0.053275  0.054455\n",
        "3   http://www.economist.com/news/united-states/21...  0.053715  0.055556\n",
        "2   http://www.economist.com/news/world-week/21590...  0.055493  0.060000\n",
        "0   http://www.economist.com/news/world-week/21590...  0.062865  0.078431\n",
        "16  http://www.economist.com/news/united-states/21...  0.063071  0.078947\n",
        "15  http://www.economist.com/news/united-states/21...  0.064117  0.081560\n",
        "14  http://www.economist.com/news/united-states/21...  0.066493  0.087500\n",
        "5   http://www.economist.com/news/united-states/21...  0.071493  0.100000\n",
        "13  http://www.economist.com/news/united-states/21...  0.074971  0.108696\n",
        "7   http://www.economist.com/news/united-states/21...  0.075302  0.109524\n",
        "19  http://www.economist.com/news/united-states/21...  0.089760  0.145669\n",
        "12  http://www.economist.com/news/united-states/21...  0.091958  0.151163\n",
        "11  http://www.economist.com/news/united-states/21...  0.098159  0.166667"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                                               links  new_bias  old_bias\n",
        "1  http://www.npr.org/blogs/health/2013/11/26/247...  0.147895  0.150794\n",
        "2  http://www.npr.org/blogs/parallels/2013/11/26/...  0.174370  0.216981\n",
        "3  http://www.npr.org/blogs/thetwo-way/2013/11/26...  0.201863  0.285714\n",
        "0  http://www.npr.org/blogs/thetwo-way/2013/11/26...  0.232405  0.362069"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 5 - Creating the Website"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have our data, we needed to create somewhere to show it. We opted to use [Ruby on Rails](http://rubyonrails.org/) due to the quick development time that the language allows. This is because Rails makes a number of assumptions that mean less time is spent setting up basic functionality such as the interaction with a database, instead allowing the developer to focus on creating the website.\n",
      "\n",
      "We chose to use [heroku](http://heroku.com/) for hosting as, apart from being free for small websites, it's quick and easy to deploy versions to and is built to work particuarly well with Ruby on Rails. For the front-end, we opted to use [Backbone.js](http://backbonejs.org/), a client-side model-view-controller framework that reduces the load on the server by creating the website in the client's browser. This also reduces lag-time as pages can be created dynamically on the client-end, rather than having to request, create and download new pages each time. Static resources are served from an AWS S3 Bucket and the Python scripts are run regularly on a separate server. You can access the repo for the site on [github](https://github.com/trender-io/trender-site).\n",
      "\n",
      "<div align=\"center\">\n",
      "<img src=\"https://trenderio.s3.amazonaws.com/devices.png\" width=\"800\" style=\"margin:20px 0;\" />\n",
      "</div>\n",
      "\n",
      "The website was designed to be responsive so content looks suitable on a range of devices, including tablets and mobiles. One of the key aims with the design was to remove any additional clutter, where the only information presented to the visitor is the stories they want to see. The design is therefore minimal, with only the necessary information presented to the individual."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 6 - Reflections"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a team, we found the process of going from idea to product interesting, where we desired not only to explore the data to find trends but to also produce a site that the public would actually want to use.\n",
      "\n",
      "### 6.1 Future work\n",
      "\n",
      "Below are some of the key areas we would like to look at to improve and extend trender.io:\n",
      "\n",
      "#### 6.1.1 Improving the algorithm\n",
      "\n",
      "- Quantifying other sources of bias would allow us to better predict the quality and objectivity of news sources. For example, looking at author bias would allow us to find how biased a particular author is.\n",
      "- Identifying and quantifying other factors contributing to the quality of an article - this may include looking at how much a specific article is being shared, a possible indicator of the quality of an article.\n",
      "\n",
      "#### 6.1.2 Increasing sources of data\n",
      "\n",
      "-\n",
      "\n",
      "#### 6.1.3 Improving the website\n",
      "\n",
      "- \n",
      "\n",
      "### 6.2 "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<style>\n",
      "div.text_cell_render {\n",
      " line-height: 150%;\n",
      " font-size: 100%;\n",
      " width: 800px;\n",
      " margin-left:50px;\n",
      " margin-right:auto;\n",
      "}\n",
      "</style>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}